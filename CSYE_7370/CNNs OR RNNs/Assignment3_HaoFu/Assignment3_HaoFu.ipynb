{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Assignment3-Deep Learning with RNNs(Sequence Models)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.Fill-Mask**  \n",
    "Run a <Fill-Mask> language model. Explain the theory behind your model, and run it.  Analyze how well you think it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.10731076449155807,\n",
       "  'token': 4827,\n",
       "  'token_str': 'fashion',\n",
       "  'sequence': \"hello i'm a fashion model.\"},\n",
       " {'score': 0.08774472028017044,\n",
       "  'token': 2535,\n",
       "  'token_str': 'role',\n",
       "  'sequence': \"hello i'm a role model.\"},\n",
       " {'score': 0.05338387191295624,\n",
       "  'token': 2047,\n",
       "  'token_str': 'new',\n",
       "  'sequence': \"hello i'm a new model.\"},\n",
       " {'score': 0.04667220637202263,\n",
       "  'token': 3565,\n",
       "  'token_str': 'super',\n",
       "  'sequence': \"hello i'm a super model.\"},\n",
       " {'score': 0.027095859870314598,\n",
       "  'token': 2986,\n",
       "  'token_str': 'fine',\n",
       "  'sequence': \"hello i'm a fine model.\"}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "unmasker(\"Hello I'm a [MASK] model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.5702442526817322,\n",
       "  'token': 2298,\n",
       "  'token_str': 'look',\n",
       "  'sequence': 'carver thought he was ready to look forward into the future.'},\n",
       " {'score': 0.23899498581886292,\n",
       "  'token': 2693,\n",
       "  'token_str': 'move',\n",
       "  'sequence': 'carver thought he was ready to move forward into the future.'},\n",
       " {'score': 0.05842737480998039,\n",
       "  'token': 2175,\n",
       "  'token_str': 'go',\n",
       "  'sequence': 'carver thought he was ready to go forward into the future.'},\n",
       " {'score': 0.05043354257941246,\n",
       "  'token': 3357,\n",
       "  'token_str': 'step',\n",
       "  'sequence': 'carver thought he was ready to step forward into the future.'},\n",
       " {'score': 0.023500006645917892,\n",
       "  'token': 2156,\n",
       "  'token_str': 'see',\n",
       "  'sequence': 'carver thought he was ready to see forward into the future.'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "unmasker(\"Carver thought he was ready to [MASK] forward into the future.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labeling them in any way with an automatic process to generate inputs and labels from those texts. There are two steps in the framework: pre-training and fine-tuning. \n",
    "\n",
    "During pre-training, the model is trained on unlabeled data over different pre-training tasks. For finetuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they are initialized with the same pre-trained parameters.\n",
    "\n",
    "I used a sentence as a test. The answer is highly correct. However, this model has a weakness that it cannot make judgments based on long or hidden contextual information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.Question Answering** \n",
    "Run a <Question Answering> language model. Explain the theory behind your model, and run it.  Analyze how well you think it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'the task of extracting an answer from a text given a question', score: 0.6177, start: 34, end: 95\n",
      "Answer: 'SQuAD dataset', score: 0.5152, start: 148, end: 161\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')\n",
    "\n",
    "context = r\"\"\"\n",
    "Extractive Question Answering is the task of extracting an answer from a text given a question. \n",
    "An example of a question answering dataset is the SQuAD dataset, which is entirely based on that \n",
    "task. If you would like to fine-tune a model on a SQuAD task, you may leverage the \n",
    "examples/pytorch/question-answering/run_squad.py script.\n",
    "\"\"\"\n",
    "\n",
    "result = question_answerer(question=\"What is extractive question answering?\", context=context)\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")\n",
    "\n",
    "result = question_answerer(question=\"What is a good example of a question answering dataset?\", context=context)\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'a party', score: 0.2784, start: 36, end: 43\n",
      "Answer: 'he first met Lincoln when he was a shop assistant at a village shop', score: 0.4572, start: 161, end: 228\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')\n",
    "\n",
    "context = r\"\"\"\n",
    "One day, President Lincoln went to a party, At the gathering, a man called Douglas was repeatedly talking about \n",
    "Lincoln's low position in life and saying that he first met Lincoln when he was a shop assistant at a village shop. \n",
    "Finally he said, \"And Mr. Lincoln was a very good waiter too.\"People burst into laughter, but they quieted down \n",
    "when Mr. Lincoln said quietly. \"Gentlemen, what Mr. Douglas has said is true. I did keep a grocery, and I did sell cotton, \n",
    "candles and cigars, and sometimes whisky. But I remember that in those days Mr. Douglas was one of my best customers. \n",
    "I often stood on one side of the counter and sold whisky to Mr. Douglas on the other side, but the difference \n",
    "between us now is：I have left my side of the counter, but Mr. Douglas still sticks to his as firmly as ever.\"\n",
    "\"\"\"\n",
    "\n",
    "result = question_answerer(question=\"Where was Douglas talking about Lincoln's low position in life?\", context=context)\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")\n",
    "\n",
    "result = question_answerer(question=\"Why was Douglas repeatedly talking about Lincoln's low position in life?\", context=context)\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark. Using pipelines to do question answering extract an answer from a text given a question. It leverages a fine-tuned model on SQuAD. This returns an answer extracted from the text, a confidence score, alongside “start” and “end” values, which are the positions of the extracted answer in the text.\n",
    "\n",
    "I used the above model to test, according to the test results, the model has a good answering ability, it can basically find the appropriate answer. However, it still has some disadvantages, the answer must appear in the original text, it can not understand the author wants to express the emotion or position behind the article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.Summarization**  \n",
    "Run a Summarization language model. Explain the theory behind your model, and run it.  Analyze how well you think it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBartForConditionalGeneration.\n",
      "\n",
      "All the layers of TFBartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-large-cnn.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'Liana Barrientos, 39, is charged with two counts of \"offering a false instrument for filing in the first degree\" In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002. She is believed to still be married to four men.'}]\n",
      "[{'summary_text': 'Liana Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002. She is believed to still be married to four men, and at one time, she was married to eight men at once.'}]\n",
      "[{'summary_text': 'Liana Barrientos, 39, is charged with two counts of \"offering a false instrument for filing in the first degree\"'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "ARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
    "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
    "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
    "In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
    "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
    "2010 marriage license application, according to court documents.\n",
    "Prosecutors said the marriages were part of an immigration scam.\n",
    "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
    "After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
    "Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
    "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
    "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
    "Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
    "The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
    "Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
    "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
    "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n",
    "\"\"\"\n",
    "print(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))\n",
    "print(summarizer(ARTICLE, max_length=50, min_length=30, do_sample=False))\n",
    "print(summarizer(ARTICLE, max_length=30, min_length=10, do_sample=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks. This particular checkpoint has been fine-tuned on CNN Daily Mail, a large collection of text-summary pairs.\n",
    "\n",
    "In terms of methods, text abstracts can be divided into two categories: Extractive and Abstractive. The former abstracts sentences from the original text directly, while the latter generates abstracts word by word. In comparison, the extraction method can not summarize the content of the original text roundly because of its inherent characteristics. Before putting into use, it is very important to make sure that the summary of the model summary is consistent with the meaning of the original text.\n",
    "\n",
    "Through the analysis of the experiment, it can be concluded that this model is very good. It will get the most complete article summary according to the limit length. Even if the limit length is small, it will try its best to summarize the central meaning of the article without changing the meaning of the article. It not only realizes the summary of the limited text, but also pays attention to the extraction of important information. You don't lose important information by saying fewer words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.Text Classification** \n",
    "Run a Text Classification language model. Explain the theory behind your model, and run it.  Analyze how well you think it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'POSITIVE', 'score': 0.9997276663780212}, {'label': 'NEGATIVE', 'score': 0.0002722955250646919}]]\n",
      "[[{'label': 'NEGATIVE', 'score': 0.9997480511665344}, {'label': 'POSITIVE', 'score': 0.0002519211848266423}]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"text-classification\", model='distilbert-base-uncased-finetuned-sst-2-english', top_k=None)\n",
    "prediction = classifier(\"He have positive work attitude and be willing and able to work diligently without supervision.\")\n",
    "print(prediction)\n",
    "prediction = classifier(\"I screwed up at work again, and I feel really depressed.\")\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is a fine-tune checkpoint of DistilBERT-base-uncased, fine-tuned on SST-2. This model reaches an accuracy of 91.3 on the dev set. This model can be used for topic classification. You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task. For emotional classification, it needs to add one or two linear layers at the last layer, output category probability, set cross entropy as loss, and train on a small number of data sets. That is input the same input to encoder and decoder. Finally, the last hidden node of decoder is input to classification layer (full connection layer) to obtain the final classification result.\n",
    "\n",
    "The model judges the input's overall emotional disposition by mastering whether the key words are positive or negative. It can be used to analyze most sentences, but it can't identify neutral expressions of emotion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.Text Generation** \n",
    "Run a Text Generation language model. Explain the theory behind your model, and run it.  Analyze how well you think it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, I'm writing a new language for you. But first, I'd like to tell you about the language itself\"},\n",
       " {'generated_text': \"Hello, I'm a language model, and I'm trying to be as expressive as possible. In order to be expressive, it is necessary to know\"},\n",
       " {'generated_text': \"Hello, I'm a language model, so I don't get much of a license anymore, but I'm probably more familiar with other languages on that\"},\n",
       " {'generated_text': \"Hello, I'm a language model, a functional model... It's not me, it's me!\\n\\nI won't bore you with how\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not an object model.\\n\\nIn a nutshell, I need to give language model a set of properties that\"}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The White man worked as a clerk at a hotel in a suburb of Manhattan that became a hotbed for the gay culture. He told the cops he was an \"alt-right\" fanatic after he was diagnosed with a mental illness.\\n\\nBut because it was known that White worked as a clerk and that he was gay, he was forced to leave the city. On the night of the arrest, Police Chief Bill Bratton called a press conference to say an anonymous source had claimed \"the White'},\n",
       " {'generated_text': 'The White man worked as a salesman, and he did not like that the White men were not there to take his orders, that they had a lot of respect for our company.\"\\n\\n\"I think we get it now.\" I said politely. \"This isn\\'t our work, and my company is our work, but our company has to take the responsibility of doing that right now.\"\\n\\n\"But what do you want?\"\\n\\n\"We want to work right now.\"\\n\\nAnd'},\n",
       " {'generated_text': 'The White man worked as a lawyer at the office and would often send his boss to his office when he wanted to have lunch (a common practice in Washington). The boss sometimes asked him to sit in on a meeting, which could lead to a short speech that was later shortened to five or six minutes.\\n\\nIn fact, at many law school conferences at which I saw law students, I was told by one law student that I was more likely to get yelled at by an African American colleague than'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"The White man worked as a\", max_length=100, num_return_sequences=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was trained to guess the next word in sentences.\n",
    "\n",
    "More precisely, inputs are sequences of continuous text of a certain length and the targets are the same sequence, shifted one token (word or piece of word) to the right. The model uses internally a mask-mechanism to make sure the predictions for the token i only uses the inputs from 1 to i but not the future tokens. This way, the model learns an inner representation of the English language that can then be used to extract features useful for downstream tasks. The model is best at what it was pretrained for however, which is generating texts from a prompt.\n",
    "\n",
    "It can be seen from this experiment that the results generated each time are different, and the text will not be repeated. But the authenticity and accuracy of the text can not be confirmed, and usually contains some misunderstanding and bias, not very objective. The training data used for this model has not been released as a dataset one can browse. We know it contains a lot of unfiltered content from the internet, which is far from neutral. As the openAI team themselves point out in their model card. Because large-scale language models like GPT-2 do not distinguish fact from fiction, we don’t support use-cases that require the generated text to be true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.Text2Text Generation** \n",
    "Run a Text2Text language model. Explain the theory behind your model, and run it.  Analyze how well you think it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "model_name = 'tuner007/pegasus_paraphrase'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "def get_response(input_text,num_return_sequences,num_beams):\n",
    "  batch = tokenizer([input_text],truncation=True,padding='longest',max_length=60, return_tensors=\"pt\").to(torch_device)\n",
    "  translated = model.generate(**batch,max_length=60,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "  tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "  return tgt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The test of your knowledge is your ability to convey it.',\n",
       " 'The ability to convey your knowledge is the ultimate test of your knowledge.',\n",
       " 'The ability to convey your knowledge is the most important test of your knowledge.',\n",
       " 'Your capacity to convey your knowledge is the ultimate test of it.',\n",
       " 'The test of your knowledge is your ability to communicate it.',\n",
       " 'Your capacity to convey your knowledge is the ultimate test of your knowledge.',\n",
       " 'Your capacity to convey your knowledge to another is the ultimate test of your knowledge.',\n",
       " 'Your capacity to convey your knowledge is the most important test of your knowledge.',\n",
       " 'The test of your knowledge is how well you can convey it.',\n",
       " 'Your capacity to convey your knowledge is the ultimate test.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_beams = 10\n",
    "num_return_sequences = 10\n",
    "context = \"The ultimate test of your knowledge is your capacity to convey it to another.\"\n",
    "get_response(context,num_return_sequences,num_beams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pegasus is a text to text generation model. It is a single pipeline for all kinds of NLP tasks like Question answering, sentiment classification, question generation, translation, paraphrasing, summarization, etc. More like a rich version supporting more training tasks.\n",
    "\n",
    "This model is used to generate different representations of input statements. It can be used to enrich sentence patterns. I think this model performs well, but it could be better, although it produces the best answers most of the time without much trouble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7.Token Classification** \n",
    "Run a Token Classification language model. Explain the theory behind your model, and run it.  Analyze how well you think it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-PER', 'score': 0.9990139, 'index': 4, 'word': 'Wolfgang', 'start': 11, 'end': 19}, {'entity': 'B-LOC', 'score': 0.999645, 'index': 9, 'word': 'Berlin', 'start': 34, 'end': 40}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"My name is Wolfgang and I live in Berlin\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bert-base-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC). Specifically, this model is a bert-base-cased model that was fine-tuned on the English version of the standard CoNLL-2003 Named Entity Recognition dataset.\n",
    "\n",
    "For Token classification tasks, the complete input is input into encoder and decoder, and all the hidden nodes in the last layer of decoder are represented as the model of each Token, and then the representation of each Token is classified, and finally the result output is obtained. The training dataset distinguishes between the beginning and continuation of an entity so that if there are back-to-back entities of the same type, the model can output where the second entity begins.\n",
    "\n",
    "This model is limited by its training dataset of entity-annotated news articles from a specific span of time. This may not generalize well for all use cases in different domains. Furthermore, the model occassionally tags subword tokens as entities and post-processing of results may be necessary to handle those cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **8.Translation** \n",
    "Run a Translation language model. Explain the theory behind your model, and run it.  Analyze how well you think it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Eu gosto de comer arroz.'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unicamp-dl/translation-en-pt-t5\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"unicamp-dl/translation-en-pt-t5\")\n",
    "\n",
    "enpt_pipeline = pipeline('text2text-generation', model=model, tokenizer=tokenizer)\n",
    "enpt_pipeline(\"translate English to Portuguese: I like to eat rice.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "translation model is used to translate text to target language. Transfer learning, where the model is first pre-trained on data-rich tasks, and then fine-tuned on downstream tasks, has become a powerful technology in natural language processing (NLP). The effectiveness of transfer learning has spawned a variety of methods, methodologies and practices. In this article, the model explores the prospects of NLP transfer learning technology by introducing a unified framework for converting each language question into a text-to-text format. The model system studies and compares the pre-training goals, architectures, unlabeled data sets, migration methods, and other factors of dozens of language understanding tasks.\n",
    "\n",
    "By combining the insights from our exploration with scale, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.\n",
    "\n",
    "This model brings an implementation of T5 for translation in EN-PT tasks using a modest hardware setup. It proposes some changes in tokenizator and post-processing that improves the result and used a Portuguese pretrained model for the translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **9.Zero-Shot Classification** \n",
    "Run a Zero-Shot language model. Explain the theory behind your model, and run it.  Analyze how well you think it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'one day I will see the world',\n",
       " 'labels': ['travel', 'dancing', 'cooking'],\n",
       " 'scores': [0.9938651919364929, 0.003273801878094673, 0.0028610294684767723]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "sequence_to_classify = \"one day I will see the world\"\n",
    "candidate_labels = ['travel', 'cooking', 'dancing']\n",
    "classifier(sequence_to_classify, candidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'Time is that we do not come loose',\n",
       " 'labels': ['time', 'friend', 'exploration', 'friendship'],\n",
       " 'scores': [0.8511101007461548,\n",
       "  0.07391443103551865,\n",
       "  0.05376188084483147,\n",
       "  0.02121361717581749]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_to_classify = \"Time is that we do not come loose\"\n",
    "candidate_labels = ['friendship', 'time', 'friend', 'exploration']\n",
    "classifier(sequence_to_classify, candidate_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bart-large-mnli is a zero-shot classification model. It's a method for using pre-trained NLI models as a ready-made zero-shot sequence classifiers. The method works by posing the sequence to be classified as the NLI premise and to construct a hypothesis from each candidate label. For example, if we want to evaluate whether a sequence belongs to the class \"politics\", we could construct a hypothesis of This text is about politics.. The probabilities for entailment and contradiction are then converted to label probabilities. This method is surprisingly effective in many cases, particularly when used with larger pre-trained models like BART and Roberta. \n",
    "\n",
    "The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a new virtual adversarial training method is used for fine-tuning to improve models' generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understand (NLU) and natural langauge generation (NLG) downstream tasks.\n",
    "\n",
    "This model basically works. Able to distinguish different labels based on the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **10.Sentence Similarity** \n",
    "Run a Sentence Similarity language model. Explain the theory behind your model, and run it.  Analyze how well you think it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<A>All plans come with unlimited private models and datasets.', '<A>AutoNLP seamlessly integrated with the Hugging Face ecosystem.', '<A>Based on how much training data and model variants are created, we send you a compute cost and payment link.']\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "question = \"<Q>How many models can I host on HuggingFace?\"\n",
    "answers = [\n",
    "  \"<A>All plans come with unlimited private models and datasets.\",\n",
    "  \"<A>AutoNLP seamlessly integrated with the Hugging Face ecosystem.\",\n",
    "  \"<A>Based on how much training data and model variants are created, we send you a compute cost and payment link.\"\n",
    "]\n",
    "model = SentenceTransformer('clips/mfaq')\n",
    "q_embedding, *a_embeddings = model.encode([question] + answers)\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is a multilingual FAQ retrieval model trained on the MFAQ dataset, it ranks candidate answers according to a given question. Some people collected around 6M FAQ pairs from the web, in 21 different languages. Although this is significantly larger than existing FAQ retrieval datasets, it comes with its own challenges: duplication of content and uneven distribution of topics.\n",
    "\n",
    "This test proves that the model analyzes the three answers given, and can find the correct answer in the answer without keywords and context, and there are several other interference items."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
