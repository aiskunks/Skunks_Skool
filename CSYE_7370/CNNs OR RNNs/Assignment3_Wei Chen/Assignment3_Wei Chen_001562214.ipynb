{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d10e75d",
   "metadata": {},
   "source": [
    "## Assignment 3_Wei Chen 001562214"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2411fb",
   "metadata": {},
   "source": [
    "Choice B- Deep Learning with RNNs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71a8da7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-11-18 21:05:26.999401: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Fisrt we import libraries\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec96301",
   "metadata": {},
   "source": [
    "### 1. Fill-Mask (10 Points)\n",
    "Run a Fill-Mask language model. Explain the theory behind your model, and run it.  Analyze how well you think it worked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511f2319",
   "metadata": {},
   "source": [
    "Fill-Mask model depends on theory of BERT, short for Bidirectional Encoder Representations from Transformers, is a Machine Learning (ML) model for natural language processing. A massive dataset of 3.3 Billion words has contributed to BERT’s continued success. Masked Language Model enables/enforces bidirectional learning from text by masking (hiding) a word in a sentence and forcing BERT to bidirectionally use the words on either side of the covered word to predict the masked word.\n",
    "\n",
    "Transformers work by leveraging attention, a powerful deep-learning algorithm, first seen in computer vision models. Transformers create differential weights signaling which words in a sentence are the most critical to further process. A transformer does this by successively processing an input through a stack of transformer layers, usually called the encoder.BERT however, doesn’t use a decoder. Transformers are uniquely suited for unsupervised learning because they can efficiently process millions of data points.\n",
    "\n",
    "the image below shows the architecture of bert.\n",
    "![bert](image/bert.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15798164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading: 100%|██████████████████████████████| 480/480 [00:00<00:00, 153kB/s]\n",
      "Downloading: 100%|███████████████████████████| 487M/487M [00:13<00:00, 37.0MB/s]\n",
      "2022-11-18 21:06:00.461972: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "All model checkpoint layers were used when initializing TFRobertaForMaskedLM.\n",
      "\n",
      "All the layers of TFRobertaForMaskedLM were initialized from the model checkpoint at distilroberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForMaskedLM for predictions without further training.\n",
      "Downloading: 100%|███████████████████████████| 899k/899k [00:00<00:00, 4.82MB/s]\n",
      "Downloading: 100%|███████████████████████████| 456k/456k [00:00<00:00, 3.02MB/s]\n",
      "Downloading: 100%|█████████████████████████| 1.36M/1.36M [00:00<00:00, 6.25MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.17146670818328857,\n",
       "  'token': 3497,\n",
       "  'token_str': ' Republic',\n",
       "  'sequence': 'Taiwan is one of Republic of China.'},\n",
       " {'score': 0.11900803446769714,\n",
       "  'token': 6611,\n",
       "  'token_str': ' neighbors',\n",
       "  'sequence': 'Taiwan is one of neighbors of China.'},\n",
       " {'score': 0.07250393182039261,\n",
       "  'token': 1667,\n",
       "  'token_str': ' parts',\n",
       "  'sequence': 'Taiwan is one of parts of China.'},\n",
       " {'score': 0.052117593586444855,\n",
       "  'token': 10689,\n",
       "  'token_str': ' neighbours',\n",
       "  'sequence': 'Taiwan is one of neighbours of China.'},\n",
       " {'score': 0.05088961496949196,\n",
       "  'token': 4295,\n",
       "  'token_str': ' dozens',\n",
       "  'sequence': 'Taiwan is one of dozens of China.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_fm = pipeline(\"fill-mask\")\n",
    "classifier_fm(\"Taiwan is one of <mask> of China.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950384f9",
   "metadata": {},
   "source": [
    "This model works well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03fa3f0",
   "metadata": {},
   "source": [
    "### 2. Question Answering (10 Points)\n",
    "Run a Question Answering language model. Explain the theory behind your model, and run it.  Analyze how well you think it worked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a44889",
   "metadata": {},
   "source": [
    "Question Answering language model depends on theory of BERT, too.\n",
    "\n",
    "Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document. Some question answering models can generate answers without context.\n",
    "\n",
    "You can infer with QA models with the Transformers library using the question-answering pipeline. If no model checkpoint is given, the pipeline will be initialized with distilbert-base-cased-distilled-squad. This pipeline takes a question and a context from which the answer will be extracted and returned.\n",
    "\n",
    "Here is the architecture of Question Answering language model.\n",
    "![QA](image/QA.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e6d6ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading: 100%|██████████████████████████████| 473/473 [00:00<00:00, 263kB/s]\n",
      "Downloading: 100%|███████████████████████████| 261M/261M [00:07<00:00, 33.3MB/s]\n",
      "Some layers from the model checkpoint at distilbert-base-cased-distilled-squad were not used when initializing TFDistilBertForQuestionAnswering: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased-distilled-squad and are newly initialized: ['dropout_38']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading: 100%|███████████████████████████| 29.0/29.0 [00:00<00:00, 11.4kB/s]\n",
      "Downloading: 100%|███████████████████████████| 213k/213k [00:00<00:00, 2.05MB/s]\n",
      "Downloading: 100%|███████████████████████████| 436k/436k [00:00<00:00, 3.29MB/s]\n"
     ]
    }
   ],
   "source": [
    "qa_model = pipeline(\"question-answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5034f46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9538118243217468, 'start': 31, 'end': 39, 'answer': 'İstanbul'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Where do I live?\"\n",
    "context = \"My name is Merve and I live in İstanbul.\"\n",
    "qa_model(question = question, context = context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98a17e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.7485843896865845, 'start': 21, 'end': 33, 'answer': 'Pulp Fiction'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Which movie have you watched most recently?\"\n",
    "context = \"My favorite movie is Pulp Fiction and I watched Fuyajo last night .\"\n",
    "qa_model(question = question, context = context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aea89d",
   "metadata": {},
   "source": [
    "From the test above, we can see that qa model don't find the key words of the question, but it may perform well for simpler question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e6d482",
   "metadata": {},
   "source": [
    "### 3. Summarization (10 Points)\n",
    "Run a Summarization language model. Explain the theory behind your model, and run it.  Analyze how well you think it worked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d6000b",
   "metadata": {},
   "source": [
    "Summarization language model depends on whether theory of BERT or T5. Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\n",
    "\n",
    "Summarization creates a shorter version of a document or an article that captures all the important information. Along with translation, it is another example of a task that can be formulated as a sequence-to-sequence task. Summarization can be:\n",
    "    Extractive: extract the most relevant information from a document.\n",
    "    Abstractive: generate new text that captures the most relevant information.\n",
    "we use T5, a pre-trained and very large (e.g., roughly twice the size of BERT-base) encoder-decoder Transformer model. T5, a model devised by Google, is an important advancement in the field of Transformers because it achieves near human-level performance on a variety of benchmarks like GLUE and SQuAD. The basic idea behind this metric is to compare a generated summary against a set of reference summaries that are typically created by humans.\n",
    "\n",
    "Here is architecture of T5:\n",
    "![t5](image/t5.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "839504ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to t5-small and revision d769bba (https://huggingface.co/t5-small).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading: 100%|██████████████████████████| 1.20k/1.20k [00:00<00:00, 692kB/s]\n",
      "Downloading: 100%|███████████████████████████| 242M/242M [00:05<00:00, 41.6MB/s]\n",
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "Downloading: 100%|███████████████████████████| 792k/792k [00:00<00:00, 4.54MB/s]\n",
      "Downloading: 100%|█████████████████████████| 1.39M/1.39M [00:00<00:00, 6.22MB/s]\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "2022-11-18 21:08:37.046531: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7fed5345a2d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-11-18 21:08:37.046756: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Host, Default Version\n",
      "2022-11-18 21:08:37.049627: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-18 21:08:37.107585: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-11-18 21:08:37.352460: I tensorflow/compiler/jit/xla_compilation_cache.cc:476] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'the city of Paris is the centre and seat of government of the region and province of Île-de-France . it has an estimated population of 2,175,601 residents as of 2018 .'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summer = pipeline(\"summarization\")\n",
    "summer(\"Paris is the capital and most populous city of France, with an estimated population of 2,175,601 residents as of 2018, in an area of more than 105 square kilometres (41 square miles). The City of Paris is the centre and seat of government of the region and province of Île-de-France, or Paris Region, which has an estimated population of 12,174,880, or about 18 percent of the population of France as of 2017.\",max_length=56)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddc021f",
   "metadata": {},
   "source": [
    "This model works well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6041554",
   "metadata": {},
   "source": [
    "### 4. Text Classification (10 Points)\n",
    "Run a Text Classification language model. Explain the theory behind your model, and run it.  Analyze how well you think it worked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4985971",
   "metadata": {},
   "source": [
    "Text classification can be performed in two ways: manual or automatic.Manual text classification involves a human annotator, who interprets the content of text and categorizes it accordingly. This method can deliver good results but it’s time-consuming and expensive. We will focus on Automatic text classification.\n",
    "\n",
    "Automatic text classification applies machine learning, natural language processing (NLP), and other AI-guided techniques to automatically classify text in a faster, more cost-effective, and more accurate manner. There are many approaches to automatic text classification, but they all fall under three types of systems:\n",
    "    Rule-based systems\n",
    "    Machine learning-based systems\n",
    "    Hybrid systems\n",
    "    \n",
    "the most popular text classification algorithms is as follows:\n",
    "    1) Support Vector Machines\n",
    "    2) Naive Bayes Classifier\n",
    "    3) XGBOOST\n",
    "    4) KNN\n",
    "    \n",
    "Below is the image of text Classification\n",
    "![text](image/text.png)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa895581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading: 100%|██████████████████████████████| 629/629 [00:00<00:00, 348kB/s]\n",
      "Downloading: 100%|███████████████████████████| 268M/268M [00:06<00:00, 43.6MB/s]\n",
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_120']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading: 100%|███████████████████████████| 48.0/48.0 [00:00<00:00, 24.7kB/s]\n",
      "Downloading: 100%|████████████████████████████| 232k/232k [00:00<00:00, 641kB/s]\n"
     ]
    }
   ],
   "source": [
    "tx_classifier = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f849a211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.8516820073127747}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx_classifier(\"A soccer game with multiple males playing. Some men are playing a sport.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eaab85",
   "metadata": {},
   "source": [
    "### 5. Text Generation (10 Points)\n",
    "Run a Text Generation language model. Explain the theory behind your model, and run it.  Analyze how well you think it worked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4034c45a",
   "metadata": {},
   "source": [
    "\n",
    "The text generation is backed by a large-scale unsupervised language model that can generate paragraphs of text. This transformer-based language model, based on the GPT-2 model by OpenAI, intakes a sentence or partial sentence and predicts subsequent text from that input. Text generation can be based on BERT, too, but they are fundamentally different in that BERT has just the encoder blocks from the transformer, whilst GPT-2 has just the decoder blocks from the transformer.\n",
    "\n",
    "GPT-2 (Generative Pre-trained Transformer 2) algorithm is an unsupervised transformer language model. Transformer language models take advantage of transformer blocks. These blocks make it possible to process intra-sequence dependencies for all tokens in a sequence at the same time. GPT2 has been developed by OpenAI and is a powerful generative NLP model that excels in processing long-range dependencies and it is pre-trained on a diverse corpus of text.\n",
    "\n",
    "Here is an image of GPT-2:\n",
    "![gt2](image/gt2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98562d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████████████████████████| 665/665 [00:00<00:00, 323kB/s]\n",
      "Downloading: 100%|███████████████████████████| 498M/498M [00:10<00:00, 45.6MB/s]\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Downloading: 100%|█████████████████████████| 1.04M/1.04M [00:00<00:00, 5.77MB/s]\n",
      "Downloading: 100%|███████████████████████████| 456k/456k [00:00<00:00, 3.32MB/s]\n",
      "Downloading: 100%|█████████████████████████| 1.36M/1.36M [00:00<00:00, 7.28MB/s]\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hello, I\\'m a language model. I\\'ve always been interested in the natural language, and I thought, \"Why not the AI?\". And for'},\n",
       " {'generated_text': \"Hello, I'm a language model.\\n\\nHow did you come to be able to write languages?\\n\\nI came to Java from a university\"},\n",
       " {'generated_text': \"Hello, I'm a language model in a Python package called G.js. It's a library for storing JavaScript objects in a very small scope.\"}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model = 'gpt2')\n",
    "generator(\"Hello, I'm a language model\", max_length = 30, num_return_sequences=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fa8a11",
   "metadata": {},
   "source": [
    "the model works well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04529c3d",
   "metadata": {},
   "source": [
    "### 6. Text2Text Generation (10 Points)\n",
    "Run a Text2Text language model. Explain the theory behind your model, and run it.  Analyze how well you think it worked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07448d6b",
   "metadata": {},
   "source": [
    "Text-to-Text generation models have a separate pipeline called Text2Text Generation. Text2Text Generation is a single pipeline for all kinds of NLP tasks like Question answering, sentiment classification, question generation, translation, paraphrasing, summarization, etc. Text2Text Generation pipeline can currently be loaded from pipeline() using the following task identifier: \"text2text-generation\"\n",
    "\n",
    "Text2Text Generation is also based on Finetuning T5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55e84b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to t5-base and revision 686f1db (https://huggingface.co/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading: 100%|██████████████████████████| 1.20k/1.20k [00:00<00:00, 542kB/s]\n",
      "Downloading: 100%|███████████████████████████| 892M/892M [00:23<00:00, 37.2MB/s]\n",
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "Downloading: 100%|███████████████████████████| 792k/792k [00:00<00:00, 4.56MB/s]\n",
      "Downloading: 100%|█████████████████████████| 1.39M/1.39M [00:00<00:00, 7.42MB/s]\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "text2text_generator = pipeline(\"text2text-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5c83bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'the answer to life, the universe and everything'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2text_generator(\"question: What is 42 ? context: 42 is the answer to life, the universe and everything\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9875030c",
   "metadata": {},
   "source": [
    "Text2Text Generation model can generate answer from the key words in question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbacdc98",
   "metadata": {},
   "source": [
    "### 7. Token Classification (10 Points)\n",
    "Run a Token Classification language model. Explain the theory behind your model, and run it.  Analyze how well you think it worked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15067990",
   "metadata": {},
   "source": [
    "The Token classification Task is similar to text classification, except each token within the text receives a prediction. Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\n",
    "\n",
    "NER is the task of recognizing named entities in a text. These entities can be the names of people, locations, or organizations. The task is formulated as labeling each token with a class for each named entity and a class named \"0\" for tokens that do not contain any entities. The input for this task is text and the output is the annotated text with named entities.\n",
    "\n",
    "In PoS tagging, the model recognizes parts of speech, such as nouns, pronouns, adjectives, or verbs, in a given text. The task is formulated as labeling each word with a part of the speech.\n",
    "\n",
    "Token-classification-model architecture of BatteryBERT. E represents the input embedding. T represents the contextual representation of token i. [CLS] is the special symbol for classification output, and [SEP] is the special symbol to separate non-consecutive token sequences.\n",
    "Here is the image:\n",
    "![token](image/token.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9babd3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading: 100%|██████████████████████████████| 998/998 [00:00<00:00, 450kB/s]\n",
      "Downloading: 100%|█████████████████████████| 1.33G/1.33G [00:49<00:00, 27.2MB/s]\n",
      "Some layers from the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing TFBertForTokenClassification: ['dropout_147']\n",
      "- This IS expected if you are initializing TFBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForTokenClassification were initialized from the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n",
      "Downloading: 100%|███████████████████████████| 60.0/60.0 [00:00<00:00, 26.4kB/s]\n",
      "Downloading: 100%|███████████████████████████| 213k/213k [00:00<00:00, 2.76MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-PER',\n",
       "  'score': 0.99770516,\n",
       "  'index': 5,\n",
       "  'word': 'Omar',\n",
       "  'start': 10,\n",
       "  'end': 14},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9968976,\n",
       "  'index': 10,\n",
       "  'word': 'Zürich',\n",
       "  'start': 29,\n",
       "  'end': 35}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk_classifier = pipeline(\"ner\")\n",
    "tk_classifier(\"Hello I'm Omar and I live in Zürich.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adfb909",
   "metadata": {},
   "source": [
    "Token classification model is similar to text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de568ea7",
   "metadata": {},
   "source": [
    "### 8. Translation (10 Points)\n",
    "Run a Translation language model. Explain the theory behind your model, and run it.  Analyze how well you think it worked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eeff4e",
   "metadata": {},
   "source": [
    "Translation is the task of converting text from one language to another.Translation models can be used to build conversational agents across different languages. This can be done in two ways.\n",
    "    Translate the dataset to a new language\n",
    "    Translate the input and output of the agent\n",
    "    \n",
    "The default model for the pipeline is t5-base which under the hood adds a task prefix indicating the task itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4908560b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to t5-base and revision 686f1db (https://huggingface.co/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': ' quel âge êtes-vous?'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_fr_translator = pipeline(\"translation_en_to_fr\")\n",
    "en_fr_translator(\"How old are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cef4912",
   "metadata": {},
   "source": [
    "translation language model(tlm) analyzes bodies of question data to provide a basis for their word predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0c0ff4",
   "metadata": {},
   "source": [
    "### 9. Zero-Shot Classification (10 Points)\n",
    "Run a Zero-Shot language model. Explain the theory behind your model, and run it.  Analyze how well you think it worked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a751b9b8",
   "metadata": {},
   "source": [
    "Zero-shot learning (ZSL) is a form of transfer learning that aims to learn patterns from labeled data in order to detect classes that were never seen during training. Zero-shot learning is a promising learning method, in which the classes covered by training instances and the classes we aim to classify are disjoint. In other words, Zero-shot learning is about leveraging supervised learning with no additional training data\n",
    "![zero](image/zero.jpeg)\n",
    "\n",
    "To realize the ZSL function, it seems that two parts need to be solved: the first problem is to obtain a suitable category description; the second problem is to build a suitable classification model.\n",
    "\n",
    "There are also some problems about ZSL\n",
    "\n",
    "domain shift problem\n",
    "![domain](image/domain.png)\n",
    "\n",
    "Hubness problem\n",
    "\n",
    "semantic gap\n",
    "![domain](image/gap.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b70a3e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to roberta-large-mnli and revision 130fb28 (https://huggingface.co/roberta-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at roberta-large-mnli.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "classifier_0 = pipeline(\"zero-shot-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b3d9976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'What kind of movie will Quentin Tarantino prefer',\n",
       " 'labels': ['action', 'science-fiction', 'romantic'],\n",
       " 'scores': [0.4491293430328369, 0.2768940329551697, 0.2739766538143158]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = \"What kind of movie will Quentin Tarantino prefer\"\n",
    "candidate_labels = [\"science-fiction\", \"action\", \"romantic\"]\n",
    "classifier_0(sequence, candidate_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1de011",
   "metadata": {},
   "source": [
    "work well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f34537",
   "metadata": {},
   "source": [
    "### 10. Sentence Similarity (10 Points)\n",
    "Run a Sentence Similarity language model. Explain the theory behind your model, and run it.  Analyze how well you think it worked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7adf309",
   "metadata": {},
   "source": [
    "Sentence Similarity is the task of determining how similar two texts are. Sentence similarity models convert input texts into vectors (embeddings) that capture semantic information and calculate how close (similar) they are between them. This task is particularly useful for information retrieval and clustering/grouping.The Sentence Transformers library is very powerful for calculating embeddings of sentences, paragraphs, and entire documents. An embedding is just a vector representation of a text and is useful for finding how similar two texts are.\n",
    "\n",
    "Sentence Similarity language model can be based on BERT and Word2Vec.A big part of NLP relies on similarity in highly-dimensional spaces. Typically an NLP solution will take some text, process it to create a big vector/array representing said text — then perform several transformations.It’s highly-dimensional magic.Sentence similarity is one of the clearest examples of how powerful highly-dimensional magic can be.\n",
    "The logic is this:\n",
    "\n",
    "    Take a sentence, convert it into a vector.\n",
    "    Take many other sentences, and convert them into vectors.\n",
    "    Find sentences that have the smallest distance (Euclidean) or smallest angle (cosine similarity) between them —       more on that here.\n",
    "    We now have a measure of semantic similarity between sentences \n",
    "\n",
    "Here is the image:\n",
    "![sentence](image/sentence.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ffe2841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-11-18 22:12:07.610060: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0225026  -0.07829174 -0.02303076 ... -0.00827927  0.02652689\n",
      "  -0.00201896]\n",
      " [ 0.04170237  0.00109738 -0.01553418 ... -0.02181629 -0.06359358\n",
      "  -0.00875285]]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# import tensorflow\n",
    "# from transformers import TFAutoModelForSequenceClassification\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58844eff",
   "metadata": {},
   "source": [
    "it performs not bad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd2219c",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bc4870",
   "metadata": {},
   "source": [
    "[1] HuggingFace https://huggingface.co/\n",
    "\n",
    "[2] Zero-Shot Learning https://zhuanlan.zhihu.com/p/34656727\n",
    "\n",
    "[3] A Full Guide to Finetuning T5 for Text2Text and Building a Demo with Streamlit https://medium.com/nlplanet/a-full-guide-to-finetuning-t5-for-text2text-and-building-a-demo-with-streamlit-c72009631887\n",
    "\n",
    "[4] Creative writing using GPT-2 Text Generation https://sagemaker-examples.readthedocs.io/en/latest/aws_marketplace/using_model_packages/creative-writing-using-gpt-2-text-generation/creative-writing-using-gpt-2-text-generation.html\n",
    "\n",
    "[5] What is Text Classification? https://www.projectpro.io/article/machine-learning-nlp-text-classification-algorithms-and-models/523#mcetoc_1fle0r0755\n",
    "\n",
    "[6] How to Train A Question-Answering Machine Learning Model (BERT) https://blog.paperspace.com/how-to-train-question-answering-machine-learning-models/\n",
    "\n",
    "[7] BERT Transformers – How Do They Work? https://www.exxactcorp.com/blog/Deep-Learning/how-do-bert-transformers-work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5f502e",
   "metadata": {},
   "source": [
    "##  Licenses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd88c257",
   "metadata": {},
   "source": [
    "All licenses in this repository are copyrighted by their respective authors.\n",
    "\n",
    "------------------------------------------------------------------------------\n",
    "\n",
    "@author: Wei Chen (chen.wei6@northeastern.edu)\n",
    "\n",
    "The person who associated a work with this deed has dedicated the work to the\n",
    "public domain by waiving all of his or her rights to the work worldwide under\n",
    "copyright law, including all related and neighboring rights,\n",
    "to the extent allowed by law.\n",
    "\n",
    "You can copy, modify, distribute and perform the work, even for commercial\n",
    "purposes, all without asking permission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9697437a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mygym",
   "language": "python",
   "name": "mygym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
