{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "367e876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9df4e4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SpaceInvaders-v0')\n",
    "height, width, channels = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71d4d045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58fa4127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b83cf382",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82ea5185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "98fb648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Convolution2D\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "39fd45ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(height, width, channels, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, (8,8), strides=(4,4), activation='relu', input_shape=(3,height, width, channels)))\n",
    "    model.add(Convolution2D(64, (4,4), strides=(2,2), activation='relu'))\n",
    "    model.add(Convolution2D(64, (3,3), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9e4208ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(height, width, channels, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5d206351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_15 (Conv2D)           (None, 3, 51, 39, 32)     6176      \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 3, 24, 18, 64)     32832     \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 3, 22, 16, 64)     36928     \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 67584)             0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 512)               34603520  \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 34,812,326\n",
      "Trainable params: 34,812,326\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "da58150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent, SARSAAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy, BoltzmannQPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0f66d5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 3000\n",
    "total_test_episodes = 10\n",
    "max_steps = 10000\n",
    "learning_rate = 0.01\n",
    "Gamma = 0.9\n",
    "epsilon = 1.0\n",
    "min_epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b26aad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=epsilon, value_min=min_epsilon, value_test=.2, nb_steps=max_steps)\n",
    "    memory = SequentialMemory(limit=1000, window_length=3)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  enable_dueling_network=True, dueling_type='avg', \n",
    "                   nb_actions=actions, nb_steps_warmup=100,\n",
    "                   gamma=Gamma\n",
    "                  )\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "faaeabe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e9b9c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39977544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 3000 steps ...\n",
      "WARNING:tensorflow:From C:\\Users\\10857\\anaconda3\\envs\\py373\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "  701/3000: episode: 1, duration: 168.648s, episode steps: 701, steps per second:   4, episode reward: 210.000, mean reward:  0.300 [ 0.000, 30.000], mean action: 2.466 [0.000, 5.000],  loss: 7082852.646556, mean_q: 188.187017, mean_eps: 0.963955\n",
      " 1118/3000: episode: 2, duration: 124.704s, episode steps: 417, steps per second:   3, episode reward: 125.000, mean reward:  0.300 [ 0.000, 25.000], mean action: 2.595 [0.000, 5.000],  loss: 1.726446, mean_q: 4.220435, mean_eps: 0.918190\n",
      " 1949/3000: episode: 3, duration: 252.506s, episode steps: 831, steps per second:   3, episode reward: 135.000, mean reward:  0.162 [ 0.000, 25.000], mean action: 2.566 [0.000, 5.000],  loss: 1.266827, mean_q: 4.011137, mean_eps: 0.862030\n",
      " 2789/3000: episode: 4, duration: 247.854s, episode steps: 840, steps per second:   3, episode reward: 200.000, mean reward:  0.238 [ 0.000, 30.000], mean action: 2.457 [0.000, 5.000],  loss: 0.969853, mean_q: 3.857902, mean_eps: 0.786835\n",
      "done, took 855.587 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x160be1ccf48>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=total_episodes, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebae0344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 210.000, steps: 1013\n",
      "Episode 2: reward: 110.000, steps: 802\n",
      "Episode 3: reward: 395.000, steps: 1356\n",
      "Episode 4: reward: 135.000, steps: 828\n",
      "Episode 5: reward: 430.000, steps: 1004\n",
      "Episode 6: reward: 275.000, steps: 1261\n",
      "Episode 7: reward: 80.000, steps: 689\n",
      "Episode 8: reward: 75.000, steps: 412\n",
      "Episode 9: reward: 140.000, steps: 762\n",
      "Episode 10: reward: 115.000, steps: 687\n",
      "196.5\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=total_test_episodes, visualize=True)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c55c502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('Save/3000/dqn.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe82889",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('Save/3000/dqn.h5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a3cdc2",
   "metadata": {},
   "source": [
    "### 1. Establish a baseline performance. How well did your Deep Q-learning do on your problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecdc21f",
   "metadata": {},
   "source": [
    "total_episodes = 3000\n",
    "total_test_episodes = 10\n",
    "max_steps = 10000\n",
    "learning_rate = 0.01\n",
    "Gamma = 0.9\n",
    "epsilon = 1.0\n",
    "min_epsilon = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd1b509",
   "metadata": {},
   "source": [
    "Scored average 196.5 point in the test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84721007",
   "metadata": {},
   "source": [
    "### 2. What are the states, the actions, and the size of the Q-table?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f907290",
   "metadata": {},
   "source": [
    "The action is one of noop,fire,right,left,rightfire,leftfire.Corresponding numbers 0-5.  \n",
    "The state is the picture information under the current frame.height 210,width 160, channel 3  \n",
    "The q-table is simulated by a neural network consisting of three convolution layers and two dense layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fada1532",
   "metadata": {},
   "source": [
    "### 3. What are the rewards? Why did you choose them? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f57277",
   "metadata": {},
   "source": [
    "reward is score of game(3 life), Because the score is the most important thing in a atari game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0193311d",
   "metadata": {},
   "source": [
    "### 4. How did you choose alpha and gamma in the Bellman equation? Try at least one additional value for alpha and gamma. How did it change the baseline performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0202a3c",
   "metadata": {},
   "source": [
    "0.01 and 0.9  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "24d6700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "Gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "975db926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=epsilon, value_min=min_epsilon, value_test=.2, nb_steps=max_steps)\n",
    "    memory = SequentialMemory(limit=1000, window_length=3)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  enable_dueling_network=True, dueling_type='avg', \n",
    "                   nb_actions=actions, nb_steps_warmup=100,\n",
    "                   gamma=Gamma\n",
    "                  )\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06935f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9f80b03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 3000 steps ...\n",
      "  397/3000: episode: 1, duration: 80.559s, episode steps: 397, steps per second:   5, episode reward: 85.000, mean reward:  0.214 [ 0.000, 25.000], mean action: 2.529 [0.000, 5.000],  loss: 33114898048163196.000000, mean_q: 19349493.085983, mean_eps: 0.977635\n",
      " 1204/3000: episode: 2, duration: 232.927s, episode steps: 807, steps per second:   3, episode reward: 95.000, mean reward:  0.118 [ 0.000, 20.000], mean action: 2.507 [0.000, 5.000],  loss: 4.172706, mean_q: 7.387519, mean_eps: 0.928000\n",
      " 1833/3000: episode: 3, duration: 186.733s, episode steps: 629, steps per second:   3, episode reward: 110.000, mean reward:  0.175 [ 0.000, 30.000], mean action: 2.552 [0.000, 5.000],  loss: 4.560092, mean_q: 7.495776, mean_eps: 0.863380\n",
      " 2570/3000: episode: 4, duration: 220.170s, episode steps: 737, steps per second:   3, episode reward: 140.000, mean reward:  0.190 [ 0.000, 30.000], mean action: 2.550 [0.000, 5.000],  loss: 6.195071, mean_q: 7.153993, mean_eps: 0.801910\n",
      "done, took 848.509 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x160cbaac488>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=total_episodes, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04292235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 270.000, steps: 721\n",
      "Episode 2: reward: 270.000, steps: 709\n",
      "Episode 3: reward: 270.000, steps: 721\n",
      "Episode 4: reward: 270.000, steps: 724\n",
      "Episode 5: reward: 270.000, steps: 713\n",
      "Episode 6: reward: 270.000, steps: 709\n",
      "Episode 7: reward: 270.000, steps: 718\n",
      "Episode 8: reward: 270.000, steps: 730\n",
      "Episode 9: reward: 270.000, steps: 722\n",
      "Episode 10: reward: 270.000, steps: 721\n",
      "270.0\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=total_test_episodes, visualize=True)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1bd4b3",
   "metadata": {},
   "source": [
    "Obviously, the larger the value of alpha, the less the previous learned results are retained. The larger the value of gamma, the more long-term future benefits we take into account the value generated by the current behavior.In this experiment, this resulted in the agent being satisfied with a score of 270"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a3a6910",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('Save/differ/dqn.h5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7af6b4",
   "metadata": {},
   "source": [
    "### 5. Try a policy other than e-greedy. How did it change the baseline performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "18d87375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=1000, window_length=3)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  enable_dueling_network=True, dueling_type='avg', \n",
    "                   nb_actions=actions, nb_steps_warmup=100,\n",
    "                   gamma=Gamma\n",
    "                  )\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "57c9fae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "21ccb31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 3000 steps ...\n",
      "  781/3000: episode: 1, duration: 245.810s, episode steps: 781, steps per second:   3, episode reward: 210.000, mean reward:  0.269 [ 0.000, 30.000], mean action: 2.744 [0.000, 5.000],  loss: 720102.249747, mean_q: -10.475616\n",
      " 1334/3000: episode: 2, duration: 214.154s, episode steps: 553, steps per second:   3, episode reward: 155.000, mean reward:  0.280 [ 0.000, 25.000], mean action: 2.826 [0.000, 5.000],  loss: 4.833099, mean_q: 7.209023\n",
      " 2053/3000: episode: 3, duration: 283.262s, episode steps: 719, steps per second:   3, episode reward: 180.000, mean reward:  0.250 [ 0.000, 30.000], mean action: 2.723 [0.000, 5.000],  loss: 5.458886, mean_q: 7.381583\n",
      " 2969/3000: episode: 4, duration: 354.171s, episode steps: 916, steps per second:   3, episode reward: 490.000, mean reward:  0.535 [ 0.000, 200.000], mean action: 2.440 [0.000, 5.000],  loss: 13.912703, mean_q: 7.651207\n",
      "done, took 1109.288 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x160cd0e5788>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=total_episodes, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ba36bfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 285.000, steps: 947\n",
      "Episode 2: reward: 285.000, steps: 968\n",
      "Episode 3: reward: 285.000, steps: 975\n",
      "Episode 4: reward: 285.000, steps: 958\n",
      "Episode 5: reward: 285.000, steps: 983\n",
      "Episode 6: reward: 285.000, steps: 959\n",
      "Episode 7: reward: 285.000, steps: 974\n",
      "Episode 8: reward: 285.000, steps: 962\n",
      "Episode 9: reward: 285.000, steps: 983\n",
      "Episode 10: reward: 285.000, steps: 979\n",
      "285.0\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=total_test_episodes, visualize=True)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f932a32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('Save/Boltzmann/dqn.h5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36af2a41",
   "metadata": {},
   "source": [
    "Boltzmann policy selects an action stochastically with a probability generated by soft-maxing Q values   \n",
    "In this experiment, this resulted in the agent being satisfied with 285 points and failing to obtain the global optimal solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ed8581",
   "metadata": {},
   "source": [
    "### 6. How did you choose your decay rate and starting epsilon? Try at least one additional value for epsilon and the decay rate. How did it change the baseline performance? What is the value of epsilon when if you reach the max steps per episode?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532192e6",
   "metadata": {},
   "source": [
    "epsilon = 1.0\n",
    "min_epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8f56300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.9\n",
    "min_epsilon = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "63ec35cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=epsilon, value_min=min_epsilon, value_test=.2, nb_steps=max_steps)\n",
    "    memory = SequentialMemory(limit=1000, window_length=3)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  enable_dueling_network=True, dueling_type='avg', \n",
    "                   nb_actions=actions, nb_steps_warmup=100,\n",
    "                   gamma=Gamma\n",
    "                  )\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "558594d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a503b5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 3000 steps ...\n",
      "  650/3000: episode: 1, duration: 167.275s, episode steps: 650, steps per second:   4, episode reward: 50.000, mean reward:  0.077 [ 0.000, 20.000], mean action: 2.562 [0.000, 5.000],  loss: 24046628.195463, mean_q: 344.595063, mean_eps: 0.866625\n",
      " 1285/3000: episode: 2, duration: 215.114s, episode steps: 635, steps per second:   3, episode reward: 120.000, mean reward:  0.189 [ 0.000, 30.000], mean action: 2.567 [0.000, 5.000],  loss: 1.648278, mean_q: 8.818841, mean_eps: 0.813937\n",
      " 2249/3000: episode: 3, duration: 326.490s, episode steps: 964, steps per second:   3, episode reward: 315.000, mean reward:  0.327 [ 0.000, 30.000], mean action: 2.526 [0.000, 5.000],  loss: 2.441885, mean_q: 7.963550, mean_eps: 0.742782\n",
      " 2780/3000: episode: 4, duration: 180.468s, episode steps: 531, steps per second:   3, episode reward: 120.000, mean reward:  0.226 [ 0.000, 25.000], mean action: 2.426 [0.000, 5.000],  loss: 3.138482, mean_q: 7.990991, mean_eps: 0.676254\n",
      "done, took 964.782 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x160cbe47d88>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=total_episodes, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "251e8c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 15.000, steps: 441\n",
      "Episode 2: reward: 60.000, steps: 677\n",
      "Episode 3: reward: 400.000, steps: 1062\n",
      "Episode 4: reward: 35.000, steps: 477\n",
      "Episode 5: reward: 140.000, steps: 1140\n",
      "Episode 6: reward: 225.000, steps: 665\n",
      "Episode 7: reward: 25.000, steps: 676\n",
      "Episode 8: reward: 125.000, steps: 901\n",
      "Episode 9: reward: 20.000, steps: 371\n",
      "Episode 10: reward: 65.000, steps: 429\n",
      "111.0\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=total_test_episodes, visualize=True)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "31627959",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('Save/eps/dqn.h5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c476cd6",
   "metadata": {},
   "source": [
    "The larger the epsilon, the more the agent focuses on exploration. In this experiment, the changed epsilon performance is not ideal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcb5c2f",
   "metadata": {},
   "source": [
    "### 7. What is the average number of steps taken per episode? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3522c140",
   "metadata": {},
   "source": [
    "About 697 steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d3cf79",
   "metadata": {},
   "source": [
    "### 8. Does Q-learning use value-based or policy-based iteration? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28cf7e9",
   "metadata": {},
   "source": [
    "Q-learning use value-based iteration, The goal is to get a complete and reliable q table, so that the agent can do the action with the largest reward in current state.  \n",
    "policy-based algorithm directly models policy, gives the state, and obtains the action. It can output an action-dimensional discrete distribution from state, representing the probability of selecting each action, and agent can select the action according to this probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b131b6",
   "metadata": {},
   "source": [
    "### 9. Could you use SARSA for this problem? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "131a5bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Sagent(model, actions):\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=epsilon, value_min=min_epsilon, value_test=.2, nb_steps=max_steps)\n",
    "    saras = SARSAAgent(model=model, nb_actions=actions, policy=policy, test_policy=policy, \n",
    "                       gamma=Gamma, nb_steps_warmup=100, train_interval=1)\n",
    "    return saras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e82a7389",
   "metadata": {},
   "outputs": [],
   "source": [
    "saras = build_agent(model, actions)\n",
    "saras.compile(Adam(lr=learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e317f4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 3000 steps ...\n",
      "  821/3000: episode: 1, duration: 306.750s, episode steps: 821, steps per second:   3, episode reward: 225.000, mean reward:  0.274 [ 0.000, 30.000], mean action: 2.319 [0.000, 5.000],  loss: 754688.322928, mean_q: 48.860606\n",
      " 1658/3000: episode: 2, duration: 447.685s, episode steps: 837, steps per second:   2, episode reward: 210.000, mean reward:  0.251 [ 0.000, 30.000], mean action: 2.387 [0.000, 5.000],  loss: 1.199248, mean_q: 0.656727\n",
      " 2407/3000: episode: 3, duration: 398.170s, episode steps: 749, steps per second:   2, episode reward: 120.000, mean reward:  0.160 [ 0.000, 30.000], mean action: 2.479 [0.000, 5.000],  loss: 1.600340, mean_q: 1.552044\n",
      "done, took 1466.318 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x160cd399bc8>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saras.fit(env, nb_steps=total_episodes, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7d5d7ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 80.000, steps: 403\n",
      "Episode 2: reward: 105.000, steps: 663\n",
      "Episode 3: reward: 155.000, steps: 785\n",
      "Episode 4: reward: 105.000, steps: 815\n",
      "Episode 5: reward: 75.000, steps: 559\n",
      "Episode 6: reward: 155.000, steps: 827\n",
      "Episode 7: reward: 70.000, steps: 666\n",
      "Episode 8: reward: 550.000, steps: 1064\n",
      "Episode 9: reward: 65.000, steps: 666\n",
      "Episode 10: reward: 105.000, steps: 563\n",
      "146.5\n"
     ]
    }
   ],
   "source": [
    "scores = saras.test(env, nb_episodes=total_test_episodes, visualize=True)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7a83ed30",
   "metadata": {},
   "outputs": [],
   "source": [
    "saras.save_weights('Save/saras/saras.h5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e981cfa3",
   "metadata": {},
   "source": [
    "Yes, I can. In this experiment, because of the small number of steps, there is no obvious gap between q-learning and saras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e34eeb9",
   "metadata": {},
   "source": [
    "### 10. What is meant by the expected lifetime value in the Bellman equation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaeede5",
   "metadata": {},
   "source": [
    "The state has its duration, and the expected reward of the action corresponding to each state is different"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf53400b",
   "metadata": {},
   "source": [
    "### 11. When would SARSA likely do better than Q-learning? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1ede5d",
   "metadata": {},
   "source": [
    "saras is more conservative and safe, so it is more suitable for some practical problems where failure will lead to loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975b9235",
   "metadata": {},
   "source": [
    "### 12. How does SARSA differ from Q-learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39341a6f",
   "metadata": {},
   "source": [
    "SARSA is an on-policy method, Q-Learning is an off-policy method, the learning of sarsa is conservative and robust, and each episode and each step of each episode will perform episilon-greedy exploration; q-learning tends to To use the accumulation of experience to learn the optimal strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e309ccc",
   "metadata": {},
   "source": [
    "### 13. Explain the Q-learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bdbc4c",
   "metadata": {},
   "source": [
    "Maintain a q table , which records all combinations of states and actions. First, we randomly initialize a Q table, and then arbitrarily initialize a state s. The agent uses the ε-greedy algorithm to select the action a corresponding to the state s according to the Q table. Because the agent makes an action, it will get a reward r from the environment. When the change occurs, the agent observes a new state s ′, according to the row of the Q table in the state s ′, query the Q table, obtain the maximum value, and then update the Q table according to the formula."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8705460c",
   "metadata": {},
   "source": [
    "### 14. Explain the SARSA algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda019f0",
   "metadata": {},
   "source": [
    "An Episode starts with a random selection of the first state s1. And select action a1 in the state based on the ε-greedy strategy. After the first step is executed, observe the next state s2, and immediately get the immediate reward r2 of s2. At this point, again based on the ε-greedy policy, action a2 is selected in state s2. After getting a2, update the Q function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eeeb16",
   "metadata": {},
   "source": [
    "### 15. What code is yours and what have you adapted?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd4429a",
   "metadata": {},
   "source": [
    "### And 16. Did I explain my code clearly?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f370e7",
   "metadata": {},
   "source": [
    "My code based on following references. I changed some of them to make it more in line with this assignment, rewrote and added some about experiment and question.  \n",
    "\n",
    "<b>References:<b>  \n",
    "[1] : https://www.youtube.com/watch?v=Mut_u40Sqz4   \n",
    "[2] : https://en.wikipedia.org/wiki/Bellman_equation   \n",
    "[3] : https://zhuanlan.zhihu.com/p/41840804/   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d74c0fe",
   "metadata": {},
   "source": [
    "### 17. Did I explain my licensing clearly?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835d568c",
   "metadata": {},
   "source": [
    "Copyright <2022> Peichen Han\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684488b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
