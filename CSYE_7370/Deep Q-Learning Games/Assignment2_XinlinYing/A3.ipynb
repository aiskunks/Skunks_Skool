{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3972c82a",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88c204da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import cv2\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476330cc",
   "metadata": {},
   "source": [
    "## Set Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b05317aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box([[[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]], [[[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]], (210, 160, 3), uint8)\n",
      "Discrete(6)\n",
      "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "ENVIRONMENT = \"PongDeterministic-v0\"\n",
    "temp_env = gym.make(ENVIRONMENT)\n",
    "\n",
    "print(temp_env.observation_space)\n",
    "print(temp_env.action_space)\n",
    "print(temp_env.env.get_action_meanings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf05f5e8",
   "metadata": {},
   "source": [
    "## Enviroment Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16063012",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SAVE_MODELS = True  # Save models to file so you can test later\n",
    "MODEL_PATH = \"./models/optimal/LRandDR/pong-LRandDR-\"  # Models path for saving or loading\n",
    "SAVE_MODEL_INTERVAL = 10  # Save models at every X epoch\n",
    "TRAIN_MODEL = True  # Train model while playing (Make it False when testing a model)\n",
    "\n",
    "LOAD_MODEL_FROM_FILE = False  # Load model from file\n",
    "LOAD_FILE_EPISODE = 285  # Load Xth episode from file\n",
    "\n",
    "BATCH_SIZE = 64  # Minibatch size that select randomly from mem for train nets\n",
    "\n",
    "MAX_EPISODE = 450  # Max episode\n",
    "MAX_STEP = 10000  # Max step size for one episode\n",
    "\n",
    "MAX_MEMORY_LEN = 40000  # Max memory len\n",
    "MIN_MEMORY_LEN = 6000  # Min memory len before start train\n",
    "\n",
    "GAMMA = 0.97  \n",
    "ALPHA = 0.00025 \n",
    "EPSILON_DECAY = 0.99  # Epsilon decay rate by step\n",
    "# EPSILON_DECAY = 1  # Epsilon decay rate by step\n",
    "\n",
    "RENDER_GAME_WINDOW = True  # Opens a new window to render the game (Won't work on colab default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe6ff5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN with Duel Algo. https://arxiv.org/abs/1511.06581\n",
    "    \"\"\"\n",
    "    def __init__(self, h, w, output_size):\n",
    "        super(DuelCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=4,  out_channels=32, kernel_size=8, stride=4)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        convw, convh = self.conv2d_size_calc(w, h, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        convw, convh = self.conv2d_size_calc(convw, convh, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        convw, convh = self.conv2d_size_calc(convw, convh, kernel_size=3, stride=1)\n",
    "\n",
    "        linear_input_size = convw * convh * 64  # Last conv layer's out sizes\n",
    "\n",
    "        # Action layer\n",
    "        self.Alinear1 = nn.Linear(in_features=linear_input_size, out_features=128)\n",
    "        self.Alrelu = nn.LeakyReLU()  # Linear 1 activation funct\n",
    "        self.Alinear2 = nn.Linear(in_features=128, out_features=output_size)\n",
    "\n",
    "        # State Value layer\n",
    "        self.Vlinear1 = nn.Linear(in_features=linear_input_size, out_features=128)\n",
    "        self.Vlrelu = nn.LeakyReLU()  # Linear 1 activation funct\n",
    "        self.Vlinear2 = nn.Linear(in_features=128, out_features=1)  # Only 1 node\n",
    "\n",
    "    def conv2d_size_calc(self, w, h, kernel_size=5, stride=2):\n",
    "        \"\"\"\n",
    "        Calcs conv layers output image sizes\n",
    "        \"\"\"\n",
    "        next_w = (w - (kernel_size - 1) - 1) // stride + 1\n",
    "        next_h = (h - (kernel_size - 1) - 1) // stride + 1\n",
    "        return next_w, next_h\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flatten every batch\n",
    "\n",
    "        Ax = self.Alrelu(self.Alinear1(x))\n",
    "        Ax = self.Alinear2(Ax)  # No activation on last layer\n",
    "\n",
    "        Vx = self.Vlrelu(self.Vlinear1(x))\n",
    "        Vx = self.Vlinear2(Vx)  # No activation on last layer\n",
    "\n",
    "        q = Vx + (Ax - Ax.mean())\n",
    "\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58f43ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, environment):\n",
    "        \"\"\"\n",
    "        Hyperparameters definition for Agent\n",
    "        \"\"\"\n",
    "        # State size for breakout env. SS images (210, 160, 3). Used as input size in network\n",
    "        self.state_size_h = environment.observation_space.shape[0]\n",
    "        self.state_size_w = environment.observation_space.shape[1]\n",
    "        self.state_size_c = environment.observation_space.shape[2]\n",
    "\n",
    "        # Activation size for breakout env. Used as output size in network\n",
    "        self.action_size = environment.action_space.n\n",
    "\n",
    "        # Image pre process params\n",
    "        self.target_h = 80  # Height after process\n",
    "        self.target_w = 64  # Widht after process\n",
    "\n",
    "        self.crop_dim = [20, self.state_size_h, 0, self.state_size_w]  # Cut 20 px from top to get rid of the score table\n",
    "\n",
    "        # Trust rate to our experiences\n",
    "        self.gamma = GAMMA  # Discount coef for future predictions\n",
    "        self.alpha = ALPHA  # Learning Rate\n",
    "\n",
    "        # After many experinces epsilon will be 0.05\n",
    "        # So we will do less Explore more Exploit\n",
    "        self.epsilon = 1  # Explore or Exploit\n",
    "        self.epsilon_decay = EPSILON_DECAY  # Adaptive Epsilon Decay Rate\n",
    "        self.epsilon_minimum = 0.05  # Minimum for Explore\n",
    "\n",
    "        # Deque holds replay mem.\n",
    "        self.memory = deque(maxlen=MAX_MEMORY_LEN)\n",
    "\n",
    "        # Create two model for DDQN algorithm\n",
    "        self.online_model = DuelCNN(h=self.target_h, w=self.target_w, output_size=self.action_size).to(DEVICE)\n",
    "        self.target_model = DuelCNN(h=self.target_h, w=self.target_w, output_size=self.action_size).to(DEVICE)\n",
    "        self.target_model.load_state_dict(self.online_model.state_dict())\n",
    "        self.target_model.eval()\n",
    "\n",
    "        # Adam used as optimizer\n",
    "        self.optimizer = optim.Adam(self.online_model.parameters(), lr=self.alpha)\n",
    "\n",
    "    def preProcess(self, image):\n",
    "        \"\"\"\n",
    "        Process image crop resize, grayscale and normalize the images\n",
    "        \"\"\"\n",
    "        frame = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # To grayscale\n",
    "        frame = frame[self.crop_dim[0]:self.crop_dim[1], self.crop_dim[2]:self.crop_dim[3]]  # Cut 20 px from top\n",
    "        frame = cv2.resize(frame, (self.target_w, self.target_h))  # Resize\n",
    "        frame = frame.reshape(self.target_w, self.target_h) / 255  # Normalize\n",
    "\n",
    "        return frame\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Get state and do action\n",
    "        Two option can be selectedd if explore select random action\n",
    "        if exploit ask nnet for action\n",
    "        \"\"\"\n",
    "\n",
    "        act_protocol = 'Explore' if random.uniform(0, 1) <= self.epsilon else 'Exploit'\n",
    "\n",
    "        if act_protocol == 'Explore':\n",
    "            action = random.randrange(self.action_size)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(state, dtype=torch.float, device=DEVICE).unsqueeze(0)\n",
    "                q_values = self.online_model.forward(state)  # (1, action_size)\n",
    "                action = torch.argmax(q_values).item()  # Returns the indices of the maximum value of all elements\n",
    "\n",
    "        return action\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train neural nets with replay memory\n",
    "        returns loss and max_q val predicted from online_net\n",
    "        \"\"\"\n",
    "        if len(agent.memory) < MIN_MEMORY_LEN:\n",
    "            loss, max_q = [0, 0]\n",
    "            return loss, max_q\n",
    "        # We get out minibatch and turn it to numpy array\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.memory, BATCH_SIZE))\n",
    "\n",
    "        # Concat batches in one array\n",
    "        # (np.arr, np.arr) ==> np.BIGarr\n",
    "        state = np.concatenate(state)\n",
    "        next_state = np.concatenate(next_state)\n",
    "\n",
    "        # Convert them to tensors\n",
    "        state = torch.tensor(state, dtype=torch.float, device=DEVICE)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float, device=DEVICE)\n",
    "        action = torch.tensor(action, dtype=torch.long, device=DEVICE)\n",
    "        reward = torch.tensor(reward, dtype=torch.float, device=DEVICE)\n",
    "        done = torch.tensor(done, dtype=torch.float, device=DEVICE)\n",
    "\n",
    "        # Make predictions\n",
    "        state_q_values = self.online_model(state)\n",
    "        next_states_q_values = self.online_model(next_state)\n",
    "        next_states_target_q_values = self.target_model(next_state)\n",
    "\n",
    "        # Find selected action's q_value\n",
    "        selected_q_value = state_q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        # Get indice of the max value of next_states_q_values\n",
    "        # Use that indice to get a q_value from next_states_target_q_values\n",
    "        # We use greedy for policy So it called off-policy\n",
    "        next_states_target_q_value = next_states_target_q_values.gather(\n",
    "            1, next_states_q_values.max(1)[1].unsqueeze(1)).squeeze(1)\n",
    "        # Use Bellman function to find expected q value\n",
    "        expected_q_value = reward + self.gamma * next_states_target_q_value * (1 - done)\n",
    "\n",
    "        # Calc loss with expected_q_value and q_value\n",
    "        loss = (selected_q_value - expected_q_value.detach()).pow(2).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss, torch.max(state_q_values).item()\n",
    "\n",
    "    def storeResults(self, state, action, reward, nextState, done):\n",
    "        \"\"\"\n",
    "        Store every result to memory\n",
    "        \"\"\"\n",
    "        self.memory.append([state[None, :], action, reward, nextState[None, :], done])\n",
    "\n",
    "    def adaptiveEpsilon(self):\n",
    "        \"\"\"\n",
    "        Adaptive Epsilon means every step\n",
    "        we decrease the epsilon so we do less Explore\n",
    "        \"\"\"\n",
    "        if self.epsilon > self.epsilon_minimum:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e7cc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:53] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Time:03:58:21 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-21.000 Avg_Max_Q:0.000 Epsilon:0.91 Duration:2.48 Step:945 CStep:947\n",
      "Episode:2 Time:03:58:24 Reward:-20.00 Loss:0.00 Last_100_Avg_Rew:-20.500 Avg_Max_Q:0.000 Epsilon:0.83 Duration:2.86 Step:927 CStep:1875\n",
      "Episode:3 Time:03:58:27 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.667 Avg_Max_Q:0.000 Epsilon:0.77 Duration:3.28 Step:823 CStep:2699\n",
      "Episode:4 Time:03:58:31 Reward:-21.00 Loss:0.00 Last_100_Avg_Rew:-20.750 Avg_Max_Q:0.000 Epsilon:0.70 Duration:3.61 Step:824 CStep:3524\n",
      "Episode:5 Time:03:58:36 Reward:-20.00 Loss:0.00 Last_100_Avg_Rew:-20.600 Avg_Max_Q:0.000 Epsilon:0.64 Duration:4.70 Step:915 CStep:4440\n",
      "Episode:6 Time:03:58:40 Reward:-20.00 Loss:0.00 Last_100_Avg_Rew:-20.500 Avg_Max_Q:0.000 Epsilon:0.59 Duration:4.75 Step:841 CStep:5282\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    environment = gym.make(ENVIRONMENT)  # Get env\n",
    "    agent = Agent(environment)  # Create Agent\n",
    "\n",
    "    if LOAD_MODEL_FROM_FILE:\n",
    "        agent.online_model.load_state_dict(torch.load(MODEL_PATH+str(LOAD_FILE_EPISODE)+\".pkl\"))\n",
    "\n",
    "        with open(MODEL_PATH+str(LOAD_FILE_EPISODE)+'.json') as outfile:\n",
    "            param = json.load(outfile)\n",
    "            agent.epsilon = param.get('epsilon')\n",
    "\n",
    "        startEpisode = LOAD_FILE_EPISODE + 1\n",
    "\n",
    "    else:\n",
    "        startEpisode = 1\n",
    "\n",
    "    last_100_ep_reward = deque(maxlen=100)  # Last 100 episode rewards\n",
    "    total_step = 1  # Cumulkative sum of all steps in episodes\n",
    "    for episode in range(startEpisode, MAX_EPISODE):\n",
    "\n",
    "        startTime = time.time()  # Keep time\n",
    "        state = environment.reset()  # Reset env\n",
    "\n",
    "        state = agent.preProcess(state)  # Process image\n",
    "\n",
    "        # Stack state . Every state contains 4 time contionusly frames\n",
    "        # We stack frames like 4 channel image\n",
    "        state = np.stack((state, state, state, state))\n",
    "\n",
    "        total_max_q_val = 0  # Total max q vals\n",
    "        total_reward = 0  # Total reward for each episode\n",
    "        total_loss = 0  # Total loss for each episode\n",
    "\n",
    "        for step in range(MAX_STEP):\n",
    "\n",
    "            if RENDER_GAME_WINDOW:\n",
    "                environment.render()  # Show state visually\n",
    "\n",
    "            # Select and perform an action\n",
    "            action = agent.act(state)  # Act\n",
    "            next_state, reward, done, info = environment.step(action)  # Observe\n",
    "\n",
    "            next_state = agent.preProcess(next_state)  # Process image\n",
    "\n",
    "            # Stack state . Every state contains 4 time contionusly frames\n",
    "            # We stack frames like 4 channel image\n",
    "            next_state = np.stack((next_state, state[0], state[1], state[2]))\n",
    "\n",
    "            # Store the transition in memory\n",
    "            agent.storeResults(state, action, reward, next_state, done)  # Store to mem\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state  # Update state\n",
    "\n",
    "            if TRAIN_MODEL:\n",
    "                # Perform one step of the optimization (on the target network)\n",
    "                loss, max_q_val = agent.train()  # Train with random BATCH_SIZE state taken from mem\n",
    "            else:\n",
    "                loss, max_q_val = [0, 0]\n",
    "\n",
    "            total_loss += loss\n",
    "            total_max_q_val += max_q_val\n",
    "            total_reward += reward\n",
    "            total_step += 1\n",
    "            if total_step % 100 == 0:\n",
    "                agent.adaptiveEpsilon()  # Decrase epsilon\n",
    "                \n",
    "            if done:  # Episode completed\n",
    "                currentTime = time.time()  # Keep current time\n",
    "                time_passed = currentTime - startTime  # Find episode duration\n",
    "                current_time_format = time.strftime(\"%H:%M:%S\", time.gmtime())  # Get current dateTime as HH:MM:SS\n",
    "                epsilonDict = {'epsilon': agent.epsilon}  # Create epsilon dict to save model as file\n",
    "\n",
    "                if SAVE_MODELS and episode % SAVE_MODEL_INTERVAL == 0:  # Save model as file\n",
    "                    weightsPath = MODEL_PATH + str(episode) + '.pkl'\n",
    "                    epsilonPath = MODEL_PATH + str(episode) + '.json'\n",
    "\n",
    "                    torch.save(agent.online_model.state_dict(), weightsPath)\n",
    "                    with open(epsilonPath, 'w') as outfile:\n",
    "                        json.dump(epsilonDict, outfile)\n",
    "\n",
    "                if TRAIN_MODEL:\n",
    "                    agent.target_model.load_state_dict(agent.online_model.state_dict())  # Update target model\n",
    "\n",
    "                last_100_ep_reward.append(total_reward)\n",
    "                avg_max_q_val = total_max_q_val / step\n",
    "\n",
    "                outStr = \"Episode:{} Time:{} Reward:{:.2f} Loss:{:.2f} Last_100_Avg_Rew:{:.3f} Avg_Max_Q:{:.3f} Epsilon:{:.2f} Duration:{:.2f} Step:{} CStep:{}\".format(\n",
    "                    episode, current_time_format, total_reward, total_loss, np.mean(last_100_ep_reward)\n",
    "                    , avg_max_q_val, agent.epsilon, time_passed, step, total_step\n",
    "                )\n",
    "\n",
    "                print(outStr)\n",
    "\n",
    "#                 if SAVE_MODELS:\n",
    "#                     outputPath = MODEL_PATH + \"out\" + '.txt'  # Save outStr to file\n",
    "#                     with open(outputPath, 'a') as outfile:\n",
    "#                         outfile.write(outStr+\"\\n\")\n",
    "\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fea7be",
   "metadata": {},
   "source": [
    "## 1. Establish a baseline performance. How well did your Deep Q-learning do on your problem? (5 Points)\n",
    "For example\n",
    "\n",
    "tMAX_EPISODE = 450  \n",
    "MAX_STEP = 100000  \n",
    "\n",
    "MAX_MEMORY_LEN = 50000  \n",
    "MIN_MEMORY_LEN = 40000  \n",
    "\n",
    "GAMMA = 0.97  \n",
    "ALPHA = 0.00025 \n",
    "EPSILON_DECAY = 0.99  \n",
    "With this baseline performance, our RL program with the Taxi-v2 Toy text gives us a score of 8.15 which is not bad.\n",
    "\n",
    "## 2. What are the states, the actions, and the size of the Q-table? (5 Points)\n",
    "n the environment, the states are RGB images of the screen, which is an array of shape (210,160,3). The images are selected every 4 frame during the episode.\n",
    "\n",
    "The actions are in range [0,1,2,3,4,5], which has the corresponding names ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']. In which, 0 & 1 are staying, 2 & 4 make the green player go up, and 3 & 5 make the green player go down.\n",
    "\n",
    "## 3. What are the rewards? Why did you choose them? (5 Points)\n",
    " \n",
    "In the Pong game, the reward is calculated by the score of my player minus the score of other player, in range[-21,21]. The advantage of the reward is easy to calculate and process. Because there is only one round in one episode, the reward will not be averaged. I choose to calculate the average reward in last 100 rounds to be a stable reference.\n",
    "## 4. How did you choose alpha and gamma in the Bellman equation? Try at least one additional value for alpha and gamma. How did it change the baseline performance?  (5 Points)\n",
    "Alpha is the learning rate that decides how important the new, learned Q-value is. The bigger the alpha is, the more percentage the new Q-value takes.\n",
    "Gamma is the discount factor that decides how important the future Q-value is. The bigger the gamma is, the future optimal Q-value is more important.\n",
    "Firstly, I chose a different alpha to try with a smaller learning rate.:\n",
    "\n",
    "GAMMA = 0.8  # Old Discount rate\n",
    "ALPHA = 0.00025  # New Learning rate\n",
    "\n",
    "\n",
    "Through the experiment, I changed the learning rate from 0.7 to 0.00025. The smaller learning rate performs quicker and better learning result. According to the Bellman equation, I think it is because the small learning rate could make the model converge quickly and stablly.\n",
    "\n",
    "Then I choose a different gamma:\n",
    "\n",
    "GAMMA = 0.97  # New Discount rate\n",
    "ALPHA = 0.00025  # New Learning rate\n",
    "\n",
    "\n",
    "## 5. Try a policy other than e-greedy. How did it change the baseline performance? (5 Points)\n",
    " Within about 400 episodes, the ramdom sampling policy with the above optimal parameters is similar to the baseline performance with e-greed. But according to the average rewards of last 100 episodes, ramdom sampling is -20.34 while the baseline is -20.44, which is 0.5% better than the baseline performance with e-greedy.\n",
    "\n",
    "## 6. How did you choose your decay rate and starting epsilon? Try at least one additional value for epsilon and the decay rate. How did it change the baseline performance? What is the value of epsilon when if you reach the max steps per episode? (5 Points)\n",
    "\n",
    "Epsilon is the hyperparameter of e-greedy algorithm. It will decide the ratio between exploration and exploidation. The bigger epsilon is, the more exploration the program does. In the beginning, the agent knows nothing about the program, so it should do more exploration to find the proper actions. After training for a while, the agent has been more confident to choose correct action in a state, so the epsilon should be decayed to make the agent do more exploidation. In the tail of training, there should be a small epsilon to ensure the program will do what it think correct at most of time but try something new sometime.\n",
    "\n",
    "## 7. What is the average number of steps taken per episode? (5 Points)\n",
    "\n",
    "The steps stop counting when the game is over, so with the training, the game last longer, the average steps also will be more. These are the steps of 440 episodes with the optimal parameters and the average steps of the 440 episodes is 1858.814. We could see that although steps are not stable, the trend of average steps are obviously growing.\n",
    "\n",
    "## 8. Does Q-learning use value-based or policy-based iteration? (5 Points)\n",
    "Explain, not a yes or no question. \n",
    "\n",
    "Q-learning is a values-based learning algorithm. Value based algorithms updates the value function based on an equation(particularly Bellman equation). Whereas the other type, policy-based estimates the value function with a greedy policy obtained from the last policy improvement.\n",
    "\n",
    "## 9. Could you use SARSA for this problem? (5 Points)\n",
    "Yes\n",
    "SARSA algorithm is a slight variation of the popular Q-Learning algorithm. For a learning agent in any Reinforcement Learning algorithm it’s policy can be of two types:- \n",
    "\n",
    "On Policy: In this, the learning agent learns the value function according to the current action derived from the policy currently being used.\n",
    "\n",
    "Off Policy: In this, the learning agent learns the value function according to the action derived from another policy.\n",
    "\n",
    "Q-Learning technique is an Off Policy technique and uses the greedy approach to learn the Q-value. SARSA technique, on the other hand, is an On Policy and uses the action performed by the current policy to learn the Q-value.\n",
    "\n",
    " \n",
    "## 10. What is meant by the expected lifetime value in the Bellman equation?(5 Points)\n",
    "The expected lifetime related to the value of being in a state to the utility return in the state and expected change from moving to another state.\n",
    "\n",
    "## 11. When would SARSA likely do better than Q-learning? (5 Points)\n",
    "If your goal is to train an optimal agent in simulation, or in a low-cost and fast-iterating environment, then Q-learning is a good choice, due to the first point (learning optimal policy directly). If your agent learns online, and you care about rewards gained whilst learning, then SARSA may be a better choice.\n",
    "\n",
    "SARSA will approach convergence allowing for possible penalties from exploratory moves, whilst Q-learning will ignore them. That makes SARSA more conservative - if there is risk of a large negative reward close to the optimal path, Q-learning will tend to trigger that reward whilst exploring, whilst SARSA will tend to avoid a dangerous optimal path and only slowly learn to use it when the exploration parameters are reduced.\n",
    "\n",
    " \n",
    "## 12. How does SARSA differ from Q-learning? (5 Points)  \n",
    "SARSA algorithm is a slight variation of the popular Q-Learning algorithm. For a learning agent in any Reinforcement Learning algorithm it’s policy can be of two types:- \n",
    "\n",
    "On Policy: In this, the learning agent learns the value function according to the current action derived from the policy currently being used.\n",
    "\n",
    "Off Policy: In this, the learning agent learns the value function according to the action derived from another policy.\n",
    "\n",
    "Q-Learning technique is an Off Policy technique and uses the greedy approach to learn the Q-value. SARSA technique, on the other hand, is an On Policy and uses the action performed by the current policy to learn the Q-value.\n",
    " \n",
    "\n",
    "## 13. Explain the Q-learning algorithm. (5 Points)  \n",
    "Q-learning does not use the behaviour policy to select an additional action At+1. Instead, it estimates the expected future returns in the update rule as maxA Q(St+1, A). The max operator used here can be viewed as \"following\" the completely greedy policy. \n",
    "\n",
    "Q(st, at) = Q(st, at) + α*(rt + γ*maxa Q(st+1, a) - Q(st, at))\n",
    " \n",
    "\n",
    "## 14. Explain the SARSA algorithm. (5 Points)  \n",
    "Details including pseudocode and math. \n",
    "\n",
    "Sarsa uses the behaviour policy (meaning, the policy used by the agent to generate experience in the environment, which is typically epsilon-greedy) to select an additional action At+1, and then uses Q(St+1, At+1) (discounted by gamma) as expected future returns in the computation of the update target.\n",
    "\n",
    "Q(st, at) = Q(st, at) + α*(rt + γ*Q(st+1, at+1) - Q(st, at))\n",
    " \n",
    "\n",
    "## 15. What code is yours and what have you adapted? (5 Points)\n",
    "I use the implementation by Behçet Şentürk in GitHub (https://github.com/bhctsntrk/OpenAIPong-DQN), changed the parameters to adapt the questions and run for different groups of parameters. \n",
    "  \n",
    "\n",
    "## 17. Did I explain my licensing clearly? (5 Points)\n",
    "\n",
    "copyright (c) 2022, Xinlin Ying All rights reserved.\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    " \n",
    "\n",
    "## 18. Professionalism (10 Points)\n",
    "\n",
    "Variable naming, style guide, conduct, behavior, and attitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accf2c32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
