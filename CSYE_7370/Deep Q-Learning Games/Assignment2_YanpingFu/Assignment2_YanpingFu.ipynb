{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klGNgWREsvQv"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_12_04_atari.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmDI-h7cI0tI"
      },
      "source": [
        "# Deep Q-Learning with an Atari-Phoenix game(OpenAI gym environment)\n",
        "\n",
        "<img src='https://www.gymlibrary.dev/_images/phoenix.gif'>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Install and Import libraries"
      ],
      "metadata": {
        "id": "172x34yvkViM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KQhYThvTCQC"
      },
      "outputs": [],
      "source": [
        "# HIDE OUTPUT\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    %tensorflow_version 2.x\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False\n",
        "\n",
        "if COLAB:\n",
        "  !sudo apt-get install -y xvfb ffmpeg\n",
        "  !pip install -q ale-py\n",
        "  !pip install -q 'gym==0.17.3'\n",
        "  !pip install -q 'imageio==2.4.0'\n",
        "  !pip install -q PILLOW\n",
        "  !pip install -q 'pyglet==1.3.2'\n",
        "  !pip install -q pyvirtualdisplay\n",
        "  !pip install -q --upgrade tensorflow-probability\n",
        "  !pip install -q 'tf-agents==0.12.0'\n",
        "  !pip install -q keras-rl2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLAgRd3k1ujn"
      },
      "outputs": [],
      "source": [
        "\n",
        "  !pip install  tensorflow==2.10.0 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMitx5qSgJk1"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Set up a virtual display for rendering OpenAI gym environments.\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWCaUF9mU53o"
      },
      "outputs": [],
      "source": [
        "# HIDE OUTPUT\n",
        "! wget http://www.atarimania.com/roms/Roms.rar\n",
        "! mkdir /content/ROM/\n",
        "! unrar e -o+ /content/Roms.rar /content/ROM/\n",
        "! python -m atari_py.import_roms /content/ROM/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Atari environment"
      ],
      "metadata": {
        "id": "ACrvlXvPkohg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iWbHx6xYeGr"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "env = gym.make(\"Phoenix-v4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIHYVBkuvPNw"
      },
      "source": [
        "We can now reset the environment and display one step.  The following image shows how the Pong game environment appears to a user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlO7WIQHu_7D",
        "outputId": "dd472cd2-a7d6-4948-cdcb-9f8b6cc2ca30"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        ...,\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0]],\n",
              "\n",
              "       [[  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        ...,\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0]],\n",
              "\n",
              "       [[  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        ...,\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[146,  70, 192],\n",
              "        [146,  70, 192],\n",
              "        [146,  70, 192],\n",
              "        ...,\n",
              "        [146,  70, 192],\n",
              "        [146,  70, 192],\n",
              "        [146,  70, 192]],\n",
              "\n",
              "       [[146,  70, 192],\n",
              "        [146,  70, 192],\n",
              "        [146,  70, 192],\n",
              "        ...,\n",
              "        [146,  70, 192],\n",
              "        [146,  70, 192],\n",
              "        [146,  70, 192]],\n",
              "\n",
              "       [[146,  70, 192],\n",
              "        [146,  70, 192],\n",
              "        [146,  70, 192],\n",
              "        ...,\n",
              "        [146,  70, 192],\n",
              "        [146,  70, 192],\n",
              "        [146,  70, 192]]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "env.reset()\n",
        "#PIL.Image.fromarray(env.render())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UwYqMZA26iR",
        "outputId": "f038e2f5-273c-4018-9061-75ed9e27d966"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(210, 160, 3)\n"
          ]
        }
      ],
      "source": [
        "height, width, channels = env.observation_space.shape\n",
        "actions = env.action_space.n\n",
        "print(env.observation_space.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create a Deep Learning Model with Keras"
      ],
      "metadata": {
        "id": "uhVF2oIck6uc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBpUE2hr3aIz"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten,Convolution2D\n",
        "from keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKylDsfU3s-_"
      },
      "outputs": [],
      "source": [
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy, BoltzmannQPolicy\n",
        "from rl.memory import SequentialMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQOCBTKu3mQJ",
        "outputId": "531247dc-ffb4-4f97-967c-14939b6cb96d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 3, 51, 39, 32)     6176      \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 3, 24, 18, 64)     32832     \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 3, 22, 16, 64)     36928     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 67584)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               34603520  \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 8)                 2056      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 34,812,840\n",
            "Trainable params: 34,812,840\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Training parameters.\n",
        "time_limit = True\n",
        "buffer_size = 200000  # observation history size\n",
        "batch_size = 25  # mini batch size sampled from history at each update step\n",
        "nb_actions = env.action_space.n\n",
        "window_length = 3\n",
        "\n",
        "# construct a MLP\n",
        "model = Sequential()\n",
        "model.add(Convolution2D(32, (8,8), strides=(4,4), activation='relu', input_shape=(3,height, width, channels)))\n",
        "model.add(Convolution2D(64, (4,4), strides=(2,2), activation='relu'))\n",
        "model.add(Convolution2D(64, (3,3), activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(actions, activation='linear'))\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Build Agent with Keras-RL and training"
      ],
      "metadata": {
        "id": "joNl7xRslBLu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zg-529dU5B02",
        "outputId": "4493e77e-2431-434b-b149-aa8193d1caf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# keras-rl2 objects\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.1, nb_steps=10000)\n",
        "memory = SequentialMemory(limit=1000, window_length=3)\n",
        "\n",
        "dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
        "                  enable_dueling_network=True, dueling_type='avg', gamma=.8,\n",
        "                   nb_actions=actions, nb_steps_warmup=1000\n",
        "                  )\n",
        "\n",
        "dqn.compile(\n",
        "    Adam(lr=1e-3),\n",
        "    metrics=['mse']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HKHooJI5F0e",
        "outputId": "e0bc7a94-67ae-4897-ec2f-529617310304"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 2500 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  250/2500: episode: 1, duration: 6.421s, episode steps: 250, steps per second:  39, episode reward: 140.000, mean reward:  0.560 [ 0.000, 80.000], mean action: 3.444 [0.000, 7.000],  loss: --, mse: --, mean_q: --, mean_eps: --\n",
            "  500/2500: episode: 2, duration: 3.747s, episode steps: 250, steps per second:  67, episode reward: 120.000, mean reward:  0.480 [ 0.000, 20.000], mean action: 3.548 [0.000, 7.000],  loss: --, mse: --, mean_q: --, mean_eps: --\n",
            "  750/2500: episode: 3, duration: 3.758s, episode steps: 250, steps per second:  67, episode reward: 120.000, mean reward:  0.480 [ 0.000, 20.000], mean action: 3.524 [0.000, 7.000],  loss: --, mse: --, mean_q: --, mean_eps: --\n",
            " 1000/2500: episode: 4, duration: 3.726s, episode steps: 250, steps per second:  67, episode reward: 80.000, mean reward:  0.320 [ 0.000, 20.000], mean action: 3.540 [0.000, 7.000],  loss: --, mse: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1250/2500: episode: 5, duration: 213.460s, episode steps: 250, steps per second:   1, episode reward: 100.000, mean reward:  0.400 [ 0.000, 20.000], mean action: 3.340 [0.000, 7.000],  loss: 218.717224, mse: 443.726453, mean_q: 5.425571, mean_eps: 0.898750\n",
            " 1500/2500: episode: 6, duration: 213.051s, episode steps: 250, steps per second:   1, episode reward: 60.000, mean reward:  0.240 [ 0.000, 20.000], mean action: 3.508 [0.000, 7.000],  loss: 0.386837, mse: 5.785922, mean_q: 3.098195, mean_eps: 0.876295\n",
            " 1750/2500: episode: 7, duration: 213.145s, episode steps: 250, steps per second:   1, episode reward: 200.000, mean reward:  0.800 [ 0.000, 80.000], mean action: 3.672 [0.000, 7.000],  loss: 0.473153, mse: 5.293258, mean_q: 3.027168, mean_eps: 0.853795\n",
            " 2000/2500: episode: 8, duration: 213.026s, episode steps: 250, steps per second:   1, episode reward: 260.000, mean reward:  1.040 [ 0.000, 80.000], mean action: 3.660 [0.000, 7.000],  loss: 3.300629, mse: 8.600682, mean_q: 3.572834, mean_eps: 0.831295\n",
            " 2250/2500: episode: 9, duration: 213.044s, episode steps: 250, steps per second:   1, episode reward: 280.000, mean reward:  1.120 [ 0.000, 80.000], mean action: 3.620 [0.000, 7.000],  loss: 3.463873, mse: 8.688869, mean_q: 3.191821, mean_eps: 0.808795\n",
            " 2500/2500: episode: 10, duration: 213.300s, episode steps: 250, steps per second:   1, episode reward: 260.000, mean reward:  1.040 [ 0.000, 80.000], mean action: 3.368 [0.000, 7.000],  loss: 3.391259, mse: 13.818214, mean_q: 3.512779, mean_eps: 0.786295\n",
            "done, took 1296.692 seconds\n"
          ]
        }
      ],
      "source": [
        "\n",
        "history = dqn.fit(env, \n",
        "                  nb_steps=2500,\n",
        "                  visualize=False,nb_max_episode_steps=250,\n",
        "                  verbose=2) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssAnznFMcZdU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac4833e9-17c7-440c-a1c4-6b80898b07c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: 120.000, steps: 589\n",
            "Episode 2: reward: 200.000, steps: 1154\n",
            "Episode 3: reward: 200.000, steps: 853\n",
            "Episode 4: reward: 200.000, steps: 2589\n",
            "Episode 5: reward: 140.000, steps: 866\n",
            "172.0\n"
          ]
        }
      ],
      "source": [
        "scores = dqn.test(env, nb_episodes=5, visualize=True)\n",
        "print(np.mean(scores.history['episode_reward']))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The baseline performance show us a mean reward with 172.0 score."
      ],
      "metadata": {
        "id": "Eo4r0yO7mKP9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train agent with different learning rate and discount rate\n",
        "\n",
        "According to the baseline with:<br>\n",
        "max_steps_per_episode = 250 <br>\n",
        "learning_rate = 0.001 <br>\n",
        "discount_rate = 0.8 <br>\n",
        "exploration_rate = 1 <br>\n",
        "max_exploration_rate = 1 <br>\n",
        "min_exploration_rate = 0.1 <br>\n",
        "exploration_decay_rate = 0.1<br>\n",
        "\n",
        "Here i try to change learning rate and discount rate to observe how they change the baseline."
      ],
      "metadata": {
        "id": "HPMbXcq793yf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# decrease learning rate α from 0.001 to 0.0001 \n",
        "# increase discount rate gamma to 0.99\n",
        "dqn1 = DQNAgent(model=model, memory=memory, policy=policy,\n",
        "                  enable_dueling_network=True, dueling_type='avg', gamma=.99,\n",
        "                   nb_actions=actions, nb_steps_warmup=1000\n",
        "                  )\n",
        "\n",
        "dqn1.compile(\n",
        "    Adam(lr=1e-4),\n",
        "    metrics=['mse']\n",
        ")"
      ],
      "metadata": {
        "id": "uOtlJ8P794DH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1cabc5a-ef12-4f90-fe63-d01fdfbee10c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = dqn1.fit(env, \n",
        "                  nb_steps=2500,\n",
        "                  visualize=False,nb_max_episode_steps=250,\n",
        "                  verbose=2) "
      ],
      "metadata": {
        "id": "FsjkMsZqAjk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ac3380f-f093-47f2-a2cf-37244d182a4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 2500 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  250/2500: episode: 1, duration: 3.905s, episode steps: 250, steps per second:  64, episode reward: 100.000, mean reward:  0.400 [ 0.000, 20.000], mean action: 3.248 [0.000, 7.000],  loss: --, mse: --, mean_q: --, mean_eps: --\n",
            "  500/2500: episode: 2, duration: 3.745s, episode steps: 250, steps per second:  67, episode reward: 140.000, mean reward:  0.560 [ 0.000, 20.000], mean action: 3.512 [0.000, 7.000],  loss: --, mse: --, mean_q: --, mean_eps: --\n",
            "  750/2500: episode: 3, duration: 3.750s, episode steps: 250, steps per second:  67, episode reward: 100.000, mean reward:  0.400 [ 0.000, 20.000], mean action: 3.544 [0.000, 7.000],  loss: --, mse: --, mean_q: --, mean_eps: --\n",
            " 1000/2500: episode: 4, duration: 3.772s, episode steps: 250, steps per second:  66, episode reward: 160.000, mean reward:  0.640 [ 0.000, 80.000], mean action: 3.676 [0.000, 7.000],  loss: --, mse: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1250/2500: episode: 5, duration: 213.420s, episode steps: 250, steps per second:   1, episode reward: 160.000, mean reward:  0.640 [ 0.000, 80.000], mean action: 3.668 [0.000, 7.000],  loss: 5.460064, mse: 7.197431, mean_q: 3.189197, mean_eps: 0.898750\n",
            " 1500/2500: episode: 6, duration: 213.254s, episode steps: 250, steps per second:   1, episode reward: 180.000, mean reward:  0.720 [ 0.000, 80.000], mean action: 3.580 [0.000, 7.000],  loss: 1.685999, mse: 12.501783, mean_q: 4.530695, mean_eps: 0.876295\n",
            " 1750/2500: episode: 7, duration: 213.356s, episode steps: 250, steps per second:   1, episode reward: 160.000, mean reward:  0.640 [ 0.000, 80.000], mean action: 3.616 [0.000, 7.000],  loss: 1.481246, mse: 11.871580, mean_q: 4.641760, mean_eps: 0.853795\n",
            " 2000/2500: episode: 8, duration: 213.297s, episode steps: 250, steps per second:   1, episode reward: 100.000, mean reward:  0.400 [ 0.000, 20.000], mean action: 3.520 [0.000, 7.000],  loss: 0.582249, mse: 10.588105, mean_q: 4.353719, mean_eps: 0.831295\n",
            " 2250/2500: episode: 9, duration: 213.468s, episode steps: 250, steps per second:   1, episode reward: 160.000, mean reward:  0.640 [ 0.000, 80.000], mean action: 3.356 [0.000, 7.000],  loss: 0.779440, mse: 8.472813, mean_q: 4.041728, mean_eps: 0.808795\n",
            " 2500/2500: episode: 10, duration: 213.288s, episode steps: 250, steps per second:   1, episode reward: 140.000, mean reward:  0.560 [ 0.000, 20.000], mean action: 3.580 [0.000, 7.000],  loss: 1.211585, mse: 10.896612, mean_q: 4.400304, mean_eps: 0.786295\n",
            "done, took 1295.272 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = dqn1.test(env, nb_episodes=5, visualize=True)\n",
        "print(np.mean(scores.history['episode_reward']))"
      ],
      "metadata": {
        "id": "VJ-I0edUAnbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4d2d4b2-f13a-496c-c6b2-9eb71d731ccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: 180.000, steps: 1714\n",
            "Episode 2: reward: 180.000, steps: 1025\n",
            "Episode 3: reward: 80.000, steps: 1422\n",
            "Episode 4: reward: 180.000, steps: 911\n",
            "Episode 5: reward: 120.000, steps: 3292\n",
            "148.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Try policy \n"
      ],
      "metadata": {
        "id": "ZDabEC-iAsmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy1 = BoltzmannQPolicy() \n",
        "\n",
        "dqn2 = DQNAgent(model=model, memory=memory, policy=policy1,\n",
        "                  enable_dueling_network=True, dueling_type='avg', gamma=.8,\n",
        "                   nb_actions=actions, nb_steps_warmup=1000\n",
        "                  )\n",
        "\n",
        "dqn2.compile(\n",
        "    Adam(lr=1e-3),\n",
        "    metrics=['mse']\n",
        ")"
      ],
      "metadata": {
        "id": "l-OqnGw-AwIm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d4f75cb-f864-4e11-c3f0-d514e4eb7961"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = dqn2.fit(env, \n",
        "                  nb_steps=2500,\n",
        "                  visualize=False,nb_max_episode_steps=250,\n",
        "                  verbose=2) "
      ],
      "metadata": {
        "id": "19-gYzS6BSbI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b9ce49e-a570-4c61-de46-ebce32be7b31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 2500 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  250/2500: episode: 1, duration: 4.054s, episode steps: 250, steps per second:  62, episode reward: 80.000, mean reward:  0.320 [ 0.000, 80.000], mean action: 2.408 [1.000, 7.000],  loss: --, mse: --, mean_q: --\n",
            "  500/2500: episode: 2, duration: 3.764s, episode steps: 250, steps per second:  66, episode reward: 120.000, mean reward:  0.480 [ 0.000, 80.000], mean action: 2.560 [1.000, 7.000],  loss: --, mse: --, mean_q: --\n",
            "  750/2500: episode: 3, duration: 3.745s, episode steps: 250, steps per second:  67, episode reward: 80.000, mean reward:  0.320 [ 0.000, 20.000], mean action: 2.292 [1.000, 7.000],  loss: --, mse: --, mean_q: --\n",
            " 1000/2500: episode: 4, duration: 3.782s, episode steps: 250, steps per second:  66, episode reward: 80.000, mean reward:  0.320 [ 0.000, 20.000], mean action: 2.576 [1.000, 7.000],  loss: --, mse: --, mean_q: --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1250/2500: episode: 5, duration: 213.712s, episode steps: 250, steps per second:   1, episode reward: 80.000, mean reward:  0.320 [ 0.000, 20.000], mean action: 3.672 [0.000, 7.000],  loss: 3.793703, mse: 10.484817, mean_q: 4.007106\n",
            " 1500/2500: episode: 6, duration: 213.280s, episode steps: 250, steps per second:   1, episode reward: 140.000, mean reward:  0.560 [ 0.000, 80.000], mean action: 3.632 [0.000, 7.000],  loss: 4.798661, mse: 14.739688, mean_q: 4.098798\n",
            " 1750/2500: episode: 7, duration: 213.365s, episode steps: 250, steps per second:   1, episode reward: 160.000, mean reward:  0.640 [ 0.000, 80.000], mean action: 3.528 [0.000, 7.000],  loss: 1.130640, mse: 11.936115, mean_q: 4.057529\n",
            " 2000/2500: episode: 8, duration: 213.371s, episode steps: 250, steps per second:   1, episode reward: 60.000, mean reward:  0.240 [ 0.000, 20.000], mean action: 3.768 [0.000, 7.000],  loss: 5.852178, mse: 17.140491, mean_q: 4.561198\n",
            " 2250/2500: episode: 9, duration: 213.434s, episode steps: 250, steps per second:   1, episode reward: 180.000, mean reward:  0.720 [ 0.000, 80.000], mean action: 3.736 [0.000, 7.000],  loss: 4.718197, mse: 13.837645, mean_q: 4.588183\n",
            " 2500/2500: episode: 10, duration: 213.363s, episode steps: 250, steps per second:   1, episode reward: 180.000, mean reward:  0.720 [ 0.000, 80.000], mean action: 3.688 [0.000, 7.000],  loss: 4.555697, mse: 16.567358, mean_q: 4.655103\n",
            "done, took 1295.881 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = dqn2.test(env, nb_episodes=5, visualize=True)\n",
        "print(np.mean(scores.history['episode_reward']))"
      ],
      "metadata": {
        "id": "aeGRR1fIBT9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16945641-9656-425d-a4a5-758b0ea1c1b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: 0.000, steps: 533\n",
            "Episode 2: reward: 0.000, steps: 530\n",
            "Episode 3: reward: 0.000, steps: 778\n",
            "Episode 4: reward: 0.000, steps: 578\n",
            "Episode 5: reward: 0.000, steps: 539\n",
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Different staring Epsilon"
      ],
      "metadata": {
        "id": "aBQLvnE1AwxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# change starting epsilon to 0.05 \n",
        "policy2 = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.05, value_test=.05, nb_steps=10000)\n",
        "\n",
        "dqn3 = DQNAgent(model=model, memory=memory, policy=policy2,\n",
        "                  enable_dueling_network=True, dueling_type='avg', gamma=.8,\n",
        "                   nb_actions=actions, nb_steps_warmup=1000\n",
        "                  )\n",
        "\n",
        "dqn3.compile(\n",
        "    Adam(lr=1e-3),\n",
        "    metrics=['mse']\n",
        ")"
      ],
      "metadata": {
        "id": "FkCVYVtFA-qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = dqn3.fit(env, \n",
        "                  nb_steps=2500,\n",
        "                  visualize=False,nb_max_episode_steps=250,\n",
        "                  verbose=2) "
      ],
      "metadata": {
        "id": "G_Q-yJq6BeMP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18386d82-db5e-4235-98e8-781987e4953c"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training for 2500 steps ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  250/2500: episode: 1, duration: 6.701s, episode steps: 250, steps per second:  37, episode reward: 120.000, mean reward:  0.480 [ 0.000, 20.000], mean action: 3.272 [0.000, 7.000],  loss: --, mse: --, mean_q: --, mean_eps: --\n",
            "  500/2500: episode: 2, duration: 3.957s, episode steps: 250, steps per second:  63, episode reward: 240.000, mean reward:  0.960 [ 0.000, 80.000], mean action: 3.192 [0.000, 7.000],  loss: --, mse: --, mean_q: --, mean_eps: --\n",
            "  750/2500: episode: 3, duration: 3.860s, episode steps: 250, steps per second:  65, episode reward: 200.000, mean reward:  0.800 [ 0.000, 80.000], mean action: 3.516 [0.000, 7.000],  loss: --, mse: --, mean_q: --, mean_eps: --\n",
            " 1000/2500: episode: 4, duration: 3.852s, episode steps: 250, steps per second:  65, episode reward: 100.000, mean reward:  0.400 [ 0.000, 20.000], mean action: 3.476 [0.000, 7.000],  loss: --, mse: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1250/2500: episode: 5, duration: 213.947s, episode steps: 250, steps per second:   1, episode reward: 80.000, mean reward:  0.320 [ 0.000, 20.000], mean action: 3.636 [0.000, 7.000],  loss: 583.913172, mse: 1486.837378, mean_q: 22.096355, mean_eps: 0.893125\n",
            " 1500/2500: episode: 6, duration: 212.958s, episode steps: 250, steps per second:   1, episode reward: 140.000, mean reward:  0.560 [ 0.000, 80.000], mean action: 3.584 [0.000, 7.000],  loss: 2.191753, mse: 232.099916, mean_q: 18.585354, mean_eps: 0.869422\n",
            " 1750/2500: episode: 7, duration: 213.040s, episode steps: 250, steps per second:   1, episode reward: 360.000, mean reward:  1.440 [ 0.000, 80.000], mean action: 3.736 [0.000, 7.000],  loss: 1.964644, mse: 229.178007, mean_q: 18.293737, mean_eps: 0.845673\n",
            " 2000/2500: episode: 8, duration: 213.092s, episode steps: 250, steps per second:   1, episode reward: 80.000, mean reward:  0.320 [ 0.000, 20.000], mean action: 3.476 [0.000, 7.000],  loss: 2.063146, mse: 228.746888, mean_q: 18.347254, mean_eps: 0.821923\n",
            " 2250/2500: episode: 9, duration: 212.918s, episode steps: 250, steps per second:   1, episode reward: 140.000, mean reward:  0.560 [ 0.000, 20.000], mean action: 3.772 [0.000, 7.000],  loss: 0.861966, mse: 228.381488, mean_q: 17.976203, mean_eps: 0.798173\n",
            " 2500/2500: episode: 10, duration: 212.994s, episode steps: 250, steps per second:   1, episode reward: 140.000, mean reward:  0.560 [ 0.000, 80.000], mean action: 3.428 [0.000, 7.000],  loss: 1.156776, mse: 228.552413, mean_q: 17.985240, mean_eps: 0.774423\n",
            "done, took 1297.333 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = dqn3.test(env, nb_episodes=5, visualize=True)\n",
        "print(np.mean(scores.history['episode_reward']))"
      ],
      "metadata": {
        "id": "QXNAOYH6BfHw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fb35a88-9027-4f39-cd40-65f3116655dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: 340.000, steps: 1102\n",
            "Episode 2: reward: 1340.000, steps: 1630\n",
            "Episode 3: reward: 1550.000, steps: 2314\n",
            "Episode 4: reward: 540.000, steps: 2042\n",
            "Episode 5: reward: 1030.000, steps: 1440\n",
            "960.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Anser Question:\n",
        "1. **Baseline performance and how well dqn on this atari game** <br>\n",
        "  **A:** max_steps_per_episode = 250,\n",
        "  learning_rate = 0.001,\n",
        "    discount_rate = 0.8,\n",
        "    exploration_rate = 1,\n",
        "    max_exploration_rate = 1,\n",
        "    min_exploration_rate = 0.1,\n",
        "    exploration_decay_rate = 0.1\n",
        "\n",
        "2. **What are the states, the actions, and the size of the Q-table? **<br>\n",
        "  **A:** Here state is 3 stacked consecutive frames from the environment. Actino of this atari environment like: NOOP, FIRE, RIGHT, LEFT, DOWN, RIGHTFIRE, LEFTFIRE, DOWNFIRE. Because DQL use a Neural Network that takes a state and approximates the Q-values for each action based on that state instead of using a Q-table, cannot answer size of q-table.\n",
        "\n",
        "3. **What are the rewards? Why did you choose them? **<br>\n",
        "  **A:** We choose the game score as th reward because the game used scoring system in single digit can be handled more convinient.\n",
        "\n",
        "4. **How did you choose alpha and gamma in the Bellman equation? Try at least one additional value for alpha and gamma. How did it change the baseline performance?**<br>\n",
        "  **A:** Lower gamma values will put more weight on short-term gains, whereas higher gamma values will put more weight towards long-term gains.So for continuous tasks, the discount factor should be as close to 1 as possible (e.g., γ=0.99) to avoid neglecting future rewards.<br> \n",
        "  The learning rate hyperparameter controls the rate or speed at which the model learns.  Generally, a large learning rate allows the model to learn faster, at the cost of arriving on a sub-optimal final set of weights. A smaller learning rate may allow the model to learn a more optimal or even globally optimal set of weights but may take significantly longer to train. <br>\n",
        "  In this environment, I set the baseline: alpha be 0.001 and gamma is 0.8. and the mean reward of this baseline is 172.0. After change the value of alpha and gamma to be 0.0001 and 0.99, mean reward decrease to 148.0 which is not good as baseline. <br>\n",
        "\n",
        "\n",
        "\n",
        "5. **Try a policy other than e-greedy. How did it change the baseline performance?**<br>\n",
        "  **A:** Here i try BoltzmannQPolicy. The Boltzmann exploration policy is intended for discrete action spaces. It assumes that each of the possible actions has some value assigned to it (such as the Q value), and uses a softmax function to convert these values into a distribution over the actions. It then samples the action for playing out of the calculated distribution.<br> It underperform baseline. \n",
        "\n",
        "6. **How did you choose your decay rate and starting epsilon? Try at least one additional value for epsilon and the decay rate. How did it change the baseline performance? What is the value of epsilon when if you reach the max steps per episode?**<br>\n",
        "  **A:** First i set epsilon to be 0.1 in baseline, and after decrease it and decay rate to be 0.05, the model perform better than baseline with mean reward 960.0. \n",
        "\n",
        "7. **What is the average number of steps taken per episode?** <br>\n",
        "  **A:**The average number of steps taken per episode is 250.\n",
        "\n",
        "8. **Does Q-learning use value-based or policy-based iteration?**<br>\n",
        "  **A:** Q-learning is a values-based learning algorithm. Value based algorithms updates the value function based on an equation(particularly Bellman equation).\n",
        "\n",
        "9. **Could you use SARSA for this problem?**<br>\n",
        "  **A:** State–action–reward–state–action (SARSA) is an algorithm for learning a Markov decision process policy, used in the reinforcement learning area of machine learning. \n",
        "It is a slight variation of the popular Q-Learning algorithm. The difference between these two algorithms is that SARSA chooses an action following the same current policy and updates its Q-values whereas Q-learning chooses the greedy action, that is, the action that gives the maximum Q-value for the state, that is, it follows an optimal policy.\n",
        "QL is a more aggressive agent, while SARSA is more conservative. In this low-cost and fast-iterating atari gym environment, mistakes are not costly like the unexpected minimal failure of robots, so I prefer to use Deep Q learning but not SARSA in this problem.\n",
        "[1]\n",
        "10. **What is meant by the expected lifetime value in the Bellman equation?**<br>\n",
        "  **A:**Discount factor\n",
        "\n",
        "11. **When would SARSA likely do better than Q-learning?**<br>\n",
        "  **A:** As we mentioned above, SARSA is more conservative, it will approach convergence allowing for possible penalties from exploratory moves. If mistakes are costly( like the unexpected minimal failure- of robots ) in our environment and we care about rewards gained while learning, then SARSA likely does better than Q-learning.\n",
        "\n",
        "12. **How does SARSA differ from Q-learning?** <br>\n",
        "  **A:** Q-Learning technique is an Off Policy technique and uses the greedy approach to learn the Q-value. SARSA technique, on the other hand, is an On Policy and uses the action performed by the current policy to learn the Q-value.\n",
        "\n",
        "13. **Explain the Q-learning algorithm.**<br>\n",
        "  **A:** Algorithm: <img src='https://wikimedia.org/api/rest_v1/media/math/render/svg/7c8c6f219d5ceabd052cb058a5135bfdac86dc0c'><br>\n",
        "  Before learning begins, Q is initialized to a possibly arbitrary fixed value (chosen by the programmer). Then, at each time t the agent selects an action at, observes a reward rt, enters a new state st+1 (that may depend on both the previous state st and the selected action), and Q is updated.\n",
        "  <img src='https://tcnguyen.github.io/reinforcement_learning/images/Q_learning_algo.png'>\n",
        "\n",
        "14. **Explain the SARSA algorithm.**<br>\n",
        "  **A:** In SARSA, this is done by choosing another action a′ following the same current policy above and using 'equation' as target. \n",
        "SARSA is called on-policy learning because new action a′ is chosen using the same epsilon-greedy policy as the action a, the one that generated s′.<br>\n",
        "   Algorithm: <img src='https://wikimedia.org/api/rest_v1/media/math/render/svg/04c4392b9a682a765571d992e8df82edc808a305'><br>\n",
        "   The Q value for a state-action is updated by an error, adjusted by the learning rate alpha. Q values represent the possible reward received in the next time step for taking action a in state s, plus the discounted future reward received from the next state-action observation.\n",
        "  <img src='https://tcnguyen.github.io/reinforcement_learning/images/SARSA_algo.png'>\n",
        "\n",
        "15. **What code is yours and what have you adapted?**<br>\n",
        "  **A:** The code import atari gym environment, create model with tensorflow and train agent with keras-rl2 are cited form [2]. And modified some parameters to answer questions.\n",
        "\n",
        "#### Reference\n",
        "[1] https://medium.com/swlh/introduction-to-reinforcement-learning-coding-sarsa-part-4-2d64d6e37617<br>\n",
        "[2] https://github.com/nicknochnack/KerasRL-OpenAI-Atari-SpaceInvadersv0/blob/main/Space%20Invaders%20Walkthrough.ipynb\n",
        "\n",
        "#### Licence\n",
        "Copyright (c) 2022, Yanping Fu All rights reserved.\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F15mkasZyJfD"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3.9 (tensorflow)",
      "language": "python",
      "name": "tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}