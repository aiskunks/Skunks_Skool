{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment2 Deep Q-Learning with an Atari-like game-Hao Fu\n",
    "\n",
    "Use the OpenAI Gym environment \"PongDeterministic-v0\" and \"Deterministic\" means that the program uses fixed sampling, every 4 frames, and \"v0\" means that the environment has 25% probability to take the last action again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.Establish a baseline performance. How well did your Deep Q-learning do on your problem?**\n",
    "\n",
    "Choose 5000 as my total_episodes, 100000 as my max_steps, because the time of one round is a little long than other environments.\n",
    "\n",
    "MAX_EPISODE = 5000  # Max episode\n",
    "\n",
    "MAX_STEP = 100000  # Max step size for one episode\n",
    "\n",
    "MAX_MEMORY_LEN = 50000  # Max memory len\n",
    "\n",
    "MIN_MEMORY_LEN = 40000  # Min memory len before start train\n",
    "\n",
    "GAMMA = 0.8  # Discount rate\n",
    "\n",
    "ALPHA = 0.7  # Learning rate\n",
    "\n",
    "EPSILON_DECAY = 0.99  # Epsilon decay rate by step\n",
    "\n",
    "After 420 training episodes, I draw the rewards of each round (the blue line) and the average rewards (the red line). Through the diagram we can see that there is hardly an improvement of the performance.\n",
    "\n",
    "<img src=\"images/r-baseline.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.What are the states, the actions, and the size of the Q-table?**\n",
    "\n",
    "The states are RGB images of the screen, which is an array of shape (210,160,3). The images are selected every 4 frame during the episode.\n",
    "\n",
    "The actions are in range [0,1,2,3,4,5], which has the corresponding names ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']. In which, 0 & 1 are staying, 2 & 4 make the green player go up, and 3 & 5 make the green player go down.\n",
    "\n",
    "The Q-table is based on the number of distinct states(S) and actions(A), whose size is the number of S * the number of A. But in the game, states are 210 * 160 pixels image, each pixel has 128 possible color. So Q-table is not suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.What are the rewards? Why did you choose them?**\n",
    "\n",
    "The reward is calculated by the score of my player minus the score of other player, in range[-21,21]. Because there is only one round in one episode, the reward will not be averaged. I choose to calculate the average reward in last 100 rounds to be a stable reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.How did you choose alpha and gamma in the Bellman equation? Try at least one additional value for alpha and gamma. How did it change the baseline performance?**\n",
    "\n",
    "Alpha is the learning rate that decides how important the new, learned Q-value is. The bigger the alpha is, the more percentage the new Q-value takes.Gamma is the discount factor that decides how important the future Q-value is. Firstly, alpha.\n",
    "\n",
    "\n",
    "GAMMA = 0.8  # Old Discount rate\n",
    "\n",
    "#ALPHA = 0.7  # Old Learning rate\n",
    "\n",
    "ALPHA = 0.00025  # New Learning rate\n",
    "\n",
    "<img src=\"images/r-learning.png\"></img>\n",
    "\n",
    "Through the experiment, I changed the learning rate from 0.7 to 0.00025. According to the Bellman equation, I think it is because the small learning rate could make the model converge quickly and stablly.\n",
    "\n",
    "Then I choose a different gamma:\n",
    "\n",
    "#GAMMA = 0.8  # Old Discount rate\n",
    "\n",
    "GAMMA = 0.97  # New Discount rate\n",
    "\n",
    "#ALPHA = 0.7  # Old Learning rate\n",
    "\n",
    "ALPHA = 0.00025  # New Learning rate\n",
    "\n",
    "<img src=\"images/r-optimal.png\"></img>\n",
    "\n",
    "Through the experiment, I changed the discount rate from 0.8 to 0.97. The bigger discount rate performs a more stable and better learning result. According to the Bellman equation, I think it is because the big discount rate could consider the future rewards more and have a longer sightview."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.Try a policy other than e-greedy. How did it change the baseline performance?**\n",
    "\n",
    "Try random sampling.\n",
    "\n",
    "<img src=\"images/r-random.png\"></img>\n",
    "\n",
    "Within about 400 episodes, the ramdom sampling policy with the above optimal parameters is similar to the baseline performance with e-greed. But according to the average rewards of last 100 episodes, ramdom sampling is 0.5% better than the baseline performance with e-greedy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.How did you choose your decay rate and starting epsilon? Try at least one additional value for epsilon and the decay rate. How did it change the baseline performance? What is the value of epsilon when if you reach the max steps per episode?**\n",
    "\n",
    "Epsilon is the hyperparameter of e-greedy algorithm. It will decide the ratio between exploration and exploidation. The bigger epsilon is, the more exploration the program does. In the beginning, the agent knows nothing about the program, so it should do more exploration to find the proper actions. After training for a while, the agent has been more confident to choose correct action in a state, so the epsilon should be decayed to make the agent do more exploidation. In the tail of training, there should be a small epsilon to ensure the program will do what it think correct at most of time but try something new sometime.\n",
    "\n",
    "I tried to make epsilon always be 1 to use the random sampling policy. It shows that the program appears improvement more quickly, but hardly to converge.\n",
    "\n",
    "In the tail of training, the epsilon will decay step-by-step, normally approach at the min_epsilon that is set before the program. In my program, I choose 0.05 as the final epsilon to make the program have enough posibility to improve itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.What is the average number of steps taken per episode?**\n",
    "\n",
    "The steps stop counting when the game is over, so with the training, the game last longer, the average steps also will be more. These are the steps of 440 episodes with the optimal parameters. The trend of average steps are obviously growing.\n",
    "\n",
    "<img src=\"images/steps.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.Does Q-learning use value-based or policy-based iteration?**\n",
    "\n",
    "Q-learning is a value-based iteration that uses the bellman equation to compute the optimal MDP policy and its value. In a value-based approach, updating the value function is repeated until it finds the optimal value function. This is also the basic idea of Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9.Could you use SARSA for this problem?**\n",
    "\n",
    "I can use SARSA. Because SARSA is also with Q-table and with e-greedy policy. The only way different from Q-learning is the approach to update Q-value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10.What is meant by the expected lifetime value in the Bellman equation?**\n",
    "\n",
    "<img src=\"images/bellman.png\" width=40%></img>\n",
    "\n",
    "The expected lifetime Q-value (in the left of equals sign) in the state s with the action a is the total max discounted reward that could be got, which is calculated by the sum of immediate reward and the result of the discount factor gamma multiplied by each max discoounted future reward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11.When would SARSA likely do better than Q-learning?**\n",
    "\n",
    "If there is a large negative reward close to the optimal path, Q-learning will tend to get that SARSA is more conservative in comparison. So if people want to use the model while training or care about the loss of fault, SARSA will be better than Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12.How does SARSA differ from Q-learning?**\n",
    "\n",
    "<img src=\"images/compare.png\" width=50%></img>\n",
    "\n",
    "Q-learning chooses the action with the largest Q-value after calculating and comparing all the actions, but SARSA will not change its mind and choose the action following its policy (e-greedy).\n",
    "\n",
    "The Q-learning algorithm, first assumes the action with the maximum reward is selected in the next step. Then the action is selected by the ε-greedy strategy. SARSA algorithm, first confirms the action through an ε-greedy policy and then updates the Q-value according to the executed action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**13.Explain the Q-learning algorithm.**\n",
    "\n",
    "<img src=\"images/Q_learning_algo.png\" width=50%></img>\n",
    "\n",
    "Q-learning is an off-policy algorithm. It will build a Q-Table with states as the row and actions as the column, and then update the Q-Table by the rewards in each step to choose the action with max total discounted rewards. In each step, it first assumes the action with the maximum reward is selected in the next step and updates the Q-value. Then the action is selected by the ε-greedy strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**14.Explain the SARSA algorithm.**\n",
    "\n",
    "<img src=\"images/SARSA_algo.png\" width=50%></img>\n",
    "\n",
    "SARSA uses Q-table and e-greedy policy. But it is an on-policy algorithm with e-greedy policy. In each step, it first confirms the action through an ε-greedy policy and then updates the Q-value according to the executed action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**15.What code is yours and what have you adapted?**\n",
    "\n",
    "I use the implementation in GitHub (https://github.com/bhctsntrk/OpenAIPong-DQN), changed the parameters to adapt the questions and have many parameters. And write a small Python script to deal with the output data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**16.Did I explain my licensing clearly?**\n",
    "\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2022 Hao Fu\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code\n",
    "import gym\n",
    "import cv2\n",
    "\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVIRONMENT = \"PongDeterministic-v0\"\n",
    "temp_env = gym.make(ENVIRONMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp_env.observation_space)\n",
    "print(temp_env.action_space)\n",
    "print(temp_env.env.get_action_meanings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SAVE_MODELS = True  # Save models to file so you can test later\n",
    "MODEL_PATH = \"./models/pong-random-\"  # Models path for saving or loading\n",
    "SAVE_MODEL_INTERVAL = 10  # Save models at every X epoch\n",
    "TRAIN_MODEL = True  # Train model while playing (Make it False when testing a model)\n",
    "\n",
    "LOAD_MODEL_FROM_FILE = False  # Load model from file\n",
    "LOAD_FILE_EPISODE = 285  # Load Xth episode from file\n",
    "\n",
    "BATCH_SIZE = 64  # Minibatch size that select randomly from mem for train nets\n",
    "# MAX_EPISODE = 5000  # Max episode\n",
    "MAX_EPISODE = 100000  # Max episode\n",
    "MAX_STEP = 100000  # Max step size for one episode\n",
    "\n",
    "MAX_MEMORY_LEN = 50000  # Max memory len\n",
    "MIN_MEMORY_LEN = 40000  # Min memory len before start train\n",
    "\n",
    "# GAMMA = 0.8  # Old Discount rate\n",
    "GAMMA = 0.97  # New Discount rate\n",
    "# ALPHA = 0.7  # Old Learning rate\n",
    "ALPHA = 0.00025  # New Learning rate\n",
    "# EPSILON_DECAY = 0.99  # Epsilon decay rate by step\n",
    "EPSILON_DECAY = 1  # Epsilon decay rate by step\n",
    "\n",
    "RENDER_GAME_WINDOW = True  # Opens a new window to render the game (Won't work on colab default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN with Duel Algo. https://arxiv.org/abs/1511.06581\n",
    "    \"\"\"\n",
    "    def __init__(self, h, w, output_size):\n",
    "        super(DuelCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=4,  out_channels=32, kernel_size=8, stride=4)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        convw, convh = self.conv2d_size_calc(w, h, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        convw, convh = self.conv2d_size_calc(convw, convh, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        convw, convh = self.conv2d_size_calc(convw, convh, kernel_size=3, stride=1)\n",
    "\n",
    "        linear_input_size = convw * convh * 64  # Last conv layer's out sizes\n",
    "\n",
    "        # Action layer\n",
    "        self.Alinear1 = nn.Linear(in_features=linear_input_size, out_features=128)\n",
    "        self.Alrelu = nn.LeakyReLU()  # Linear 1 activation funct\n",
    "        self.Alinear2 = nn.Linear(in_features=128, out_features=output_size)\n",
    "\n",
    "        # State Value layer\n",
    "        self.Vlinear1 = nn.Linear(in_features=linear_input_size, out_features=128)\n",
    "        self.Vlrelu = nn.LeakyReLU()  # Linear 1 activation funct\n",
    "        self.Vlinear2 = nn.Linear(in_features=128, out_features=1)  # Only 1 node\n",
    "\n",
    "    def conv2d_size_calc(self, w, h, kernel_size=5, stride=2):\n",
    "        \"\"\"\n",
    "        Calcs conv layers output image sizes\n",
    "        \"\"\"\n",
    "        next_w = (w - (kernel_size - 1) - 1) // stride + 1\n",
    "        next_h = (h - (kernel_size - 1) - 1) // stride + 1\n",
    "        return next_w, next_h\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flatten every batch\n",
    "\n",
    "        Ax = self.Alrelu(self.Alinear1(x))\n",
    "        Ax = self.Alinear2(Ax)  # No activation on last layer\n",
    "\n",
    "        Vx = self.Vlrelu(self.Vlinear1(x))\n",
    "        Vx = self.Vlinear2(Vx)  # No activation on last layer\n",
    "\n",
    "        q = Vx + (Ax - Ax.mean())\n",
    "\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, environment):\n",
    "        \"\"\"\n",
    "        Hyperparameters definition for Agent\n",
    "        \"\"\"\n",
    "        # State size for breakout env. SS images (210, 160, 3). Used as input size in network\n",
    "        self.state_size_h = environment.observation_space.shape[0]\n",
    "        self.state_size_w = environment.observation_space.shape[1]\n",
    "        self.state_size_c = environment.observation_space.shape[2]\n",
    "\n",
    "        # Activation size for breakout env. Used as output size in network\n",
    "        self.action_size = environment.action_space.n\n",
    "\n",
    "        # Image pre process params\n",
    "        self.target_h = 80  # Height after process\n",
    "        self.target_w = 64  # Widht after process\n",
    "\n",
    "        self.crop_dim = [20, self.state_size_h, 0, self.state_size_w]  # Cut 20 px from top to get rid of the score table\n",
    "\n",
    "        # Trust rate to our experiences\n",
    "        self.gamma = GAMMA  # Discount coef for future predictions\n",
    "        self.alpha = ALPHA  # Learning Rate\n",
    "\n",
    "        # After many experinces epsilon will be 0.05\n",
    "        # So we will do less Explore more Exploit\n",
    "        self.epsilon = 1  # Explore or Exploit\n",
    "        self.epsilon_decay = EPSILON_DECAY  # Adaptive Epsilon Decay Rate\n",
    "        self.epsilon_minimum = 0.05  # Minimum for Explore\n",
    "\n",
    "        # Deque holds replay mem.\n",
    "        self.memory = deque(maxlen=MAX_MEMORY_LEN)\n",
    "\n",
    "        # Create two model for DDQN algorithm\n",
    "        self.online_model = DuelCNN(h=self.target_h, w=self.target_w, output_size=self.action_size).to(DEVICE)\n",
    "        self.target_model = DuelCNN(h=self.target_h, w=self.target_w, output_size=self.action_size).to(DEVICE)\n",
    "        self.target_model.load_state_dict(self.online_model.state_dict())\n",
    "        self.target_model.eval()\n",
    "        \n",
    "        # Adam used as optimizer\n",
    "        self.optimizer = optim.Adam(self.online_model.parameters(), lr=self.alpha)\n",
    "\n",
    "    def preProcess(self, image):\n",
    "        \"\"\"\n",
    "        Process image crop resize, grayscale and normalize the images\n",
    "        \"\"\"\n",
    "        frame = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # To grayscale\n",
    "        frame = frame[self.crop_dim[0]:self.crop_dim[1], self.crop_dim[2]:self.crop_dim[3]]  # Cut 20 px from top\n",
    "        frame = cv2.resize(frame, (self.target_w, self.target_h))  # Resize\n",
    "        frame = frame.reshape(self.target_w, self.target_h) / 255  # Normalize\n",
    "\n",
    "        return frame\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Get state and do action\n",
    "        Two option can be selectedd if explore select random action\n",
    "        if exploit ask nnet for action\n",
    "        \"\"\"\n",
    "\n",
    "        act_protocol = 'Explore' if random.uniform(0, 1) <= self.epsilon else 'Exploit'\n",
    "\n",
    "        if act_protocol == 'Explore':\n",
    "            action = random.randrange(self.action_size)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(state, dtype=torch.float, device=DEVICE).unsqueeze(0)\n",
    "                q_values = self.online_model.forward(state)  # (1, action_size)\n",
    "                action = torch.argmax(q_values).item()  # Returns the indices of the maximum value of all elements\n",
    "\n",
    "        return action\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train neural nets with replay memory\n",
    "        returns loss and max_q val predicted from online_net\n",
    "        \"\"\"\n",
    "        if len(agent.memory) < MIN_MEMORY_LEN:\n",
    "            loss, max_q = [0, 0]\n",
    "            return loss, max_q\n",
    "        # We get out minibatch and turn it to numpy array\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.memory, BATCH_SIZE))\n",
    "\n",
    "        # Concat batches in one array\n",
    "        # (np.arr, np.arr) ==> np.BIGarr\n",
    "        state = np.concatenate(state)\n",
    "        next_state = np.concatenate(next_state)\n",
    "\n",
    "        # Convert them to tensors\n",
    "        state = torch.tensor(state, dtype=torch.float, device=DEVICE)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float, device=DEVICE)\n",
    "        action = torch.tensor(action, dtype=torch.long, device=DEVICE)\n",
    "        reward = torch.tensor(reward, dtype=torch.float, device=DEVICE)\n",
    "        done = torch.tensor(done, dtype=torch.float, device=DEVICE)\n",
    "\n",
    "        # Make predictions\n",
    "        state_q_values = self.online_model(state)\n",
    "        next_states_q_values = self.online_model(next_state)\n",
    "        next_states_target_q_values = self.target_model(next_state)\n",
    "\n",
    "        # Find selected action's q_value\n",
    "        selected_q_value = state_q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        # Get indice of the max value of next_states_q_values\n",
    "        # Use that indice to get a q_value from next_states_target_q_values\n",
    "        # We use greedy for policy So it called off-policy\n",
    "        next_states_target_q_value = next_states_target_q_values.gather(\n",
    "            1, next_states_q_values.max(1)[1].unsqueeze(1)).squeeze(1)\n",
    "        # Use Bellman function to find expected q value\n",
    "        expected_q_value = reward + self.gamma * next_states_target_q_value * (1 - done)\n",
    "\n",
    "        # Calc loss with expected_q_value and q_value\n",
    "        loss = (selected_q_value - expected_q_value.detach()).pow(2).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss, torch.max(state_q_values).item()\n",
    "\n",
    "    def storeResults(self, state, action, reward, nextState, done):\n",
    "        \"\"\"\n",
    "        Store every result to memory\n",
    "        \"\"\"\n",
    "        self.memory.append([state[None, :], action, reward, nextState[None, :], done])\n",
    "\n",
    "    def adaptiveEpsilon(self):\n",
    "        \"\"\"\n",
    "        Adaptive Epsilon means every step\n",
    "        we decrease the epsilon so we do less Explore\n",
    "        \"\"\"\n",
    "        if self.epsilon > self.epsilon_minimum:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    environment = gym.make(ENVIRONMENT)  # Get env\n",
    "    agent = Agent(environment)  # Create Agent\n",
    "\n",
    "    if LOAD_MODEL_FROM_FILE:\n",
    "        agent.online_model.load_state_dict(torch.load(MODEL_PATH+str(LOAD_FILE_EPISODE)+\".pkl\"))\n",
    "\n",
    "        with open(MODEL_PATH+str(LOAD_FILE_EPISODE)+'.json') as outfile:\n",
    "            param = json.load(outfile)\n",
    "            agent.epsilon = param.get('epsilon')\n",
    "\n",
    "        startEpisode = LOAD_FILE_EPISODE + 1\n",
    "\n",
    "    else:\n",
    "        startEpisode = 1\n",
    "\n",
    "    last_100_ep_reward = deque(maxlen=100)  # Last 100 episode rewards\n",
    "    total_step = 1  # Cumulkative sum of all steps in episodes\n",
    "    for episode in range(startEpisode, MAX_EPISODE):\n",
    "\n",
    "        startTime = time.time()  # Keep time\n",
    "        state = environment.reset()  # Reset env\n",
    "\n",
    "        state = agent.preProcess(state)  # Process image\n",
    "\n",
    "        # Stack state . Every state contains 4 time contionusly frames\n",
    "        # We stack frames like 4 channel image\n",
    "        state = np.stack((state, state, state, state))\n",
    "\n",
    "        total_max_q_val = 0  # Total max q vals\n",
    "        total_reward = 0  # Total reward for each episode\n",
    "        total_loss = 0  # Total loss for each episode\n",
    "        for step in range(MAX_STEP):\n",
    "\n",
    "            if RENDER_GAME_WINDOW:\n",
    "                environment.render()  # Show state visually\n",
    "\n",
    "            # Select and perform an action\n",
    "            action = agent.act(state)  # Act\n",
    "            next_state, reward, done, info = environment.step(action)  # Observe\n",
    "\n",
    "            next_state = agent.preProcess(next_state)  # Process image\n",
    "\n",
    "            # Stack state . Every state contains 4 time contionusly frames\n",
    "            # We stack frames like 4 channel image\n",
    "            next_state = np.stack((next_state, state[0], state[1], state[2]))\n",
    "\n",
    "            # Store the transition in memory\n",
    "            agent.storeResults(state, action, reward, next_state, done)  # Store to mem\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state  # Update state\n",
    "\n",
    "            if TRAIN_MODEL:\n",
    "                # Perform one step of the optimization (on the target network)\n",
    "                loss, max_q_val = agent.train()  # Train with random BATCH_SIZE state taken from mem\n",
    "            else:\n",
    "                loss, max_q_val = [0, 0]\n",
    "\n",
    "            total_loss += loss\n",
    "            total_max_q_val += max_q_val\n",
    "            total_reward += reward\n",
    "            total_step += 1\n",
    "            if total_step % 1000 == 0:\n",
    "                agent.adaptiveEpsilon()  # Decrase epsilon\n",
    "\n",
    "                if done:  # Episode completed\n",
    "                currentTime = time.time()  # Keep current time\n",
    "                time_passed = currentTime - startTime  # Find episode duration\n",
    "                current_time_format = time.strftime(\"%H:%M:%S\", time.gmtime())  # Get current dateTime as HH:MM:SS\n",
    "                epsilonDict = {'epsilon': agent.epsilon}  # Create epsilon dict to save model as file\n",
    "\n",
    "                if SAVE_MODELS and episode % SAVE_MODEL_INTERVAL == 0:  # Save model as file\n",
    "                    weightsPath = MODEL_PATH + str(episode) + '.pkl'\n",
    "                    epsilonPath = MODEL_PATH + str(episode) + '.json'\n",
    "\n",
    "                    torch.save(agent.online_model.state_dict(), weightsPath)\n",
    "                    with open(epsilonPath, 'w') as outfile:\n",
    "                        json.dump(epsilonDict, outfile)\n",
    "\n",
    "                if TRAIN_MODEL:\n",
    "                    agent.target_model.load_state_dict(agent.online_model.state_dict())  # Update target model\n",
    "\n",
    "                last_100_ep_reward.append(total_reward)\n",
    "                avg_max_q_val = total_max_q_val / step\n",
    "\n",
    "                outStr = \"Episode:{} Time:{} Reward:{:.2f} Loss:{:.2f} Last_100_Avg_Rew:{:.3f} \\\n",
    "                    Avg_Max_Q:{:.3f} Epsilon:{:.2f} Duration:{:.2f} Step:{} CStep:{}\".format(\n",
    "                    episode, current_time_format, total_reward, total_loss, np.mean(last_100_ep_reward)\n",
    "                    , avg_max_q_val, agent.epsilon, time_passed, step, total_step\n",
    "                )\n",
    "\n",
    "                print(outStr)\n",
    "\n",
    "                if SAVE_MODELS:\n",
    "                    outputPath = MODEL_PATH + \"out\" + '.txt'  # Save outStr to file\n",
    "                    with open(outputPath, 'a') as outfile:\n",
    "                        outfile.write(outStr+\"\\n\")\n",
    "\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
