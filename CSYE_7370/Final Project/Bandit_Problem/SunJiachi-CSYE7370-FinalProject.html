<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Bandit Problem</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Bandit Problem</h1>
</header>
<section data-field="body" class="e-content">
<section name="fca5" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="bb7f" id="bb7f" class="graf graf--h3 graf--leading graf--title">Bandit Problem</h3><p name="13b1" id="13b1" class="graf graf--p graf-after--h3">Adviser: Brown Nicholas</p><p name="0ed6" id="0ed6" class="graf graf--p graf-after--p">Author: Jiachi Sun</p><figure name="c917" id="c917" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*WAbG0ewR1rNFb_SG5D0ykw.jpeg" data-width="298" data-height="247" src="https://cdn-images-1.medium.com/max/800/1*WAbG0ewR1rNFb_SG5D0ykw.jpeg"></figure><p name="89be" id="89be" class="graf graf--p graf-after--figure">Before discussing the bandit and any details related to him, I first want to discuss the exploration-exploitation dilemma.</p><h4 name="9066" id="9066" class="graf graf--h4 graf-after--p">Exploration-exploitation dilemma</h4><p name="a4c1" id="a4c1" class="graf graf--p graf-after--h4">To use a simple analogy: a new restaurant opened downstairs, and after the opening I often go there to eat, ordering two or three dishes at a time. The first thing to emphasize is that if we choose a dish that is not good, we also need to bear the consequences and can not be left without a check. Then, to be on the safe side, when ordering I would probably order a dish that I thought was good and another one that I hadn‚Äôt ordered yet. This way, I will not eat the same dish every time, and not once ordered all the bad dishes, so I can not swallow. It‚Äôs also a trade-off between ‚Äúexploration‚Äù and ‚Äú exploitation‚Äù. How do I order to make sure I don‚Äôt get a table of dishes I don‚Äôt like at all while trying to discover if there are new dishes that will satisfy me?</p><p name="716d" id="716d" class="graf graf--p graf-after--p">How to allocate resources wisely in both exploration and utilization is the question we want to explore, if the restaurant mentioned above has 100 dishes on the menu. We can have many kinds of strategies. For example.</p><p name="92f0" id="92f0" class="graf graf--p graf-after--p">1. First try 20 dishes, and then later each time only choose the best few dishes in these 20 dishes. This is a way to explore first and then take advantage.</p><p name="e999" id="e999" class="graf graf--p graf-after--p">2. Each time I order a dish that I thought was good last time, and then order a dish that has not been ordered. After trying them all choose the two dishes you like best. This is the method of exploring while taking advantage.</p><p name="700c" id="700c" class="graf graf--p graf-after--p">So how do we allocate our opportunities wisely to get the most happiness from our food and get the most out of it? This brings us to the Bandit problem.</p><h4 name="06b4" id="06b4" class="graf graf--h4 graf-after--p">Bandit Problem</h4><figure name="ab8e" id="ab8e" class="graf graf--figure graf-after--h4"><img class="graf-image" data-image-id="1*lzB-5vp0TypKDwHYuHuAhg.jpeg" data-width="660" data-height="249" src="https://cdn-images-1.medium.com/max/800/1*lzB-5vp0TypKDwHYuHuAhg.jpeg"></figure><p name="8d73" id="8d73" class="graf graf--p graf-after--figure">Imagine that a gambler has N slot machines in front of him, and he doesn‚Äôt know the real profit of each slot machine beforehand, how can he choose which one to pull next time or whether to stop gambling to maximize his profit based on the result of each slot machine he plays.</p><p name="0173" id="0173" class="graf graf--p graf-after--p">Suppose that instead of deciding which dish is the best, we come to a casino to play a slot machine. Now we have N slot machines in front of us and we don‚Äôt know in advance the real profitability of each slot machine. We choose one machine at a time, put in one dollar, and pull down the lever. At this point there are two results, we win money or we lose a dollar. At this point the question shifts from getting the most out of the different dishes above to‚Ää‚Äî‚Äähow to choose that slot machine with a reasonable strategy to get the most out of it. This is the famous bandit problem.</p><p name="2308" id="2308" class="graf graf--p graf-after--p">A simpler strategy is to start with $200 and invest $20 in each machine, regardless of the winner. After that, choose the machine with the highest win rate and put the remaining $800 into it. And we call this strategy Epsilon first.</p><p name="086a" id="086a" class="graf graf--p graf-after--p">There is no denying that this strategy is better than a completely random strategy. But there is one obvious disadvantage to this strategy, namely that after the initial exploration process, we will not explore at all. And it is likely that we will miss out on the most rewarding machines as a result. And, over time, we may find that the return on the machine we chose begins to fade, but we still have no option to try something else.</p><p name="0a96" id="0a96" class="graf graf--p graf-after--p">It is clear that the Epsilon First strategy is not satisfying us. Therefore, I will explore several relatively more effective strategies in the next articles: Epsilon greedy, Upper Confidence Bound (UCB), and Thompson Sampling.</p><p name="af3e" id="af3e" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The code below sets the necessary import and set up the standard k-arm bandit environment.</strong></p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="8aa8" id="8aa8" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br /><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br /><span class="hljs-keyword">from</span> pdb <span class="hljs-keyword">import</span> set_trace<br /><br />stationary=<span class="hljs-literal">True</span><br /><span class="hljs-keyword">class</span> <span class="hljs-title class_">Bandit</span>():<br />  <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, arm_count</span>):<br />    <span class="hljs-string">&quot;&quot;&quot;<br />    Multi-armed bandit with rewards 1 or 0.<br />    <br />    At initialization, multiple arms are created. The probability of each arm<br />    returning reward 1 if pulled is sampled from Bernouilli(p), where p randomly<br />    chosen from Uniform(0,1) at initialization<br />    &quot;&quot;&quot;</span><br />    self.arm_count = arm_count<br />    self.generate_thetas()<br />    self.timestep = <span class="hljs-number">0</span><br />    <span class="hljs-keyword">global</span> stationary<br />    self.stationary=stationary<br />    <br />  <span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_thetas</span>(<span class="hljs-params">self</span>):<br />    self.thetas = np.random.uniform(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,self.arm_count)<br />  <br />  <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_reward_regret</span>(<span class="hljs-params">self, arm</span>):<br />    <span class="hljs-string">&quot;&quot;&quot; Returns random reward for arm action. Assumes actions are 0-indexed<br />    Args:<br />      arm is an int<br />    &quot;&quot;&quot;</span><br />    self.timestep += <span class="hljs-number">1</span><br />    <span class="hljs-keyword">if</span> (self.stationary==<span class="hljs-literal">False</span>) <span class="hljs-keyword">and</span> (self.timestep%<span class="hljs-number">100</span> == <span class="hljs-number">0</span>) :<br />      self.generate_thetas()<br />    <span class="hljs-comment"># Simulate bernouilli sampling</span><br />    sim = np.random.uniform(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,self.arm_count)<br />    rewards = (sim&lt;self.thetas).astype(<span class="hljs-built_in">int</span>)<br />    reward = rewards[arm]<br />    regret = self.thetas.<span class="hljs-built_in">max</span>() - self.thetas[arm]<br />    <br />    <span class="hljs-keyword">return</span> reward, regret</span></pre><h4 name="8da2" id="8da2" class="graf graf--h4 graf-after--pre"><strong class="markup--strong markup--h4-strong">Epsilon greedy</strong></h4><p name="3017" id="3017" class="graf graf--p graf-after--h4">Epsilon greedy is divided into two steps.</p><p name="c27a" id="c27a" class="graf graf--p graf-after--p">1. Randomly select one of the N slot machines with probability œµ to pull the joystick (each slot machine has 1/N probability to be selected), i.e., explore.</p><p name="3626" id="3626" class="graf graf--p graf-after--p">2. Select one of the N slot machines with a probability of 1-œµ and choose the one with the highest return on investment at the cut-off, i.e., exploit.</p><p name="e937" id="e937" class="graf graf--p graf-after--p">Here the value of œµ controls the preference for exploit and explore, and each decision is made with probability œµ for Exploration and 1-œµ for Exploitation.</p><p name="fee2" id="fee2" class="graf graf--p graf-after--p">It can be found that the selection of œµ directly determines the performance of the algorithm, and if œµ=1, it will become a completely randomized algorithm. In general, we choose a smaller value between (0,1).</p><p name="1bc1" id="1bc1" class="graf graf--p graf-after--p">Compared to Epsilon first, Epsilon greedy has a better balance between exploration and exploitation. It offers a certain possibility to explore other slots whenever possible, choosing the slot with the highest payoff more often to get the payoff.</p><p name="69d8" id="69d8" class="graf graf--p graf-after--p">At the same time, however, this randomness becomes a drawback as the number of rounds grows. After enough rounds, we may have found the machine with the highest payoff, but the algorithm will still have an œµ probability of making a random selection. This actually causes losses as well. And Epsilon greedy splits our choices per round into two cases by the hyperparameter œµ. But there is no good use of historical information such as the number of times each machine has been selected, or the total number of rounds.</p><p name="ac3e" id="ac3e" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The code below implement the Epsilon-Greedy algorithm</strong></p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="087f" id="087f" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">epsilon = <span class="hljs-number">0.1</span><br /><span class="hljs-keyword">class</span> <span class="hljs-title class_">EpsilonGreedy</span>():<br />  <span class="hljs-string">&quot;&quot;&quot;<br />  Epsilon Greedy with incremental update.<br />  Based on Sutton and Barto pseudo-code, page. 24<br />  &quot;&quot;&quot;</span><br />  <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, bandit</span>):<br />    <span class="hljs-keyword">global</span> epsilon<br />    self.epsilon = epsilon<br />    self.bandit = bandit<br />    self.arm_count = bandit.arm_count<br />    self.Q = np.zeros(self.arm_count) <span class="hljs-comment"># q-value of actions</span><br />    self.N = np.zeros(self.arm_count) <span class="hljs-comment"># action count</span><br />  <br /><span class="hljs-meta">  @staticmethod</span><br />  <span class="hljs-keyword">def</span> <span class="hljs-title function_">name</span>():<br />    <span class="hljs-keyword">return</span> <span class="hljs-string">&#x27;epsilon-greedy&#x27;</span><br />  <br />  <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_action</span>(<span class="hljs-params">self</span>):<br />    <span class="hljs-keyword">if</span> np.random.uniform(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>) &gt; self.epsilon:<br />      action = self.Q.argmax()<br />    <span class="hljs-keyword">else</span>:<br />      action = np.random.randint(<span class="hljs-number">0</span>, self.arm_count)<br />    <span class="hljs-keyword">return</span> action<br />  <br />  <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_reward_regret</span>(<span class="hljs-params">self, arm</span>):<br />    reward, regret = self.bandit.get_reward_regret(arm)<br />    self._update_params(arm, reward)<br />    <span class="hljs-keyword">return</span> reward, regret<br />  <br />  <span class="hljs-keyword">def</span> <span class="hljs-title function_">_update_params</span>(<span class="hljs-params">self, arm, reward</span>):<br />    self.N[arm] += <span class="hljs-number">1</span> <span class="hljs-comment"># increment action count</span><br />    self.Q[arm] += <span class="hljs-number">1</span>/self.N[arm] * (reward - self.Q[arm]) <span class="hljs-comment"># inc. update rule</span></span></pre><h4 name="fae6" id="fae6" class="graf graf--h4 graf-after--pre"><strong class="markup--strong markup--h4-strong">Upper Confidence Bound</strong></h4><p name="4aae" id="4aae" class="graf graf--p graf-after--h4">Let‚Äôs return to UCB, whose core formula is shown below.</p><figure name="541e" id="541e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*EJaVWmDl69aEFZp-8_osag.png" data-width="385" data-height="89" src="https://cdn-images-1.medium.com/max/800/1*EJaVWmDl69aEFZp-8_osag.png"></figure><p name="83cd" id="83cd" class="graf graf--p graf-after--figure">Where Qt(a) is the current gain of this arm A as of the current turn. t is the total number of turns in the experiment so far. Nt(a) is the number of times a has been selected so far. c is the hyperparameter specified by us and referred to as UCB1 when c = 1.</p><p name="f814" id="f814" class="graf graf--p graf-after--p">As the number of rounds increases log(t) gets larger. At this point, if a particular arm is selected a few times, then log(t)/Nt(a) will also be larger, and then the chances of this arm being selected will be larger and larger, and this is when we are performing the round of exploration. And when the Qt(a) of a certain arm is large, i.e., he has a high payoff, then it will be selected, and this is the exploitation. When the payoff of a particular arm is large enough or even larger than the upper bound of the confidence interval of any of the other arms, this arm will always be selected and UCB will stop doing exploration and focus on exploitation.</p><p name="39cf" id="39cf" class="graf graf--p graf-after--p">Compared to the previous methods, UCB better balances exploration and discovery, while making better use of historical information such as the number of times each machine is selected, or the total number of rounds. Even if an arm has a low return in the first few times, it still has a chance to be selected as the number of rounds increases.</p><p name="e71a" id="e71a" class="graf graf--p graf-after--p">Also, the choice of c is critical, the larger c is, then the larger the upper bound of the confidence interval for the arm that was selected less often, and UCB will be more inclined to explore rather than perform an exploit.</p><p name="4921" id="4921" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The code below implement the UCB algorithm.</strong></p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="0f64" id="0f64" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">ucb_c = <span class="hljs-number">2</span><br /><span class="hljs-keyword">class</span> <span class="hljs-title class_">UCB</span>():<br />  <span class="hljs-string">&quot;&quot;&quot;<br />  Epsilon Greedy with incremental update.<br />  Based on Sutton and Barto pseudo-code, page. 24<br />  &quot;&quot;&quot;</span><br />  <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, bandit</span>):<br />    <span class="hljs-keyword">global</span> ucb_c<br />    self.ucb_c = ucb_c<br />    self.bandit = bandit<br />    self.arm_count = bandit.arm_count<br />    self.Q = np.zeros(self.arm_count) <span class="hljs-comment"># q-value of actions</span><br />    self.N = np.zeros(self.arm_count) + <span class="hljs-number">0.0001</span> <span class="hljs-comment"># action count</span><br />    self.timestep = <span class="hljs-number">1</span><br />  <br /><span class="hljs-meta">  @staticmethod</span><br />  <span class="hljs-keyword">def</span> <span class="hljs-title function_">name</span>():<br />    <span class="hljs-keyword">return</span> <span class="hljs-string">&#x27;ucb&#x27;</span><br />  <br />  <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_action</span>(<span class="hljs-params">self</span>):<br />    ln_timestep = np.log(np.full(self.arm_count, self.timestep))<br />    confidence = self.ucb_c * np.sqrt(ln_timestep/self.N)<br />    action = np.argmax(self.Q + confidence)<br />    self.timestep += <span class="hljs-number">1</span><br />    <span class="hljs-keyword">return</span> action<br />  <br />  <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_reward_regret</span>(<span class="hljs-params">self, arm</span>):<br />    reward, regret = self.bandit.get_reward_regret(arm)<br />    self._update_params(arm, reward)<br />    <span class="hljs-keyword">return</span> reward, regret<br />  <br />  <span class="hljs-keyword">def</span> <span class="hljs-title function_">_update_params</span>(<span class="hljs-params">self, arm, reward</span>):<br />    self.N[arm] += <span class="hljs-number">1</span> <span class="hljs-comment"># increment action count</span><br />    self.Q[arm] += <span class="hljs-number">1</span>/self.N[arm] * (reward - self.Q[arm]) <span class="hljs-comment"># inc. update rule</span></span></pre><h4 name="2c92" id="2c92" class="graf graf--h4 graf-after--pre"><strong class="markup--strong markup--h4-strong">Thompson Sampling</strong></h4><p name="ffa1" id="ffa1" class="graf graf--p graf-after--h4">First of all, we need to talk about what is Beta distribution. It has two parameters greater than 0, Œ±, Œ≤. Depending on the different values of a and b, it has the following image of the function.</p><figure name="16eb" id="16eb" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*yyeQ7Teb4wTm2K0aqUNUqQ.jpeg" data-width="601" data-height="361" src="https://cdn-images-1.medium.com/max/800/1*yyeQ7Teb4wTm2K0aqUNUqQ.jpeg"></figure><p name="76c9" id="76c9" class="graf graf--p graf-after--figure">We can see that the images drawn for different a‚Äôs are different when the beta is the same.The larger the a, the more to the right the image is, and the greater the probability of getting a larger value when we take a random value from the beta distribution. Thompson Sampling makes good use of this property.</p><p name="1aa5" id="1aa5" class="graf graf--p graf-after--p">For the multi-armed slot machine scenario, the parameter for any slot machine X‚Äô beta distribution:</p><p name="5ce0" id="5ce0" class="graf graf--p graf-after--p">1. Œ± is the number of times we pull the arm of the slot machine X and get a reward</p><p name="7a22" id="7a22" class="graf graf--p graf-after--p">2. Œ≤ is the number of times we pull the arm of slot machine X and do not get a reward</p><p name="8fea" id="8fea" class="graf graf--p graf-after--p">In each selection, we let the beta distribution of each slot machine generate a random number and then select the slot machine that generates the maximum value and pull the arm. Subsequently, the Œ± or Œ≤ of the selected slot machine is updated depending on whether we get a reward or not.</p><p name="2d15" id="2d15" class="graf graf--p graf-after--p">The advantage of this is that we make full use of historical information and machines that are rarely selected or arms with low returns have a chance to be selected because we are generating a random number in the range of the beta distribution each round. Machines that perform better have a higher probability of generating a larger number and thus being selected, i.e., exploitation. At the same time, a machine that performs poorly also has some chance of generating a larger number and thus being selected, i.e., exploration.</p><p name="bef3" id="bef3" class="graf graf--p graf-after--p">It is worth mentioning that when the beta distribution of a machine is large enough that the minimum value that can be generated is larger than the maximum value that can be generated by the beta distribution of any other set of slot machines, Thompson Sampling will also stop exploring and focus on exploiting instead.</p><p name="bebb" id="bebb" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The code below implement the Thompson Sampling with beta-distribution.</strong></p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="0fa2" id="0fa2" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BetaAlgo</span>():<br />  <span class="hljs-string">&quot;&quot;&quot;<br />  The algos try to learn which Bandit arm is the best to maximize reward.<br />  <br />  It does this by modelling the distribution of the Bandit arms with a Beta, <br />  assuming the true probability of success of an arm is Bernouilli distributed.<br />  &quot;&quot;&quot;</span><br />  <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, bandit</span>):<br />    <span class="hljs-string">&quot;&quot;&quot;<br />    Args:<br />      bandit: the bandit class the algo is trying to model<br />    &quot;&quot;&quot;</span><br />    self.bandit = bandit<br />    self.arm_count = bandit.arm_count<br />    self.alpha = np.ones(self.arm_count)<br />    self.beta = np.ones(self.arm_count)<br />  <br />  <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_reward_regret</span>(<span class="hljs-params">self, arm</span>):<br />    reward, regret = self.bandit.get_reward_regret(arm)<br />    self._update_params(arm, reward)<br />    <span class="hljs-keyword">return</span> reward, regret<br />  <br />  <span class="hljs-keyword">def</span> <span class="hljs-title function_">_update_params</span>(<span class="hljs-params">self, arm, reward</span>):<br />    self.alpha[arm] += reward<br />    self.beta[arm] += <span class="hljs-number">1</span> - reward<br /><br />  <br /><span class="hljs-keyword">class</span> <span class="hljs-title class_">BernThompson</span>(<span class="hljs-title class_ inherited__">BetaAlgo</span>):<br />  <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, bandit</span>):<br />    <span class="hljs-built_in">super</span>().__init__(bandit)<br /><br /><span class="hljs-meta">  @staticmethod</span><br />  <span class="hljs-keyword">def</span> <span class="hljs-title function_">name</span>():<br />    <span class="hljs-keyword">return</span> <span class="hljs-string">&#x27;thompson&#x27;</span><br />  <br />  <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_action</span>(<span class="hljs-params">self</span>):<br />    <span class="hljs-string">&quot;&quot;&quot; Bernouilli parameters are sampled from the beta&quot;&quot;&quot;</span><br />    theta = np.random.beta(self.alpha, self.beta)<br />    <span class="hljs-keyword">return</span> theta.argmax()</span></pre><h4 name="0512" id="0512" class="graf graf--h4 graf-after--pre">Comparison of the performance of various methods and the effect of hyperparameters on performance</h4><figure name="2ace" id="2ace" class="graf graf--figure graf-after--h4"><img class="graf-image" data-image-id="1*KjXWYa9_2DTCYOXu91sR8g.png" data-width="382" data-height="247" src="https://cdn-images-1.medium.com/max/800/1*KjXWYa9_2DTCYOXu91sR8g.png"><figcaption class="imageCaption">10 slot machine, epilson = 0.1, ucb_c =¬†2</figcaption></figure><figure name="1ec6" id="1ec6" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*0AJ3p5cuncEkTJlleR9P1w.png" data-width="387" data-height="249" src="https://cdn-images-1.medium.com/max/800/1*0AJ3p5cuncEkTJlleR9P1w.png"><figcaption class="imageCaption">10 slot machine, epilson = 0..03, ucb_c =¬†0.1</figcaption></figure><p name="0110" id="0110" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Thompson Sampling</strong></p><p name="e3cc" id="e3cc" class="graf graf--p graf-after--p">The performance of Thompson Sampling is stable because it is not affected by hyperparameters, which is an advantage of Thompson Sampling. We do not need to adjust hyperparameters.</p><p name="24c8" id="24c8" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Epilson-Greedy</strong></p><p name="7339" id="7339" class="graf graf--p graf-after--p">The hyperparameter for ùúñœµ-greedy is the œµ</p><p name="a667" id="a667" class="graf graf--p graf-after--p">The œµ determines the probability of whether we choose to exploration or exploitation for each round.If we set œµ to 1,the œµ-greedy algorithm will like random sampling.If we set œµ to 0.1, then there is a 10% probability that one restaurant will be randomly selected for exploration each time. There is a 90% probability of choosing the one with the highest current return.</p><p name="dad8" id="dad8" class="graf graf--p graf-after--p">We can see from the two sets of experiments below that the experimental results differ greatly when we set œµ to 0.1 and 0.03, respectively. When ùúñœµ is 0.1, both œµ-greedy and Thompson Sampling can be stabilized at about 200 times. However, when we set ùúñœµ to 0.03, œµ-greedy converge to a steady regret value after 800 steps.</p><p name="bf5b" id="bf5b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">UCB</strong></p><p name="820c" id="820c" class="graf graf--p graf-after--p">The hyperparameter for UCB is the constant c.</p><p name="52eb" id="52eb" class="graf graf--p graf-after--p">The larger we set the value of c, the smaller the weight of ùëÑùë°(ùëé)Qt(a). Also, the larger the value of c is set, the more the UCB algorithm focuses on exploration.</p><p name="2577" id="2577" class="graf graf--p graf-after--p">From the comparison of the two sets of experiments below we can find that the value of the constant c has a very strong influence on the UCB. When we set c to 2, the UCB algorithm does not stabilize until the 1000th time. But when we set C to 0.1, UCB performs very well and converge to a steady regret value at 100 steps.</p><p name="501f" id="501f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Some helper method</strong></p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="2d76" id="2d76" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">def</span> <span class="hljs-title function_">plot_data</span>(<span class="hljs-params">y</span>):<br />  <span class="hljs-string">&quot;&quot;&quot; y is a 1D vector &quot;&quot;&quot;</span><br />  x = np.arange(y.size)<br />  _ = plt.plot(x, y, <span class="hljs-string">&#x27;o&#x27;</span>)<br />  <br /><span class="hljs-keyword">def</span> <span class="hljs-title function_">multi_plot_data</span>(<span class="hljs-params">data, names</span>):<br />  <span class="hljs-string">&quot;&quot;&quot; data, names are lists of vectors &quot;&quot;&quot;</span><br />  x = np.arange(data[<span class="hljs-number">0</span>].size)<br />  <span class="hljs-keyword">for</span> i, y <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(data):<br />    plt.plot(x, y, <span class="hljs-string">&#x27;o&#x27;</span>, markersize=<span class="hljs-number">2</span>, label=names[i])<br />  plt.legend(loc=<span class="hljs-string">&#x27;upper right&#x27;</span>, prop={<span class="hljs-string">&#x27;size&#x27;</span>: <span class="hljs-number">16</span>}, numpoints=<span class="hljs-number">10</span>)<br />  plt.show()<br />  <br /><span class="hljs-keyword">def</span> <span class="hljs-title function_">simulate</span>(<span class="hljs-params">simulations, timesteps, arm_count, Algorithm</span>):<br />  <span class="hljs-string">&quot;&quot;&quot; Simulates the algorithm over &#x27;simulations&#x27; epochs &quot;&quot;&quot;</span><br />  sum_regrets = np.zeros(timesteps)<br />  <span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(simulations):<br />    bandit = Bandit(arm_count)<br />    algo = Algorithm(bandit)<br />    regrets = np.zeros(timesteps)<br />    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(timesteps):<br />      action = algo.get_action()<br />      reward, regret = algo.get_reward_regret(action)<br />      regrets[i] = regret<br />    sum_regrets += regrets  <br />  mean_regrets = sum_regrets / simulations<br />  <span class="hljs-keyword">return</span> mean_regrets<br /><br /><span class="hljs-keyword">def</span> <span class="hljs-title function_">experiment</span>(<span class="hljs-params">arm_count, timesteps=<span class="hljs-number">1000</span>, simulations=<span class="hljs-number">1000</span></span>):<br />  <span class="hljs-string">&quot;&quot;&quot; <br />  Standard setup across all experiments <br />  Args:<br />    timesteps: (int) how many steps for the algo to learn the bandit<br />    simulations: (int) number of epochs<br />  &quot;&quot;&quot;</span><br /><span class="hljs-comment"># EpsilonGreedy, UCB, BernThompson, RandomSampling,</span><br />  algos = [EpsilonGreedy, UCB, BernThompson, RandomSampling]<br />  regrets = []<br />  names = []<br />  <span class="hljs-keyword">for</span> algo <span class="hljs-keyword">in</span> algos:<br />    regrets.append(simulate(simulations, timesteps, arm_count, algo))<br />    names.append(algo.name())<br />  multi_plot_data(regrets, names)</span></pre><h4 name="9991" id="9991" class="graf graf--h4 graf-after--pre">Application of¬†Bandit</h4><p name="6280" id="6280" class="graf graf--p graf-after--h4">The multi-armed slot machine problem has a wide range of applications in business, including advertising displays, medical trials, and finance. For example, in a recommendation system, we have N items, and we do not know in advance how user A will react to N items, we need to recommend a certain item to the user each time to maximize the value of the user (or try to make user A convert), such as the user‚Äôs purchase.</p><p name="a5e0" id="a5e0" class="graf graf--p graf-after--p">Or in life, we need to choose a dress, pick a restaurant, can be done in this way to choose.</p><h4 name="1429" id="1429" class="graf graf--h4 graf-after--p">Code</h4><p name="42b8" id="42b8" class="graf graf--p graf-after--h4">You can find the complete code here:¬†</p><h4 name="edd1" id="edd1" class="graf graf--h4 graf-after--p">Reference</h4><p name="b936" id="b936" class="graf graf--p graf-after--h4"><br>1. Andre Cianflone‚Ää‚Äî‚ÄäThompson sampling<br><a href="https://github.com/andrecianflone/thompson/blob/master/thompson.ipynb" data-href="https://github.com/andrecianflone/thompson/blob/master/thompson.ipynb" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://github.com/andrecianflone/thompson/blob/master/thompson.ipynb</a></p><p name="042f" id="042f" class="graf graf--p graf-after--p">2. Improve Your Project Management Through Beta Distribution <a href="https://www.6sigma.us/beta-distribution/improve-project-management-beta-distribution/" data-href="https://www.6sigma.us/beta-distribution/improve-project-management-beta-distribution/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://www.6sigma.us/beta-distribution/improve-project-management-beta-distribution/</a></p><p name="6cec" id="6cec" class="graf graf--p graf-after--p graf--trailing">3. Êé¢Á¥¢-Âà©Áî®Âõ∞Â¢ÉÔºàexploration-exploitation dilemmaÔºâ<a href="https://zhuanlan.zhihu.com/p/161284124" data-href="https://zhuanlan.zhihu.com/p/161284124" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://zhuanlan.zhihu.com/p/161284124</a></p></div></div></section>
</section>
<footer><p><a href="https://medium.com/p/665f2ac1d584">View original.</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 17, 2022.</p></footer></article></body></html>