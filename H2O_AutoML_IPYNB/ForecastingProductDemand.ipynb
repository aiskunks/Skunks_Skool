{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting Product Demand\n",
    "\n",
    "In this example, we will be forecasting the monthly demand of different products.  The data used is a public Kaggle dataset consisting of order demand per product and warehouse: [Product Demand Data](https://www.kaggle.com/felixzhao/productdemandforecasting).\n",
    "\n",
    "We will be using Sparkling Water to ingest the data and add historical lags.\n",
    "\n",
    "Our Machine Learning Workflow is: \n",
    "\n",
    "1. Import data into Spark\n",
    "2. Exploratory Analysis\n",
    "3. Feature engineering\n",
    "   * Add time lag columns\n",
    "4. Train a baseline model\n",
    "5. Train model with lag features\n",
    "6. Train model with additional features\n",
    "7. Compare models\n",
    "8. Shut down sparkling water cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 (of 8).  Import data into Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initiate H2OContext on top of Spark\n",
    "\n",
    "from pysparkling import *\n",
    "hc = H2OContext.getOrCreate(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([StructField(\"Product_Code\", StringType(), True),\n",
    "                     StructField(\"Warehouse\", StringType(), True),\n",
    "                     StructField(\"Product_Category\", StringType(), True),\n",
    "                     StructField(\"Month\", DateType(), True),\n",
    "                     StructField(\"Total_Demand\", DoubleType(), True),\n",
    "                     StructField(\"Number_Orders\", DoubleType(), True)])\n",
    "\n",
    "# https://s3-us-west-2.amazonaws.com/h2o-tutorials/data/topics/time_series/product_demand/Monthly_Product_Demand.csv\n",
    "products_df = spark.read.csv(\"../../data/topics/time_series/product_demand/Monthly_Product_Demand.csv\", header = True, schema = schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "products_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 (of 8).  Exploratory Analysis\n",
    "\n",
    "We will start by exploring and analyzing our data.  We will first explore the data in PySpark and then do the same exploration in H2O. \n",
    "\n",
    "### PySpark Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data Schema\n",
    "products_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dimensions\n",
    "print(\"Number of rows: \", \"{:,}\".format(products_df.count()))\n",
    "print(\"Number of columns: \", \"{:,}\".format(len(products_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Distinct Products\n",
    "print(\"Number of Products: \", products_df.select(\"Product_Code\").distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Distinct Warehouses\n",
    "print(\"Number of Warehouses: \", products_df.select(\"Warehouse\").distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Range of Demand\n",
    "min_demand = products_df.agg({\"Total_Demand\": \"min\"}).collect()[0][0]\n",
    "max_demand = products_df.agg({\"Total_Demand\": \"max\"}).collect()[0][0]\n",
    "print(\"Demand Range: \", min_demand, \" to \", max_demand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Date Range\n",
    "min_date = products_df.agg({\"Month\": \"min\"}).collect()[0][0]\n",
    "max_date = products_df.agg({\"Month\": \"max\"}).collect()[0][0]\n",
    "print(\"Date Range: \", min_date, \" to \", max_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of records per product and warehouse\n",
    "from pyspark.sql.functions import count, col \n",
    "cnts = products_df.groupBy(\"Product_Code\", \"Product_Category\", \"Warehouse\").agg(count(\"*\").alias(\"cnt\")).alias(\"cnts\")\n",
    "cnts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot some of the time series data\n",
    "plot_data = products_df.where((products_df.Product_Code == \"Product_1846\"))\n",
    "plot_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_data = plot_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1,1);\n",
    "plot_data.groupby(\"Warehouse\").plot(x=\"Month\", y=\"Total_Demand\", ax=ax);\n",
    "plt.legend([v[0] for v in plot_data.groupby('Warehouse')['Warehouse']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H2O Exploratory Analysis\n",
    "\n",
    "We will repeat the same exploratory analysis on the H2O Frame to show the differences in functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Convert Spark DataFrame to H2O Frame\n",
    "\n",
    "import h2o\n",
    "products_hf = hc.as_h2o_frame(products_df, \"productsTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data Schema\n",
    "products_hf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert String to Categorical\n",
    "cat_cols = [\"Product_Code\", \"Product_Category\", \"Warehouse\"]\n",
    "for i in cat_cols:\n",
    "    products_hf[i] = products_hf[i].asfactor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dimensions\n",
    "print(\"Number of rows: \", \"{:,}\".format(products_hf.nrow))\n",
    "print(\"Number of columns: \", \"{:,}\".format(products_hf.ncol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of unique products\n",
    "print(\"Number of Products: \", products_hf[\"Product_Code\"].table().nrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of unique warehouses\n",
    "print(\"Number of Warehouses: \", products_hf[\"Warehouse\"].table().nrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Range of Demand\n",
    "products_hf[\"Total_Demand\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Date Range\n",
    "print(\"Date Range\")\n",
    "products_hf = products_hf.sort(\"Month\")\n",
    "print(products_hf[\"Month\"][[0, (products_hf.nrow - 1)], \"Month\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count per Product and Warehouse\n",
    "print(\"Count per Product and Warehouse\")\n",
    "cnts = products_hf.group_by([\"Product_Code\", \"Product_Category\", \"Warehouse\"]).count().get_frame()\n",
    "cnts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 (of 8).  Feature Engineering\n",
    "\n",
    "We will add new features to our data that can help predict the Demand for a given product and warehouse.  Features that tell us:\n",
    "* what was the Demand for a product and warehouse last month, two months ago, three months ago, etc?  \n",
    "* what was the Number of Order for a product and warehouse last month?\n",
    "\n",
    "can be very predictive in forecasting.  To create these features we will use PySpark's window function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Add Total Demand from the Previous month, Previous 2 months, Previous 3 months, etc per Product and Warehouse\n",
    "from pyspark.sql.functions import lag, col\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "w = Window().partitionBy([col(x) for x in [\"Product_Code\", \"Product_Category\", \"Warehouse\"]]).orderBy(col(\"Month\"))\n",
    "ext_products_df = products_df.select(\"*\", lag(\"Total_Demand\", count = 1).over(w).alias(\"Demand_lag1\"),\n",
    "                                     lag(\"Total_Demand\", count = 2).over(w).alias(\"Demand_lag2\"),\n",
    "                                     lag(\"Total_Demand\", count = 3).over(w).alias(\"Demand_lag3\"),\n",
    "                                     lag(\"Total_Demand\", count = 4).over(w).alias(\"Demand_lag4\"),\n",
    "                                     lag(\"Total_Demand\", count = 5).over(w).alias(\"Demand_lag5\"),\n",
    "                                     lag(\"Total_Demand\", count = 6).over(w).alias(\"Demand_lag6\"),\n",
    "                                     lag(\"Total_Demand\", count = 12).over(w).alias(\"Demand_lag12\")).na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ext_products_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Add Number of Orders for Previous month per Product and Warehouse\n",
    "\n",
    "ext_products_df = ext_products_df.select(\"*\", lag(\"Number_Orders\", count = 1).over(w).alias(\"Number_Orders_lag1\")).na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Convert Spark DataFrame to H2O Frame\n",
    "ext_products_hf = hc.as_h2o_frame(ext_products_df, \"productsWithLagsTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert String to Categorical\n",
    "for i in cat_cols:\n",
    "    ext_products_hf[i] = ext_products_hf[i].asfactor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 (of 8).  Train baseline model\n",
    "\n",
    "We will begin our machine learning section by training a model using solely our original data.  We will use this model as a baseline to compare the performance of new models with additional features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set Predictors\n",
    "predictors = [\"Product_Code\", \"Warehouse\", \"Product_Category\"]\n",
    "response = \"Total_Demand\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing by time\n",
    "\n",
    "train = ext_products_hf[ext_products_hf[\"Month\"].year() < 2015]\n",
    "test = ext_products_hf[ext_products_hf[\"Month\"].year() == 2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train GBM Model with Early Stopping\n",
    "\n",
    "from h2o.estimators import H2OGradientBoostingEstimator\n",
    "baseline_model = H2OGradientBoostingEstimator(model_id = \"baseline_model.hex\",\n",
    "                                              stopping_rounds = 3, stopping_metric = \"MAE\",\n",
    "                                              score_tree_interval = 10, ntrees = 500)\n",
    "baseline_model.train(x = predictors,\n",
    "                     y = response,\n",
    "                     training_frame = train,\n",
    "                     validation_frame = test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "baseline_model.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Baseline - MAE: \" + \"{:,}\".format(round(baseline_model.mae(valid = True))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 (of 8).  Train model with lags\n",
    "\n",
    "We will see if we can improve our model by adding our lag features as new predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add Lag Predictors\n",
    "ext_predictors = list(set(train.col_names) - set([\"Total_Demand\", \"Number_Orders\"]))\n",
    "ext_predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train GBM Model with Early Stopping\n",
    "\n",
    "lag_features_v1 = H2OGradientBoostingEstimator(model_id = \"lag_features_v1.hex\",\n",
    "                                              stopping_rounds = 3,  stopping_metric = \"MAE\",\n",
    "                                              score_tree_interval = 10, ntrees = 500)\n",
    "lag_features_v1.train(x = ext_predictors,\n",
    "                      y = response,\n",
    "                      training_frame = train,\n",
    "                      validation_frame = test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lag_features_v1.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Baseline - MAE: \" + \"{:,}\".format(round(baseline_model.mae(valid = True))))\n",
    "print(\"Lag Features - MAE: \" + \"{:,}\".format(round(lag_features_v1.mae(valid = True))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph below shows the variable importance for the gradient boosted model.  The most important predictors are the demand lags.  We can use the partial dependency plots to see the relationship between these features and the model's prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "lag_features_v1.varimp_plot(num_of_features = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter data to common demand\n",
    "max_demand = train[\"Total_Demand\"].quantile(prob = [0.9])[0, 1]\n",
    "pdp_data = train[(train[\"Demand_lag3\"] < max_demand) & (train[\"Demand_lag3\"] >= 0)]\n",
    "# create pdp's\n",
    "pdps = lag_features_v1.partial_plot(data = pdp_data, cols = [\"Demand_lag3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partial plots show that the Demand from last quarter is related to the current month Demand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 (of 8).  Train Model with Additional Features\n",
    "\n",
    "We will now use H2O to add additional features such as the change in demand over days.\n",
    "\n",
    "A feature we would like to add is the average demand for product, warehouse, and product & warehouse.  Using the actual demand when calculating the average, however, can result in data leakage.  Instead we will calculate the average of the previous month's Demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Average Demand Lag 1 by Product & Warehouse\n",
    "avg_demand_group = ext_products_hf.group_by([\"Product_Code\", \"Product_Category\", \"Warehouse\"]).mean([\"Demand_lag1\"])\n",
    "avg_demand_group = avg_demand_group.get_frame()\n",
    "avg_demand_group.col_names = [\"Product_Code\", \"Product_Category\", \"Warehouse\", \"mean_Demand_lag1_Product_Warehouse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg_demand_group.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Average Demand Lag 1 by Warehouse\n",
    "avg_demand_warehouse = ext_products_hf.group_by([\"Warehouse\"]).mean([\"Demand_lag1\"]).get_frame()\n",
    "avg_demand_warehouse.col_names = [\"Warehouse\", \"mean_Demand_lag1_Warehouse\"]\n",
    "avg_demand_warehouse.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Average Demand Lag 1 by Product\n",
    "avg_demand_product = ext_products_hf.group_by([\"Product_Code\"]).mean([\"Demand_lag1\"]).get_frame()\n",
    "avg_demand_product.col_names = [\"Product_Code\", \"mean_Demand_lag1_Product\"]\n",
    "avg_demand_product.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Average Demand Lag 1 by Product Category\n",
    "avg_demand_product_cat = ext_products_hf.group_by([\"Product_Category\"]).mean([\"Demand_lag1\"]).get_frame()\n",
    "avg_demand_product_cat.col_names = [\"Product_Category\", \"mean_Demand_lag1_ProductCat\"]\n",
    "avg_demand_product_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merge averages to our original frame\n",
    "ext_products_hf = ext_products_hf.merge(avg_demand_group, all_x = True, all_y = False)\n",
    "ext_products_hf = ext_products_hf.merge(avg_demand_warehouse, all_x = True, all_y = False)\n",
    "ext_products_hf = ext_products_hf.merge(avg_demand_product, all_x = True, all_y = False)\n",
    "ext_products_hf = ext_products_hf.merge(avg_demand_product_cat, all_x = True, all_y = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ext_products_hf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract the month in case there is seasonality \n",
    "ext_products_hf[\"Categorical_Month\"] = ext_products_hf[\"Month\"].month().asfactor()\n",
    "ext_products_hf[\"Categorical_Month\"].table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add Lag Predictors\n",
    "ext_predictors = list(set(ext_products_hf.col_names) - set([\"Total_Demand\", \"Number_Orders\"]))\n",
    "ext_predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing by time\n",
    "\n",
    "train = ext_products_hf[ext_products_hf[\"Month\"].year() < 2015]\n",
    "test = ext_products_hf[ext_products_hf[\"Month\"].year() == 2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train GBM Model with Early Stopping\n",
    "\n",
    "lag_features_v2 = H2OGradientBoostingEstimator(model_id = \"lag_features_v2.hex\",\n",
    "                                              stopping_rounds = 3,  stopping_metric = \"MAE\",\n",
    "                                              score_tree_interval = 10, ntrees = 500)\n",
    "lag_features_v2.train(x = ext_predictors,\n",
    "                      y = response,\n",
    "                      training_frame = train,\n",
    "                      validation_frame = test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7 (of 8).  Compare Models\n",
    "\n",
    "Below we compare our models by the Mean Absolute Error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Baseline - MAE: \" + \"{:,}\".format(round(baseline_model.mae(valid = True))))\n",
    "print(\"Lag Features V1 - MAE: \" + \"{:,}\".format(round(lag_features_v1.mae(valid = True))))\n",
    "print(\"Lag Features V2 - MAE: \" + \"{:,}\".format(round(lag_features_v2.mae(valid = True))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lag_features_v2.varimp_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8 (of 8).  Shutdown the Sparkling Water Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stop H2O and Spark services\n",
    "h2o.cluster().shutdown()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Github location for this tutorial\n",
    "\n",
    "* https://github.com/h2oai/h2o-tutorials/tree/master/training/sparkling_water_hands_on/time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySparkling",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
