{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting Stock Volume\n",
    "\n",
    "In this example, we will be forecasting the volume of different Dow Jones stocks for a given day.  The data used is a public Kaggle dataset consisting of stock market data for the DJIA 30: [DJIA Stock Data](https://www.kaggle.com/szrlee/stock-time-series-20050101-to-20171231).\n",
    "\n",
    "We will be using Sparkling Water to ingest the data and add historical lags.\n",
    "\n",
    "Our Machine Learning Workflow is: \n",
    "\n",
    "1. Import data into Spark\n",
    "2. Feature engineering\n",
    "   * Add time lag columns\n",
    "3. Train a single DRF model\n",
    "4. Examine DRF model\n",
    "5. Run AutoML (from Python)\n",
    "6. Watch AutoML progress (in the H2O Flow Web UI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 (of 6).  Import data into Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate H2OContext on top of Spark\n",
    "\n",
    "from pysparkling import *\n",
    "hc = H2OContext.getOrCreate(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([StructField(\"Date\", TimestampType(), True),\n",
    "                     StructField(\"Open\", DoubleType(), True),\n",
    "                     StructField(\"High\", DoubleType(), True),\n",
    "                     StructField(\"Low\", DoubleType(), True),\n",
    "                     StructField(\"Close\", DoubleType(), True),\n",
    "                     StructField(\"Volume\", DoubleType(), True),\n",
    "                     StructField(\"Name\", StringType(), True)])\n",
    "\n",
    "# https://s3.amazonaws.com/h2o-training/events/h2o_world/TimeSeries/all_stocks_2006-01-01_to_2018-01-01.csv\n",
    "stock_df = spark.read.csv(\"data/all_stocks_2006-01-01_to_2018-01-01.csv\", header = True, schema = schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 (of 6).  Feature Engineering\n",
    "\n",
    "We will add new features to our data that can help predict the Volume for a given company.  Features that tell us:\n",
    "* what was the Volume for a company yesterday, two days ago, three days ago?  \n",
    "* what was the Close price, Open price, High price, Low price for a company yesterday?\n",
    "\n",
    "can be very predictive in forecasting.  To create these features we will use PySpark's window function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add Volume from the Previous Day, Previous 2 days, Previous 3 days per Company\n",
    "from pyspark.sql.functions import lag, col\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "w = Window().partitionBy(col(\"Name\")).orderBy(col(\"Date\"))\n",
    "ext_stock_df = stock_df.select(\"*\", lag(\"Volume\", count = 1).over(w).alias(\"Volume_lag1\"),\n",
    "               lag(\"Volume\", count = 2).over(w).alias(\"Volume_lag2\"),\n",
    "               lag(\"Volume\", count = 3).over(w).alias(\"Volume_lag3\")).na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_stock_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add Close, Open, Low, and High by Company for Previous day per Company\n",
    "\n",
    "ext_stock_df = ext_stock_df.select(\"*\", lag(\"Close\", count = 1).over(w).alias(\"Close_lag1\")).na.drop()\n",
    "ext_stock_df = ext_stock_df.select(\"*\", lag(\"Low\", count = 1).over(w).alias(\"Low_lag1\")).na.drop()\n",
    "ext_stock_df = ext_stock_df.select(\"*\", lag(\"High\", count = 1).over(w).alias(\"High_lag1\")).na.drop()\n",
    "ext_stock_df = ext_stock_df.select(\"*\", lag(\"Open\", count = 1).over(w).alias(\"Open_lag1\")).na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert Spark DataFrame to H2O Frame\n",
    "\n",
    "import h2o\n",
    "ext_stock_hf = hc.as_h2o_frame(ext_stock_df, \"stockWithLagsTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert strings to categoricals\n",
    "\n",
    "ext_stock_hf[\"Name\"] = ext_stock_hf[\"Name\"].asfactor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 (of 6).  Train a single DRF model\n",
    "\n",
    "We will train a random forest model with our added lag features as predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Predictors\n",
    "predictors = list(set(ext_stock_hf.col_names) - set([\"Volume\", \"Open\", \"Close\", \"High\", \"Low\"]))\n",
    "response = \"Volume\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing by time\n",
    "# Test data is the last day of data\n",
    "\n",
    "is_test = (ext_stock_hf[\"Date\"].year() == 2017) & \\\n",
    "          (ext_stock_hf[\"Date\"].month() == 12) & \\\n",
    "          (ext_stock_hf[\"Date\"].day() == 29)\n",
    "\n",
    "train = ext_stock_hf[is_test == 0]\n",
    "test = ext_stock_hf[is_test == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "\n",
    "from h2o.estimators import H2ORandomForestEstimator\n",
    "drf_model = H2ORandomForestEstimator(model_id = \"drf_model.hex\",\n",
    "                                     seed = 1234,\n",
    "                                     ntrees = 5)\n",
    "drf_model.train(x = predictors,\n",
    "                y = response,\n",
    "                training_frame = train,\n",
    "                validation_frame = test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 (of 6).  Examine DRF model\n",
    "\n",
    "The Mean Absolute Percent Error is about 20% on our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = drf_model.predict(test)\n",
    "mape = ((test[\"Volume\"] - preds).abs()/test[\"Volume\"]).mean()[0]\n",
    "print(\"Mean Absolute Percent Error: \" + \"{0:.0f}%\".format(100*mape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph below shows the variable importance for the random forest model.  The most important predictors are the volume lags.  We can use the partial dependency plots to see the relationship between these features and the model's prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "drf_model.varimp_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data to common volume\n",
    "max_volume = train[\"Volume\"].quantile(prob = [0.9])[0, 1]\n",
    "pdp_data = train[(train[\"Volume_lag1\"] < max_volume) & \n",
    "                 (train[\"Volume_lag2\"] < max_volume) & \n",
    "                 (train[\"Volume_lag3\"] < max_volume) ]\n",
    "# create pdp's\n",
    "pdps = drf_model.partial_plot(data = pdp_data, cols = [\"Volume_lag1\", \"Volume_lag2\", \"Volume_lag3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partial plots show that the Volume trend tracks the Volume values from the previous days for the company."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 (of 6).  Run AutoML\n",
    "\n",
    "Now we can try running AutoML to see if we can improve the results even further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "auto_ml = H2OAutoML(project_name = \"stock_forecast\",\n",
    "                    max_runtime_secs = 120, \n",
    "                    exclude_algos = [\"DRF\"],\n",
    "                    keep_cross_validation_predictions = False,\n",
    "                    keep_cross_validation_models = False,\n",
    "                    seed = 1234)\n",
    "\n",
    "auto_ml.train(x = predictors,\n",
    "              y = response,\n",
    "              training_frame = train,\n",
    "              leaderboard_frame = test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_ml.leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 (of 6). Watch AutoML progress (in the H2O Flow Web UI)\n",
    "\n",
    "* Go to port 54321\n",
    "* In H2O Flow, go to Admin -> Jobs\n",
    "* Click on the \"Auto Model\" job with the \"stock_forecast\" job name and explore it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Github location for this tutorial\n",
    "\n",
    "* https://github.com/h2oai/h2o-tutorials/tree/master/nyc-workshop-2018/h2o_sw/sparkling-water-hands-on"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySparkling",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
