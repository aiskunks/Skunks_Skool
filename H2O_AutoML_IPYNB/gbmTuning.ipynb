{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBM Tuning\n",
    "\n",
    "This script is meant to show an example of gbm tuning.  It is based off of this blog post: http://blog.h2o.ai/2016/06/h2o-gbm-tuning-tutorial-for-r/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch H2O Cluster\n",
    "import h2o\n",
    "h2o.init(nthreads = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "df = h2o.import_file(path = \"../data/titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert some integer columns to factor/categorical\n",
    "df[\"survived\"] = df[\"survived\"].asfactor()\n",
    "df[\"ticket\"] = df[\"ticket\"].asfactor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set predictors and response variable\n",
    "response = \"survived\"\n",
    "\n",
    "predictors = df.columns\n",
    "predictors.remove(\"survived\")\n",
    "predictors.remove(\"name\")\n",
    "predictors.remove(\"ticket\")\n",
    "predictors.remove(\"home.dest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data for machine learning\n",
    "splits = df.split_frame(ratios = [0.6, 0.2], \n",
    "                        destination_frames = [\"train.hex\", \"valid.hex\", \"test.hex\"],\n",
    "                        seed = 1234)\n",
    "\n",
    "train = splits[0]\n",
    "valid = splits[1]\n",
    "test = splits[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a Baseline\n",
    "\n",
    "from h2o.estimators import H2OGeneralizedLinearEstimator\n",
    "glm_model = H2OGeneralizedLinearEstimator(family = \"binomial\", \n",
    "                                          model_id = \"glm_default.hex\")\n",
    "glm_model.train(x = predictors, y = response, training_frame = train, validation_frame = valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.estimators import H2ORandomForestEstimator\n",
    "drf_model = H2ORandomForestEstimator(model_id = \"drf_default.hex\")\n",
    "drf_model.train(x = predictors, y = response, training_frame = train, validation_frame = valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.estimators import H2OGradientBoostingEstimator\n",
    "gbm_model = H2OGradientBoostingEstimator(model_id = \"gbm_default.hex\")\n",
    "gbm_model.train(x = predictors, y = response, training_frame = train, validation_frame = valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.estimators import H2ODeepLearningEstimator\n",
    "dl_model = H2ODeepLearningEstimator(model_id = \"dl_default.hex\")\n",
    "dl_model.train(x = predictors, y = response, training_frame = train, validation_frame = valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [\"Model\", \"Training AUC\", \"Validation AUC\"]\n",
    "table = [\n",
    "    [\"GLM\", glm_model.auc(train = True), glm_model.auc(valid = True)],\n",
    "    [\"DRF\", drf_model.auc(train = True), drf_model.auc(valid = True)],\n",
    "    [\"GBM\", gbm_model.auc(train = True), gbm_model.auc(valid = True)],\n",
    "    [\"DL\", dl_model.auc(train = True), dl_model.auc(valid = True)]\n",
    "]\n",
    "h2o.display.H2ODisplay(table, header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate GBM Model\n",
    "%matplotlib inline\n",
    "gbm_model.partial_plot(train, cols = [\"sex\", \"age\"], plot = True, plot_stddev = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decrease Learning Rate\n",
    "gbm_learn_rate = H2OGradientBoostingEstimator(learn_rate = 0.05,\n",
    "                                              model_id = \"gbm_learnrate.hex\")\n",
    "gbm_learn_rate.train(x = predictors, y = response, training_frame = train, validation_frame = valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Learn Rate AUC: \" + str(gbm_learn_rate.auc(valid = True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Early Stopping\n",
    "\n",
    "# Early stopping once the moving average (window length = 5) of the validation AUC \n",
    "# doesnâ€™t improve by at least 0.1% for 5 consecutive scoring events\n",
    "    \n",
    "gbm_early_stopping = H2OGradientBoostingEstimator(learn_rate = 0.05,\n",
    "                                                  score_tree_interval = 10, \n",
    "                                                  stopping_rounds = 5, \n",
    "                                                  stopping_metric = \"AUC\", \n",
    "                                                  stopping_tolerance = 0.001,\n",
    "                                                  ntrees = 5000,\n",
    "                                                  model_id = \"gbm_early_stopping.hex\")\n",
    "gbm_early_stopping.train(x = predictors, y = response, training_frame = train, validation_frame = valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Early Stopping AUC: \" + str(gbm_early_stopping.auc(valid = True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import H2O Grid Search\n",
    "from h2o.grid.grid_search import H2OGridSearch\n",
    "\n",
    "# Use Cartesian Grid Search to find best max depth\n",
    "# Max depth can have a big impact on training time so we will first narrow down the best max depths\n",
    "\n",
    "hyper_params = {'max_depth' : list(range(1, 25, 2))}\n",
    "\n",
    "gbm_grid = H2OGradientBoostingEstimator(\n",
    "    ## more trees is better if the learning rate is small enough \n",
    "    ## here, use \"more than enough\" trees - we have early stopping\n",
    "    ntrees = 5000, \n",
    "    \n",
    "    ## smaller learning rate is better\n",
    "    ## since we have learning_rate_annealing, we can afford to start with a bigger learning rate\n",
    "    learn_rate = 0.05,                                                         \n",
    "    \n",
    "    ## learning rate annealing: learning_rate shrinks by 1% after every tree \n",
    "    ## (use 1.00 to disable, but then lower the learning_rate)\n",
    "    learn_rate_annealing = 0.99,                                               \n",
    "    \n",
    "    ## sample 80% of rows per tree\n",
    "    sample_rate = 0.8,                                                       \n",
    "   \n",
    "    ## sample 80% of columns per split\n",
    "    col_sample_rate = 0.8, \n",
    "    \n",
    "    ## fix a random number generator seed for reproducibility\n",
    "    seed = 1234,                                                             \n",
    "    \n",
    "    ## early stopping once the validation AUC doesn't improve by at least 0.01% for 5 consecutive scoring events\n",
    "    stopping_rounds = 5,\n",
    "    stopping_tolerance = 0.001,\n",
    "    stopping_metric = \"AUC\", \n",
    "  \n",
    "    ## score every 10 trees to make early stopping reproducible (it depends on the scoring interval)\n",
    "    score_tree_interval = 10    \n",
    ")\n",
    "\n",
    "# Build grid search with previously made GBM and hyper parameters\n",
    "grid = H2OGridSearch(gbm_grid, \n",
    "                     hyper_params, \n",
    "                     grid_id = 'depth_grid',\n",
    "                     search_criteria = {'strategy': \"Cartesian\"})\n",
    "\n",
    "# Train grid search\n",
    "grid.train(x = predictors, \n",
    "           y = response,\n",
    "           training_frame = train,\n",
    "           validation_frame = valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## by default, display the grid search results sorted by increasing logloss (since this is a classification task)\n",
    "\n",
    "grid   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sort the grid models by decreasing AUC\n",
    "\n",
    "sorted_grid = grid.get_grid(sort_by=\"auc\", decreasing = True)    \n",
    "sorted_grid.sorted_metric_table()[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find the range of max_depth for the top 5 models\n",
    "top_depths = sorted_grid.sorted_metric_table()['max_depth'][0:4] \n",
    "new_min = int(min(top_depths, key = int))\n",
    "new_max = int(max(top_depths, key = int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Random Discrete Hyper-parameterization\n",
    "import math\n",
    "\n",
    "hyper_params_tune = {'max_depth': list(range(new_min, new_max + 1, 1)),\n",
    "                     'sample_rate': [x/100. for x in range(20, 101)],\n",
    "                     'col_sample_rate': [x/100. for x in range(20, 101)],\n",
    "                     'min_rows': [2**x for x in range(0, int(math.log(train.nrow, 2) - 1) + 1)],\n",
    "                     'nbins_cats': [2**x for x in range(4, 13, 1)],\n",
    "                     'histogram_type': [\"UniformAdaptive\", \"QuantilesGlobal\"]\n",
    "                    }\n",
    "\n",
    "search_criteria_tune = {\n",
    "    ## Random grid search\n",
    "    'strategy': \"RandomDiscrete\",\n",
    "    \n",
    "    ## limit the runtime to 60 minutes\n",
    "    'max_runtime_secs': 3600,         \n",
    "  \n",
    "    ## build no more than 100 models\n",
    "    'max_models': 100,                  \n",
    "  \n",
    "    ## random number generator seed to make sampling of parameter combinations reproducible\n",
    "    'seed': 1234,                        \n",
    "  \n",
    "    ## early stopping once the leaderboard of the top 5 models is converged to 0.1% relative difference\n",
    "    'stopping_rounds': 5,                \n",
    "    'stopping_metric': \"AUC\",\n",
    "    'stopping_tolerance': 0.001\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_final_grid = H2OGradientBoostingEstimator(\n",
    "    ## more trees is better if the learning rate is small enough \n",
    "    ## here, use \"more than enough\" trees - we have early stopping\n",
    "    ntrees = 5000, \n",
    "    \n",
    "    ## smaller learning rate is better\n",
    "    ## since we have learning_rate_annealing, we can afford to start with a bigger learning rate\n",
    "    learn_rate = 0.05,                                                         \n",
    "    \n",
    "    ## learning rate annealing: learning_rate shrinks by 1% after every tree \n",
    "    ## (use 1.00 to disable, but then lower the learning_rate)\n",
    "    learn_rate_annealing = 0.99,      \n",
    "    \n",
    "    ## fix a random number generator seed for reproducibility\n",
    "    seed = 1234,                                                             \n",
    "    \n",
    "    ## early stopping once the validation AUC doesn't improve by at least 0.01% for 5 consecutive scoring events\n",
    "    stopping_rounds = 5,\n",
    "    stopping_tolerance = 0.001,\n",
    "    stopping_metric = \"AUC\", \n",
    "    \n",
    "    ## early stopping based on timeout (no model should take more than 1 hour - modify as needed)\n",
    "    max_runtime_secs = 3600,\n",
    "  \n",
    "    ## score every 10 trees to make early stopping reproducible (it depends on the scoring interval)\n",
    "    score_tree_interval = 10    \n",
    ")\n",
    "\n",
    "final_grid = H2OGridSearch(gbm_final_grid, \n",
    "                           hyper_params = hyper_params_tune,\n",
    "                           grid_id = 'final_grid',\n",
    "                           search_criteria = search_criteria_tune)\n",
    "\n",
    "# Train final grid search\n",
    "final_grid.train(x = predictors, \n",
    "                 y = response,\n",
    "                 training_frame = train,\n",
    "                 validation_frame = valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sort the grid models by AUC\n",
    "\n",
    "sorted_final_grid = final_grid.get_grid(sort_by = \"auc\", decreasing = True)    \n",
    "sorted_final_grid.sorted_metric_table()[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Test Scoring\n",
    "# How well does our best model do on the final hold out dataset\n",
    "\n",
    "best_model = h2o.get_model(sorted_final_grid.sorted_metric_table()['model_ids'][0])\n",
    "performance_best_model = best_model.model_performance(test)\n",
    "\n",
    "print(\"AUC on validation: \" + str(best_model.auc(valid = True)))\n",
    "print(\"AUC on test: \" + str(performance_best_model.auc()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown h2o cluster\n",
    "h2o.cluster().shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
