{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT53EbFtHTR5"
      },
      "source": [
        "# Image classification using  Convolutional Neural Network(CNN) which is a Deep learning algorithm used for image classification."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will be using the MNIST dataset for studying Digit recognition.\n",
        "The task is to classify a given image of a handwritten digit into one of 10 classes representing integer values from 0 to 9, inclusively. \n",
        "\n",
        "*To help you understand the fundamentals of deep learning, this notebook will walk through the basic steps of building four models for classifying handwritten numbers with accuracies surpassing 95%. The first model will be a basic fully-connected neural network, and the second model will be a deeper network that introduces the concepts of convolution and pooling.*\n",
        "\n",
        "MNIST is a large database of handwritten digits that is commonly used for training various image processing systems. This database is widely used for training and for testing in the field of machine learning.\n",
        "Here we will be using a deep learning algorithm, CNN (Convolutional Neural Network) with a functional model. The reason for using a functional model is to maintain easiness while connecting the layers.\n",
        "It is one of the best architectures that is being widely used for classifying images.\n",
        "\n",
        "CNNs are used for pattern matching or image detection everywhere in several fields like facial recognition, self-driving cars, object detection, fashion etc. For this notebook, I will be designing a 2-D Convolutional Neural Network model using keras with tensorflow backend for MNIST digit recognition task. \n",
        "\n",
        "The workflow will look like\n",
        "1. Preparing the dataset to work on\n",
        "2. Building and compilation of the model\n",
        "3. Training and evaluating the model\n",
        "\n",
        "The model has two main aspects which are the feature extraction front end comprised of convolutional and pooling layers and the classifier backend that will make a prediction.\n",
        "\n",
        "For a better understanding, model architecture that may be followed here is 2 convolution layers followed by pooling layer, a fully connected layer and softmax layer respectively. Multiple filters are used at each convolution layer, for different types of feature extraction. One intuitive explanation can be if first filter helps in detecting the straight lines in the image, second filter will help in detecting circles and so on.\n",
        "A solution for overfitting problem is also needed."
      ],
      "metadata": {
        "id": "39fBAcfOZrzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset that is used or worked upon is the MNIST dataset which is a large dataset that contains images of handwritten digits from 0 to 9.\n",
        "Keras is the library from which  you can use the MNIST dataset for digit recognition.\n",
        "The database contains 60,000 training images and 10,000 testing images each of size 28x28. The first step is to load the dataset, which can be easily done through the keras api."
      ],
      "metadata": {
        "id": "NywIZhzXaQgs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpKYcv-UHTR6"
      },
      "source": [
        "## What is deep learning?\n",
        "\n",
        "Deep learning is a type of machine learning, which is a subset of artificial intelligence. Machine learning is about computers being able to think and act with less human intervention; deep learning is about computers learning to think using structures modeled on the human brain\n",
        "\n",
        "Deep learning is inspired by the functionality of human brain cells called artificial neural network.\n",
        "Deep learning is one of machine learning algorithm that takes data in form of images, videos, text and connections between all the neurons are adjusted according to the pattern of the data.\n",
        "\n",
        "Deep learning is a sub field within machine learning used for learning multiple levels of representation in order to model complex connection between the data.\n",
        "Higher-level features are defined with the help of lower levels and such hierarchy of features is called deep architecture. If we draw a graph showing how these concepts are built over each other the graph is deep with many layers. Hence, we call this learning approach as deep learning.\n",
        "\n",
        "Practical examples of deep learning are Virtual assistants, vision for driverless cars, money laundering, face recognition and many more\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wol72vjWHTR7"
      },
      "source": [
        "## CNN- Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNINOr_YHTR7"
      },
      "source": [
        "A Convolutional Neural Network (ConvNet/CNN) is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image and be able to differentiate one from the other.\n",
        "\n",
        "CNN is a supervised type of Deep learning, most preferable used in image recognition and computer vision. Convolutional Networks work by moving small filters across the input image. This means the filters are re-used for recognizing patterns throughout the entire input image. This makes the Convolutional Networks much more powerful than Fully-Connected networks with the same number of variables.\n",
        "\n",
        "A convolution in CNN is nothing but a element wise multiplication i.e. dot product of the image matrix and the filter.\n",
        "\n",
        "\n",
        "![convolution.gif](https://miro.medium.com/max/1400/1*vkQ0hXDaQv57sALXAJquxA.jpeg)\n",
        "\n",
        "The image above  shows us the various layers in a CNN. A convolution operation takes place between the image and the filter and the convolved feature is generated. Each filter in a CNN, learns different characteristic of an image.\n",
        "\n",
        "\n",
        "The input image is processed in the first convolutional layer using the filter-weights. This results in 16 new images, one for each filter in the convolutional layer. The images are also down-sampled so the image resolution is decreased from 28x28 to 14x14.\n",
        "\n",
        "These 16 smaller images are then processed in the second convolutional layer. We need filter-weights for each of these 16 channels, and we need filter-weights for each output channel of this layer. There are 36 output channels so there are a total of 16 x 36 = 576 filters in the second convolutional layer. The resulting images are down-sampled again to 7x7 pixels.\n",
        "\n",
        "The output of the second convolutional layer is 36 images of 7x7 pixels each. These are then flattened to a single vector of length 7 x 7 x 36 = 1764, which is used as the input to a fully-connected layer with 128 neurons (or elements). This feeds into another fully-connected layer with 10 neurons, one for each of the classes, which is used to determine the class of the image, that is, which number is depicted in the image.\n",
        "\n",
        "The convolutional filters are initially chosen at random, so the classification is done randomly. The error between the predicted and true class of the input image is measured as the so-called cross-entropy. The optimizer then automatically propagates this error back through the Convolutional Network using the chain-rule of differentiation and updates the filter-weights so as to improve the classification error. This is done iteratively thousands of times until the classification error is sufficiently low."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjXy1tXxHTR7"
      },
      "source": [
        "Installing the necessary libraries.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbhmj4AkHTR8"
      },
      "source": [
        "### Installing Keras\n",
        "\n",
        "Keras is a high-level neural network API, written in Python which runs on top of either Tensorflow or Theano. You can install Keras from <a href=\"https://keras.io/#installation\">here</a>\n",
        "\n",
        "Keras was written to simplify the construction of neural nets, as tensorflow’s API is very verbose. Keras makes everything very easy and you will see it in action below. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "collapsed": true,
        "id": "_DiwDXs9HTR8"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Convolution2D, MaxPooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adagrad\n",
        "from tensorflow.keras.optimizers import Adamax\n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "from keras.utils import np_utils\n",
        "import keras\n",
        "from keras import backend as K\n",
        "import warnings; \n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "#adagrad,adamax,nadam\n",
        "from keras.datasets import mnist\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Task for the AI\n",
        "Our goal is to construct and train an artificial neural network on thousands of images of handwritten digits so that it may successfully identify others when presented. The data that will be incorporated is the MNIST database which contains 60,000 images for training and 10,000 test images. We will use the Keras Python API with TensorFlow as the backend."
      ],
      "metadata": {
        "id": "_BtU5HGdz9k-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Is0RpWOHTR8"
      },
      "source": [
        "###  Load MNIST dataset\n",
        "\n",
        "![mnist-IMG.png](https://miro.medium.com/max/800/1*LyRlX__08q40UJohhJG9Ow.png)\n",
        "\n",
        "The MNIST dataset is provided by Keras.\n",
        "It is a large dataset that contains images of handwritten digits from 0 to 9.\n",
        "Keras is the library from which I’ll be using the MNIST dataset for digit recognition.\n",
        "\n",
        "The database contains 60,000 training images and 10,000 testing images each of size 28x28. The first step is to load the dataset, which can be easily done through the keras api.\n",
        "\n",
        "The shape of X_train is (60000, 28, 28). Each image has 28 x 28 resolution. The shape of X_test is (10000, 28, 28).\n",
        "\n",
        "The input shape that a CNN accepts should be in a specific format. If you are using Tensorflow, the format should be (batch, height, width, channels). If you are using Theano, the format should be (batch, channels, height, width)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "id": "d1FUXB5CHTR9"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "nb_classes = 10\n",
        "#epochs = 10\n",
        "\n",
        "img_rows, img_cols = 28, 28         # input image dimensions\n",
        "pool_size = (2, 2)                  # size of pooling area for max pooling\n",
        "prob_drop_conv = 0.2                # drop probability for dropout @ conv layer\n",
        "prob_drop_hidden = 0.5              # drop probability for dropout @ fc layer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Training Data\n",
        "The MNIST dataset is conveniently bundled within Keras, and we can easily analyze some of its features in Python."
      ],
      "metadata": {
        "id": "1YlWGmA20LWT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKH37yazHTR9",
        "outputId": "62b77f73-2a85-4dc9-ebcc-95fcb9ca3cca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train original shape: (60000, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "print('X_train original shape:', X_train.shape)\n",
        "\n",
        "#if K.image_dim_ordering() == 'th':\n",
        "if K.image_data_format() == 'th':\n",
        "    # For Theano backend\n",
        "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
        "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    # For TensorFlow backend\n",
        "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
        "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8inpmprHTR9"
      },
      "source": [
        "### Reshaping of images\n",
        "The shape of X_train is (60000, 28, 28, 1). As all the images are in grayscale, the number of channels is 1. If it was a color image, then the number of channels would be 3 (R, G, B).\n",
        "\n",
        "Here we’ve rescaled the image data so that each pixel lies in the interval [0, 1] instead of [0, 255]. It is always a good idea to normalize the input so that each dimension has approximately the same scale.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRJpc78tHTR-",
        "outputId": "3078045a-0c95-4f66-fb8b-2a887a268fa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ]
        }
      ],
      "source": [
        "X_train = X_train.astype('float32') / 255.\n",
        "X_test = X_test.astype('float32') / 255.\n",
        "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "print('X_train shape:', X_train.shape)\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKezv4yyHTR-"
      },
      "source": [
        "### Convolutional layers, Max Pooling layers, Dropout values and Activation Functions\n",
        "\n",
        "\n",
        "Keras allows us to specify the number of filters we want and the size of the filters. So, in our first layer, 32 is number of filters and (3, 3) is the size of the filter. We also need to specify the shape of the input which is (28, 28, 1), but we have to specify it only once.\n",
        "\n",
        "The second layer is the <b>Activation layer</b>. \n",
        "\n",
        "The third layer is the <b>MaxPooling layer</b>. MaxPooling layer is used to down-sample the input to enable the model to make assumptions about the features so as to reduce over-fitting. It also reduces the number of parameters to learn, reducing the training time.\n",
        "\n",
        "After creating all the convolutional layers, we need to flatten them, so that they can act as an input to the Dense layers.\n",
        "\n",
        "<b>Dense layers</b> are keras’s alias for Fully connected layers. These layers give the ability to classify the features learned by the CNN.\n",
        "\n",
        "<b>Dropout</b> is the method used to reduce overfitting. It forces the model to learn multiple independent representations of the same data by randomly disabling neurons in the learning phase. In our model, dropout will randomnly disable 20% of the neurons.\n",
        "\n",
        "The second last layer is the Dense layer with 10 neurons. The neurons in this layer should be equal to the number of classes we want to predict as this is the output layer.\n",
        "\n",
        "The last layer is the <b>Softmax Activation layer</b>. Softmax activation enables us to calculate the output based on the probabilities. Each class is assigned a probability and the class with the maximum probability is the model’s output for the input."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introducing Convolution! What is it?"
      ],
      "metadata": {
        "id": "Iw323hmX0spR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before, we built a network that accepts the normalized pixel values of each value and operates soley on those values. What if we could instead feed different features (e.g. **curvature, edges**) of each image into a network, and have the network learn which features are important for classifying an image?\n",
        "\n",
        "This possible through convolution! Convolution applies **kernels** (filters) that traverse through each image and generate **feature maps**.\n",
        "\n",
        "![convolution.gif](https://eg.bucknell.edu/~cld028/courses/379-FA19/images/ConvEx.gif)"
      ],
      "metadata": {
        "id": "4-S99X790hpM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above example, the image is a 5 x 5 matrix and the kernel going over it is a 3 x 3 matrix. A dot product operation takes place between the image and the kernel and the convolved feature is generated. Each kernel in a CNN learns a different characteristic of an image.\n",
        "\n",
        "Kernels are often used in photoediting software to apply blurring, edge detection, sharpening, etc."
      ],
      "metadata": {
        "id": "z4sLIZ2T2fPA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kernels in deep learning networks are used in similar ways, i.e. highlighting some feature. Combined with a system called **max pooling**, the non-highlighted elements are discarded from each feature map, leaving only the features of interest, reducing the number of learned parameters, and decreasing the computational cost (e.g. system memory)."
      ],
      "metadata": {
        "id": "0bhIaedl2lrd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also take convolutions of convolutions -- we can stack as many convolutions as we want, as long as there are enough pixels to fit a kernel."
      ],
      "metadata": {
        "id": "lqfs1bmG2w3y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX0LxbAoHTR-"
      },
      "source": [
        "# Case 1\n",
        "## Convolutional model\n",
        "\n",
        "The convolutional model consists of 3 convolutional layers and two fully connected layers\n",
        "\n",
        "<b>Activation function</b> - Activation function calculates a “weighted sum” of its input, adds a bias and then decides whether it should be “fired” or not.\n",
        "\n",
        "ReLU(Rectified Linear Unit) - A(x) = max(0,x)\n",
        "\n",
        "The ReLu function is as shown above. It gives an output x if x is positive and 0 otherwise.\n",
        "\n",
        "ReLu is nonlinear in nature.\n",
        "\n",
        "<b>Output optimization function</b> - \n",
        "softmax - The softmax function squashes the outputs of each unit to be between 0 and 1. It also divides each output such that the total sum of the outputs is equal to 1. The output of the softmax function is equivalent to a categorical probability distribution, it tells you the probability that any of the classes are true.\n",
        "\n",
        "<b>Loss function</b> - loss function or cost function is a function that maps an event or values of one or more variables onto a real number intuitively representing some \"cost\" associated with the event.\n",
        "categorical_crossentropy - Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label.\n",
        "\n",
        "<b>Optimizer</b>- The optimizer automatically propagates this error back through the Convolutional Network using the chain-rule of differentiation and updates the filter-weights so as to improve the classification error.\n",
        "\n",
        "RMSprop - Its one of the adaptive learning rate optimization algorithm, Root Mean Square Prop (RMSProp) works by keeping an exponentially weighted average of the squares of past gradients. RMSProp then divides the learning rate by this average to speed up convergence.\n",
        "\n",
        "<b>Network initialization</b>- Initializations define the way to set the initial random weights of Keras layers.\n",
        "he_normal - \n",
        "It draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / fan_in) where fan_in is the number of input units in the weight tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGaoopZmHTR-",
        "outputId": "77d4d335-dc0d-4218-9e0b-a5b2759d1853"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_2 (Conv2D)           (None, 10, 10, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 5, 5, 32)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 5, 5, 32)          0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 2, 2, 64)          18496     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 1, 1, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 1, 1, 64)          0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 1, 1, 128)         73856     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 1, 1, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 128)               0         \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 625)               80625     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 625)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                6260      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 179,557\n",
            "Trainable params: 179,557\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Convolutional model\n",
        "model = Sequential()\n",
        "\n",
        "# conv1 layer\n",
        "model.add(Convolution2D(32, 3, 3, padding='same', activation='relu', input_shape=input_shape, kernel_initializer='he_normal'))\n",
        "model.add(MaxPooling2D(pool_size=pool_size, strides=(2,2), padding='same'))\n",
        "model.add(Dropout(prob_drop_conv))\n",
        "\n",
        "# conv2 layer\n",
        "model.add(Convolution2D(64, 3, 3, padding='same', activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(MaxPooling2D(pool_size=pool_size, strides=(2,2), padding='same'))\n",
        "model.add(Dropout(prob_drop_conv))\n",
        "\n",
        "# conv3 layer\n",
        "model.add(Convolution2D(128, 3, 3, padding='same', activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(MaxPooling2D(pool_size=pool_size, strides=(2,2), padding='same'))\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(prob_drop_conv))\n",
        "\n",
        "# fc1 layer\n",
        "model.add(Dense(625, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dropout(prob_drop_hidden))\n",
        "\n",
        "# fc2 layer\n",
        "model.add(Dense(10, activation='softmax', kernel_initializer='he_normal'))\n",
        "\n",
        "opt = RMSprop(lr=0.001, rho=0.9)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtgMxWsYHTR-",
        "outputId": "107659e8-1958-4136-d205-025a6948cabe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "469/469 [==============================] - 10s 20ms/step - loss: 0.8913 - accuracy: 0.6945\n",
            "Epoch 2/10\n",
            "469/469 [==============================] - 9s 20ms/step - loss: 0.4507 - accuracy: 0.8533\n",
            "Epoch 3/10\n",
            "469/469 [==============================] - 9s 20ms/step - loss: 0.3523 - accuracy: 0.8886\n",
            "Epoch 4/10\n",
            "469/469 [==============================] - 9s 20ms/step - loss: 0.3007 - accuracy: 0.9059\n",
            "Epoch 5/10\n",
            "469/469 [==============================] - 9s 20ms/step - loss: 0.2748 - accuracy: 0.9147\n",
            "Epoch 6/10\n",
            "469/469 [==============================] - 10s 21ms/step - loss: 0.2507 - accuracy: 0.9219\n",
            "Epoch 7/10\n",
            "469/469 [==============================] - 9s 20ms/step - loss: 0.2354 - accuracy: 0.9272\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - 9s 20ms/step - loss: 0.2267 - accuracy: 0.9295\n",
            "Epoch 9/10\n",
            "469/469 [==============================] - 9s 20ms/step - loss: 0.2145 - accuracy: 0.9332\n",
            "Epoch 10/10\n",
            "469/469 [==============================] - 9s 19ms/step - loss: 0.2137 - accuracy: 0.9351\n"
          ]
        }
      ],
      "source": [
        "# Train\n",
        "history = model.fit(X_train, Y_train, epochs=10, batch_size=batch_size, shuffle=True, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrLPuW4gHTR_",
        "outputId": "92874698-7f62-486c-a4a7-6f05783500e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40/40 [==============================] - 1s 9ms/step - loss: 0.1169 - accuracy: 0.9646\n",
            "Summary: Loss over the test dataset: 0.12, Accuracy: 0.9646\n"
          ]
        }
      ],
      "source": [
        "# Evaluate\n",
        "evaluation = model.evaluate(X_test, Y_test, batch_size=256, verbose=1)\n",
        "print('Summary: Loss over the test dataset: %.2f, Accuracy: %.4f' % (evaluation[0], evaluation[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "CjIwYBqeHTR_"
      },
      "source": [
        "## The accuracy of the model  is 99.4%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "VSUtH4S2HTR_"
      },
      "source": [
        "# Case 2 \n",
        "## Convolutional model\n",
        "\n",
        "The convolutional model consists of 3 convolutional layers and two fully connected layers\n",
        "\n",
        "<b>Activation function</b> - Activation function calculates a “weighted sum” of its input, adds a bias and then decides whether it should be “fired” or not.\n",
        "\n",
        "ReLU(Rectified Linear Unit) - A(x) = max(0,x)\n",
        "\n",
        "The ReLu function is as shown above. It gives an output x if x is positive and 0 otherwise.\n",
        "ReLu is nonlinear in nature.\n",
        "\n",
        "<b>Output optimization function</b> - \n",
        "softmax - The softmax function squashes the outputs of each unit to be between 0 and 1. It also divides each output such that the total sum of the outputs is equal to 1. The output of the softmax function is equivalent to a categorical probability distribution, it tells you the probability that any of the classes are true.\n",
        "\n",
        "<b>Loss function</b> - loss function or cost function is a function that maps an event or values of one or more variables onto a real number intuitively representing some \"cost\" associated with the event.\n",
        "\n",
        "<b>Optimizer</b>- The optimizer automatically propagates this error back through the Convolutional Network using the chain-rule of differentiation and updates the filter-weights so as to improve the classification error.\n",
        "adagrad - AdaGrad is an optimization method that allows different step sizes for different features. It increases the influence of rare but informative features.\n",
        "\n",
        "<b>network initialization</b>- Initializations define the way to set the initial random weights of Keras layers.\n",
        "uniform - Initializer that generates tensors with a uniform distribution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8Gj5Y_AHTR_",
        "outputId": "950737db-74ce-47a4-d861-e45b35831b7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_23 (Conv2D)          (None, 10, 10, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d_22 (MaxPoolin  (None, 5, 5, 32)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_29 (Dropout)        (None, 5, 5, 32)          0         \n",
            "                                                                 \n",
            " conv2d_24 (Conv2D)          (None, 2, 2, 64)          18496     \n",
            "                                                                 \n",
            " max_pooling2d_23 (MaxPoolin  (None, 1, 1, 64)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_30 (Dropout)        (None, 1, 1, 64)          0         \n",
            "                                                                 \n",
            " conv2d_25 (Conv2D)          (None, 1, 1, 128)         73856     \n",
            "                                                                 \n",
            " max_pooling2d_24 (MaxPoolin  (None, 1, 1, 128)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 128)               0         \n",
            "                                                                 \n",
            " dropout_31 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 625)               80625     \n",
            "                                                                 \n",
            " dropout_32 (Dropout)        (None, 625)               0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 10)                6260      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 179,557\n",
            "Trainable params: 179,557\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Convolutional model\n",
        "model2 = Sequential()\n",
        "\n",
        "# conv1 layer\n",
        "model2.add(Convolution2D(32, 3, 3, padding='same', activation='relu', input_shape=input_shape, kernel_initializer='uniform'))\n",
        "model2.add(MaxPooling2D(pool_size=pool_size, strides=(2,2), padding='same'))\n",
        "model2.add(Dropout(prob_drop_conv))\n",
        "\n",
        "# conv2 layer\n",
        "model2.add(Convolution2D(64, 3, 3, padding='same', activation='relu', kernel_initializer='uniform'))\n",
        "model2.add(MaxPooling2D(pool_size=pool_size, strides=(2,2), padding='same'))\n",
        "model2.add(Dropout(prob_drop_conv))\n",
        "\n",
        "# conv3 layer\n",
        "model2.add(Convolution2D(128, 3, 3, padding='same', activation='relu', kernel_initializer='uniform'))\n",
        "model2.add(MaxPooling2D(pool_size=pool_size, strides=(2,2), padding='same'))\n",
        "model2.add(Flatten())\n",
        "model2.add(Dropout(prob_drop_conv))\n",
        "\n",
        "# fc1 layer\n",
        "model2.add(Dense(625, activation='relu', kernel_initializer='uniform'))\n",
        "model2.add(Dropout(prob_drop_hidden))\n",
        "\n",
        "# fc2 layer\n",
        "model2.add(Dense(10, activation='softmax', kernel_initializer='uniform'))\n",
        "\n",
        "model2.compile(optimizer=Adagrad(), loss='poisson', metrics=['accuracy'])\n",
        "model2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxZGdGQJHTR_",
        "outputId": "026bf99d-0a3e-4b3b-c467-0480032325e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "469/469 [==============================] - 10s 19ms/step - loss: 0.3303 - accuracy: 0.1233\n",
            "Epoch 2/10\n",
            "469/469 [==============================] - 10s 20ms/step - loss: 0.3302 - accuracy: 0.1253\n",
            "Epoch 3/10\n",
            "469/469 [==============================] - 9s 19ms/step - loss: 0.3302 - accuracy: 0.1145\n",
            "Epoch 4/10\n",
            "469/469 [==============================] - 9s 19ms/step - loss: 0.3302 - accuracy: 0.1125\n",
            "Epoch 5/10\n",
            "469/469 [==============================] - 9s 19ms/step - loss: 0.3302 - accuracy: 0.1124\n",
            "Epoch 6/10\n",
            "469/469 [==============================] - 9s 19ms/step - loss: 0.3302 - accuracy: 0.1124\n",
            "Epoch 7/10\n",
            "469/469 [==============================] - 9s 19ms/step - loss: 0.3302 - accuracy: 0.1124\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - 9s 19ms/step - loss: 0.3302 - accuracy: 0.1124\n",
            "Epoch 9/10\n",
            "469/469 [==============================] - 9s 19ms/step - loss: 0.3302 - accuracy: 0.1124\n",
            "Epoch 10/10\n",
            "469/469 [==============================] - 9s 19ms/step - loss: 0.3302 - accuracy: 0.1124\n"
          ]
        }
      ],
      "source": [
        "# Train\n",
        "case2 = model2.fit(X_train, Y_train, epochs=10, batch_size=batch_size, shuffle=True, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygU4URyuHTR_",
        "outputId": "03280658-dd21-4c77-8ef7-0e3ab2976bc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 4s 383us/step\n",
            "Summary: Loss over the test dataset: 0.10, Accuracy: 0.9938\n"
          ]
        }
      ],
      "source": [
        "# Evaluate\n",
        "evaluation2 = model2.evaluate(X_test, Y_test, batch_size=256, verbose=1)\n",
        "print('Summary: Loss over the test dataset: %.2f, Accuracy: %.4f' % (evaluation2[0], evaluation2[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M60C_UDHHTR_"
      },
      "source": [
        "## The accuracy predicted from case 2 is 99.38%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g67ho9EHTR_"
      },
      "source": [
        "# Case 3\n",
        "## Convolutional model\n",
        "\n",
        "The convolutional model consists of 4 convolutional layers and two fully connected layers\n",
        "\n",
        "<b>Activation function</b> - Activation function calculates a “weighted sum” of its input, adds a bias and then decides whether it should be “fired” or not.\n",
        "Linear - A = cx\n",
        "A straight line function where activation is proportional to input ( which is the weighted sum from neuron ).\n",
        "\n",
        "<b>Output optimization function</b> - \n",
        "softmax - The softmax function squashes the outputs of each unit to be between 0 and 1. It also divides each output such that the total sum of the outputs is equal to 1. The output of the softmax function is equivalent to a categorical probability distribution, it tells you the probability that any of the classes are true.\n",
        "\n",
        "<b>Loss function</b> - loss function or cost function is a function that maps an event or values of one or more variables onto a real number intuitively representing some \"cost\" associated with the event.\n",
        "kullback_leibler_divergence - The Kullback–Leibler divergence (also called relative entropy) is a measure of how one probability distribution diverges from a second, expected probability distribution.\n",
        "\n",
        "<b>Optimizer</b>- The optimizer automatically propagates this error back through the Convolutional Network using the chain-rule of differentiation and updates the filter-weights so as to improve the classification error.\n",
        "adamax - It is a variant of Adam based on the infinity norm\n",
        "\n",
        "<b>network initialization</b>- Initializations define the way to set the initial random weights of Keras layers.\n",
        "uniform - Initializer that generates tensors with a uniform distribution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLkzIbvJHTSA",
        "outputId": "ed1ee609-88d5-4aa4-cf13-ca7799d7a80e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_26 (Conv2D)          (None, 10, 10, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d_25 (MaxPoolin  (None, 5, 5, 32)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_33 (Dropout)        (None, 5, 5, 32)          0         \n",
            "                                                                 \n",
            " conv2d_27 (Conv2D)          (None, 2, 2, 64)          18496     \n",
            "                                                                 \n",
            " max_pooling2d_26 (MaxPoolin  (None, 1, 1, 64)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_34 (Dropout)        (None, 1, 1, 64)          0         \n",
            "                                                                 \n",
            " conv2d_28 (Conv2D)          (None, 1, 1, 128)         73856     \n",
            "                                                                 \n",
            " max_pooling2d_27 (MaxPoolin  (None, 1, 1, 128)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_35 (Dropout)        (None, 1, 1, 128)         0         \n",
            "                                                                 \n",
            " conv2d_29 (Conv2D)          (None, 1, 1, 256)         295168    \n",
            "                                                                 \n",
            " max_pooling2d_28 (MaxPoolin  (None, 1, 1, 256)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 256)               0         \n",
            "                                                                 \n",
            " dropout_36 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 625)               160625    \n",
            "                                                                 \n",
            " dropout_37 (Dropout)        (None, 625)               0         \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 10)                6260      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 554,725\n",
            "Trainable params: 554,725\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Convolutional model\n",
        "model3 = Sequential()\n",
        "\n",
        "# conv1 layer\n",
        "model3.add(Convolution2D(32, 3, 3, padding='same', activation='linear', input_shape=input_shape, kernel_initializer='uniform'))\n",
        "model3.add(MaxPooling2D(pool_size=pool_size, strides=(2,2), padding='same'))\n",
        "model3.add(Dropout(prob_drop_conv))\n",
        "\n",
        "# conv2 layer\n",
        "model3.add(Convolution2D(64, 3, 3, padding='same', activation='linear', kernel_initializer='uniform'))\n",
        "model3.add(MaxPooling2D(pool_size=pool_size, strides=(2,2), padding='same'))\n",
        "model3.add(Dropout(prob_drop_conv))\n",
        "\n",
        "# conv3 layer\n",
        "model3.add(Convolution2D(128, 3, 3, padding='same', activation='linear', kernel_initializer='uniform'))\n",
        "model3.add(MaxPooling2D(pool_size=pool_size, strides=(2,2), padding='same'))\n",
        "model3.add(Dropout(prob_drop_conv)) \n",
        "\n",
        "# conv4 layer\n",
        "model3.add(Convolution2D(256, 3, 3, padding='same', activation='linear', kernel_initializer='uniform'))\n",
        "model3.add(MaxPooling2D(pool_size=pool_size, strides=(2,2), padding='same'))\n",
        "model3.add(Flatten())\n",
        "model3.add(Dropout(prob_drop_conv))\n",
        "\n",
        "# fc1 layer\n",
        "model3.add(Dense(625, activation='linear', kernel_initializer='uniform'))\n",
        "model3.add(Dropout(prob_drop_hidden))\n",
        "\n",
        "# fc2 layer\n",
        "model3.add(Dense(10, activation='softmax', kernel_initializer='uniform'))\n",
        "\n",
        "model3.compile(optimizer=Adamax(), loss='kullback_leibler_divergence', metrics=['accuracy'])\n",
        "model3.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-bET_QRHTSA",
        "outputId": "0ccd991c-f226-4587-b6a2-95af15eb151b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "469/469 [==============================] - 50s 105ms/step - loss: 1.1833 - accuracy: 0.5656\n",
            "Epoch 2/20\n",
            "469/469 [==============================] - 53s 113ms/step - loss: 0.5505 - accuracy: 0.8228\n",
            "Epoch 3/20\n",
            "469/469 [==============================] - 50s 107ms/step - loss: 0.3916 - accuracy: 0.8752\n",
            "Epoch 4/20\n",
            "469/469 [==============================] - 48s 103ms/step - loss: 0.3328 - accuracy: 0.8964\n",
            "Epoch 5/20\n",
            "469/469 [==============================] - 49s 104ms/step - loss: 0.2978 - accuracy: 0.9077\n",
            "Epoch 6/20\n",
            "469/469 [==============================] - 49s 103ms/step - loss: 0.2738 - accuracy: 0.9137\n",
            "Epoch 7/20\n",
            "469/469 [==============================] - 49s 104ms/step - loss: 0.2572 - accuracy: 0.9201\n",
            "Epoch 8/20\n",
            "469/469 [==============================] - 49s 104ms/step - loss: 0.2412 - accuracy: 0.9244\n",
            "Epoch 9/20\n",
            "469/469 [==============================] - 49s 104ms/step - loss: 0.2284 - accuracy: 0.9288\n",
            "Epoch 10/20\n",
            "469/469 [==============================] - 49s 104ms/step - loss: 0.2168 - accuracy: 0.9319\n",
            "Epoch 11/20\n",
            "469/469 [==============================] - 49s 104ms/step - loss: 0.2103 - accuracy: 0.9345\n",
            "Epoch 12/20\n",
            "469/469 [==============================] - 49s 104ms/step - loss: 0.2017 - accuracy: 0.9377\n",
            "Epoch 13/20\n",
            "469/469 [==============================] - 49s 105ms/step - loss: 0.1955 - accuracy: 0.9393\n",
            "Epoch 14/20\n",
            "469/469 [==============================] - 49s 104ms/step - loss: 0.1910 - accuracy: 0.9405\n",
            "Epoch 15/20\n",
            "469/469 [==============================] - 49s 104ms/step - loss: 0.1839 - accuracy: 0.9415\n",
            "Epoch 16/20\n",
            "469/469 [==============================] - 49s 105ms/step - loss: 0.1854 - accuracy: 0.9424\n",
            "Epoch 17/20\n",
            "469/469 [==============================] - 49s 104ms/step - loss: 0.1774 - accuracy: 0.9440\n",
            "Epoch 18/20\n",
            "469/469 [==============================] - 49s 104ms/step - loss: 0.1730 - accuracy: 0.9448\n",
            "Epoch 19/20\n",
            "469/469 [==============================] - 49s 105ms/step - loss: 0.1720 - accuracy: 0.9454\n",
            "Epoch 20/20\n",
            "469/469 [==============================] - 49s 104ms/step - loss: 0.1693 - accuracy: 0.9458\n"
          ]
        }
      ],
      "source": [
        "# Train\n",
        "case3 = model3.fit(X_train, Y_train, epochs=20, batch_size=batch_size, shuffle=True, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHQcXOXQHTSA",
        "outputId": "c286321a-8a03-4bb3-8cbd-a0553595c665"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 7s 667us/step\n",
            "Summary: Loss over the test dataset: 0.03, Accuracy: 0.9934\n"
          ]
        }
      ],
      "source": [
        "# Evaluate\n",
        "evaluation3 = model3.evaluate(X_test, Y_test, batch_size=256, verbose=1)\n",
        "print('Summary: Loss over the test dataset: %.2f, Accuracy: %.4f' %  (evaluation3[0], evaluation3[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOvZX53dHTSA"
      },
      "source": [
        "## The accuracy of the model for case 3 is 99.34%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7JEZ6uvHTSA"
      },
      "source": [
        "# Case 4\n",
        "## Convolutional model\n",
        "\n",
        "The convolutional model consists of 3 convolutional layers and two fully connected layers\n",
        "\n",
        "<b>Activation function</b> - Activation function calculates a “weighted sum” of its input, adds a bias and then decides whether it should be “fired” or not.\n",
        "ReLU(Rectified Linear Unit) -\n",
        "A(x) = max(0,x)\n",
        "\n",
        "The ReLu function is as shown above. It gives an output x if x is positive and 0 otherwise.\n",
        "ReLu is nonlinear in nature.\n",
        "\n",
        "<b>Output optimization function</b> - \n",
        "softmax - The softmax function squashes the outputs of each unit to be between 0 and 1. It also divides each output such that the total sum of the outputs is equal to 1. The output of the softmax function is equivalent to a categorical probability distribution, it tells you the probability that any of the classes are true.\n",
        "\n",
        "<b>Loss function</b> - loss function or cost function is a function that maps an event or values of one or more variables onto a real number intuitively representing some \"cost\" associated with the event.\n",
        "categorical_crossentropy - Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label.\n",
        "\n",
        "<b>Optimizer</b>- The optimizer automatically propagates this error back through the Convolutional Network using the chain-rule of differentiation and updates the filter-weights so as to improve the classification error.\n",
        "RMSprop - Its one of the adaptive learning rate optimization algorithm, Root Mean Square Prop (RMSProp) works by keeping an exponentially weighted average of the squares of past gradients. RMSProp then divides the learning rate by this average to speed up convergence.\n",
        "\n",
        "<b>network initialization</b>- Initializations define the way to set the initial random weights of Keras layers.\n",
        "he_normal - \n",
        "It draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / fan_in) where fan_in is the number of input units in the weight tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAtr3XBUHTSA",
        "outputId": "9e5bcd65-4d30-4340-ccf7-d7e6d98c4df9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_30 (Conv2D)          (None, 10, 10, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d_29 (MaxPoolin  (None, 5, 5, 32)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_38 (Dropout)        (None, 5, 5, 32)          0         \n",
            "                                                                 \n",
            " conv2d_31 (Conv2D)          (None, 2, 2, 64)          18496     \n",
            "                                                                 \n",
            " max_pooling2d_30 (MaxPoolin  (None, 1, 1, 64)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_39 (Dropout)        (None, 1, 1, 64)          0         \n",
            "                                                                 \n",
            " conv2d_32 (Conv2D)          (None, 1, 1, 128)         73856     \n",
            "                                                                 \n",
            " max_pooling2d_31 (MaxPoolin  (None, 1, 1, 128)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_9 (Flatten)         (None, 128)               0         \n",
            "                                                                 \n",
            " dropout_40 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 625)               80625     \n",
            "                                                                 \n",
            " dropout_41 (Dropout)        (None, 625)               0         \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 10)                6260      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 179,557\n",
            "Trainable params: 179,557\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Convolutional model\n",
        "model5 = Sequential()\n",
        "\n",
        "# conv1 layer\n",
        "model5.add(Convolution2D(32, 3, 3, padding='same', activation='relu', input_shape=input_shape, kernel_initializer='he_normal'))\n",
        "model5.add(MaxPooling2D(pool_size=pool_size, strides=(2,2), padding='same'))\n",
        "model5.add(Dropout(prob_drop_conv))\n",
        "\n",
        "# conv2 layer\n",
        "model5.add(Convolution2D(64, 3, 3, padding='same', activation='relu', kernel_initializer='he_normal'))\n",
        "model5.add(MaxPooling2D(pool_size=pool_size, strides=(2,2), padding='same'))\n",
        "model5.add(Dropout(prob_drop_conv))\n",
        "\n",
        "# conv3 layer\n",
        "model5.add(Convolution2D(128, 3, 3, padding='same', activation='relu', kernel_initializer='he_normal'))\n",
        "model5.add(MaxPooling2D(pool_size=pool_size, strides=(2,2), padding='same'))\n",
        "model5.add(Flatten())\n",
        "model5.add(Dropout(prob_drop_conv)) \n",
        "\n",
        "# fc1 layer\n",
        "model5.add(Dense(625, activation='relu', kernel_initializer='he_normal'))\n",
        "model5.add(Dropout(prob_drop_hidden))\n",
        "\n",
        "# fc2 layer\n",
        "model5.add(Dense(10, activation='softmax', kernel_initializer='he_normal'))\n",
        "\n",
        "opt = RMSprop(lr=0.001, rho=0.9)\n",
        "model5.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model5.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKEwPSS_HTSA",
        "outputId": "2eae9aa2-3b4b-4ea1-b638-f768594df105"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "60000/60000 [==============================] - 801s 13ms/step - loss: 0.2755 - acc: 0.9142\n",
            "Epoch 2/50\n",
            "60000/60000 [==============================] - 2330s 39ms/step - loss: 0.0753 - acc: 0.9770\n",
            "Epoch 3/50\n",
            "60000/60000 [==============================] - 121s 2ms/step - loss: 0.0592 - acc: 0.9824\n",
            "Epoch 4/50\n",
            "60000/60000 [==============================] - 117s 2ms/step - loss: 0.0506 - acc: 0.9853\n",
            "Epoch 5/50\n",
            "60000/60000 [==============================] - 115s 2ms/step - loss: 0.0428 - acc: 0.9868\n",
            "Epoch 6/50\n",
            "60000/60000 [==============================] - 115s 2ms/step - loss: 0.0437 - acc: 0.9875\n",
            "Epoch 7/50\n",
            "60000/60000 [==============================] - 114s 2ms/step - loss: 0.0366 - acc: 0.9894\n",
            "Epoch 8/50\n",
            "60000/60000 [==============================] - 114s 2ms/step - loss: 0.0353 - acc: 0.9891\n",
            "Epoch 9/50\n",
            "60000/60000 [==============================] - 114s 2ms/step - loss: 0.0326 - acc: 0.9907\n",
            "Epoch 10/50\n",
            "60000/60000 [==============================] - 114s 2ms/step - loss: 0.0328 - acc: 0.9905\n",
            "Epoch 11/50\n",
            "60000/60000 [==============================] - ETA: 0s - loss: 0.0319 - acc: 0.990 - 114s 2ms/step - loss: 0.0319 - acc: 0.9906\n",
            "Epoch 12/50\n",
            "60000/60000 [==============================] - 114s 2ms/step - loss: 0.0312 - acc: 0.9907\n",
            "Epoch 13/50\n",
            "60000/60000 [==============================] - 115s 2ms/step - loss: 0.0301 - acc: 0.9913\n",
            "Epoch 14/50\n",
            "60000/60000 [==============================] - 113s 2ms/step - loss: 0.0306 - acc: 0.9914\n",
            "Epoch 15/50\n",
            "60000/60000 [==============================] - 115s 2ms/step - loss: 0.0306 - acc: 0.9916\n",
            "Epoch 16/50\n",
            "60000/60000 [==============================] - 130s 2ms/step - loss: 0.0281 - acc: 0.9922\n",
            "Epoch 17/50\n",
            "60000/60000 [==============================] - 297s 5ms/step - loss: 0.0281 - acc: 0.9926\n",
            "Epoch 18/50\n",
            "60000/60000 [==============================] - 1617s 27ms/step - loss: 0.0308 - acc: 0.9916\n",
            "Epoch 19/50\n",
            "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0294 - acc: 0.9923\n",
            "Epoch 20/50\n",
            "60000/60000 [==============================] - 81s 1ms/step - loss: 0.0313 - acc: 0.9914\n",
            "Epoch 21/50\n",
            "60000/60000 [==============================] - 79s 1ms/step - loss: 0.0296 - acc: 0.9923\n",
            "Epoch 22/50\n",
            "60000/60000 [==============================] - 460s 8ms/step - loss: 0.0324 - acc: 0.9919\n",
            "Epoch 23/50\n",
            "60000/60000 [==============================] - 79s 1ms/step - loss: 0.0310 - acc: 0.9922\n",
            "Epoch 24/50\n",
            "60000/60000 [==============================] - 78s 1ms/step - loss: 0.0343 - acc: 0.9915\n",
            "Epoch 25/50\n",
            "60000/60000 [==============================] - 1012s 17ms/step - loss: 0.0319 - acc: 0.9916\n",
            "Epoch 26/50\n",
            "60000/60000 [==============================] - 82s 1ms/step - loss: 0.0296 - acc: 0.9918\n",
            "Epoch 27/50\n",
            "60000/60000 [==============================] - 80s 1ms/step - loss: 0.0339 - acc: 0.9913\n",
            "Epoch 28/50\n",
            "60000/60000 [==============================] - 78s 1ms/step - loss: 0.0309 - acc: 0.9920\n",
            "Epoch 29/50\n",
            "60000/60000 [==============================] - 79s 1ms/step - loss: 0.0376 - acc: 0.9910\n",
            "Epoch 30/50\n",
            "60000/60000 [==============================] - 78s 1ms/step - loss: 0.0350 - acc: 0.9915\n",
            "Epoch 31/50\n",
            "60000/60000 [==============================] - 78s 1ms/step - loss: 0.0338 - acc: 0.9916\n",
            "Epoch 32/50\n",
            "60000/60000 [==============================] - 79s 1ms/step - loss: 0.0371 - acc: 0.9909\n",
            "Epoch 33/50\n",
            "60000/60000 [==============================] - 494s 8ms/step - loss: 0.0376 - acc: 0.9906\n",
            "Epoch 34/50\n",
            "60000/60000 [==============================] - 2565s 43ms/step - loss: 0.0354 - acc: 0.9911\n",
            "Epoch 35/50\n",
            "60000/60000 [==============================] - 91s 2ms/step - loss: 0.0382 - acc: 0.9911\n",
            "Epoch 36/50\n",
            "60000/60000 [==============================] - 87s 1ms/step - loss: 0.0395 - acc: 0.9907\n",
            "Epoch 37/50\n",
            "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0396 - acc: 0.9904\n",
            "Epoch 38/50\n",
            "60000/60000 [==============================] - 79s 1ms/step - loss: 0.0397 - acc: 0.9908\n",
            "Epoch 39/50\n",
            "60000/60000 [==============================] - 81s 1ms/step - loss: 0.0413 - acc: 0.9904\n",
            "Epoch 40/50\n",
            "60000/60000 [==============================] - 87s 1ms/step - loss: 0.0413 - acc: 0.9901\n",
            "Epoch 41/50\n",
            "60000/60000 [==============================] - 86s 1ms/step - loss: 0.0427 - acc: 0.9898\n",
            "Epoch 42/50\n",
            "60000/60000 [==============================] - 79s 1ms/step - loss: 0.0441 - acc: 0.9893\n",
            "Epoch 43/50\n",
            "60000/60000 [==============================] - 76s 1ms/step - loss: 0.0456 - acc: 0.9896\n",
            "Epoch 44/50\n",
            "60000/60000 [==============================] - 78s 1ms/step - loss: 0.0445 - acc: 0.9895\n",
            "Epoch 45/50\n",
            "60000/60000 [==============================] - 76s 1ms/step - loss: 0.0447 - acc: 0.9896\n",
            "Epoch 46/50\n",
            "60000/60000 [==============================] - 76s 1ms/step - loss: 0.0451 - acc: 0.9896\n",
            "Epoch 47/50\n",
            "60000/60000 [==============================] - 75s 1ms/step - loss: 0.0431 - acc: 0.9902\n",
            "Epoch 48/50\n",
            "60000/60000 [==============================] - 76s 1ms/step - loss: 0.0451 - acc: 0.9892\n",
            "Epoch 49/50\n",
            "60000/60000 [==============================] - 76s 1ms/step - loss: 0.0428 - acc: 0.9898\n",
            "Epoch 50/50\n",
            "60000/60000 [==============================] - 76s 1ms/step - loss: 0.0444 - acc: 0.9897\n"
          ]
        }
      ],
      "source": [
        "# Train\n",
        "case5 = model5.fit(X_train, Y_train, epochs=50, batch_size=batch_size, shuffle=True, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMmrSJpjHTSB",
        "outputId": "5f680c75-87ce-40ca-afb6-cb7c38b3cd10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 4s 389us/step\n",
            "Summary: Loss over the test dataset: 0.04, Accuracy: 0.9901\n"
          ]
        }
      ],
      "source": [
        "# Evaluate\n",
        "evaluation5 = model5.evaluate(X_test, Y_test, batch_size=256, verbose=1)\n",
        "print('Summary: Loss over the test dataset: %.2f, Accuracy: %.4f' %  (evaluation5[0], evaluation5[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVAKpVRvHTSB"
      },
      "source": [
        "## The accuracy of the model for case 4 is 99.01%"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This concludes the MNIST tutorial."
      ],
      "metadata": {
        "id": "odCsPBvz3n_3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "d7eCPU3vHTSB"
      },
      "source": [
        "References:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Dataset \n",
        "http://yann.lecun.com/exdb/mnist/\n",
        "\n",
        "2.  https://github.com/nikbearbrown/NEU_COE\n",
        "3.  MIT 6.S191: Introduction to Deep Learning https://youtu.be/JN6H4rQvwgY\n",
        "4.  https://youtu.be/v5JvvbP0d44\n",
        "5.  https://youtu.be/ixF5WNpTzCA\n",
        "6.  https://youtu.be/jajksuQW4mc\n",
        "7. https://yashk2810.github.ioApplying-Convolutional-Neural-Network-on-the-MNIST-dataset/\n",
        "8.  http://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html\n",
        "9.  https://keras.io/losses/\n",
        "10. Hyperparameter Optimization: This Tutorial Is All You Need Abhishek Thakur:Jul 19, 2020 https://www.youtube.com/watch?v=5nYqK-HaoKY&ab_channel=AbhishekThakur"
      ],
      "metadata": {
        "id": "Wc3RdG5cxyHj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Licences:\n",
        "\n",
        "1. impyute 3.7 by Elton Law https://impyute.readthedocs.io/en/master/user_guide/getting_started.html#versions\n",
        "2. Pandas 1.4 https://pandas.pydata.org/docs/getting_started/overview.html\n",
        "3. Scipy.stats https://docs.scipy.org/doc/scipy/reference/stats.html\n",
        "4. sklearn.simpleimputer https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n",
        "5. sklearn.LinearRegression, GridSearch https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"
      ],
      "metadata": {
        "id": "tBdSeRKkySmc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MIT License\n",
        "\n",
        "\n",
        "Copyright (c) 2022 Priyanka Kuklani\n",
        "\n",
        " \n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        " \n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        " \n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ],
      "metadata": {
        "id": "qMsFgKQ9xsou"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "I2ML_CNN_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}