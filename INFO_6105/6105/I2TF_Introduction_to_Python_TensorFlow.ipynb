{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"I2TF_Introduction_to_Python_TensorFlow.ipynb","provenance":[],"collapsed_sections":["Q4DYbVvnmvv8","tRoomsx3mvwI","XYpnt_5QmvwQ","qcxeDTIVmvwV","WsFsQxdLmvwa","rO4EYusbmvwe","9cOfXeW2mvwo","piY35K6xmvw9","gdyCCs4gmvxA"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"W1rb0HGImvvi"},"source":[" # Introduction to TensorFlow\n"," \n","  This lesson is adapted from\n"," \n"," \n","1. TensorFlow Tutorial -**from Kulbear** \n","https://github.com/Kulbear/deep-learning-coursera/blob/master/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Tensorflow%20Tutorial.ipynb\n","\n","2. Python TensorFlow Tutorial – Build a Neural Network **from Andy**\n","http://adventuresinmachinelearning.com/python-tensorflow-tutorial/\n","\n","\n","which are licensed under the [MIT license](https://opensource.org/licenses/MIT)   "]},{"cell_type":"markdown","metadata":{"id":"fiYyj0T9mvvj"},"source":["![TensorFlow](https://raw.githubusercontent.com/nikbearbrown/Google_Colab/master/img/TensorFlow.png)\n","\n","\n"," TensorFlow is an open source software library for numerical computation using data flow graphs. The graph nodes represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) that flow between them. This flexible architecture enables you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device without rewriting code. TensorFlow also includes TensorBoard, a data visualization toolkit.\n","\n","TensorFlow was originally developed by researchers and engineers working on the Google Brain team within Google's Machine Intelligence Research organization for the purposes of conducting machine learning and deep neural networks research. The system is general enough to be applicable in a wide variety of other domains, as well.\n","\n","\n","### TODO  What is a Tensor?\n","\n","For blog credit - TODO  What is a Tensor?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2zhVfVJImvvj"},"source":["* In simple terms, TensorFlow is based on graph based computation which means it’s an alternative way of conceptualising mathematical calculations.  Consider the following expression a=(b+c)∗(c+2).  We can break this function down into the following components: \n","\n","### d = b + c \n","\n","### e = c + 2\n","\n","### a = d * e\n","\n","which can be represented graphically as:\n","\n","![Computatonal Graph](https://raw.githubusercontent.com/nikbearbrown/Google_Colab/master/img/Computatonal_Graph.png)\n","\n","This may seem like a silly example – but notice a powerful idea in expressing the equation this way: two of the computations (d=b+c and e = c+2) can be performed in parallel.  By splitting up these calculations across CPUs or GPUs, this can give us significant gains in computational times.  These gains are a must for big data applications and deep learning – especially for complicated neural network architectures such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs).  The idea behind TensorFlow is to ability to create these computational graphs in code and allow significant performance improvements via parallel operations and other efficiency gains.\n"]},{"cell_type":"markdown","metadata":{"id":"vTgvkLp-mvvk"},"source":["# TensorFlow data flow graph"]},{"cell_type":"code","metadata":{"id":"sxrY7uWhmvvl","outputId":"3f299c4d-9b27-4c8b-e431-bf04a918301c"},"source":["![TensorFlow data flow graph gif](https://raw.githubusercontent.com/nikbearbrown/Google_Colab/master/img/TensorFlow-data-flow-graph.gif)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<img src=\"http://adventuresinmachinelearning.com/wp-content/uploads/2017/03/TensorFlow-data-flow-graph.gif\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"baNdsmG1mvvq"},"source":["#### The TensorFlow graph above, which shows the computational graph of a three-layer neural network.The animated data flows between different nodes in the graph are tensors which are multi-dimensional data arrays.  For instance, the input data tensor may be 5000 x 64 x 1, which represents a 64 node input layer with 5000 training samples.  After the input layer there is a hidden layer with rectified linear units as the activation function.  There is a final output layer (called a “logit layer” in the above graph) which uses cross entropy as a cost/loss function.  At each point we see the relevant tensors flowing to the “Gradients” block which finally flow to the Stochastic Gradient Descent optimiser which performs the back-propagation and gradient descent."]},{"cell_type":"markdown","metadata":{"id":"_GW9HwT-mvvq"},"source":["## Really Awesome TensorFlow Tutorials\n","\n","\n","TensorFlow 101 (Really Awesome Intro Into TensorFlow) [https://youtu.be/arl8O22aa6I](https://youtu.be/arl8O22aa6I)    \n","\n","\n","Getting Started with TensorFlow and Deep Learning | SciPy 2018 Tutorial ... [https://youtu.be/tYYVSEHq-io](https://youtu.be/tYYVSEHq-io)   \n","\n","\n","Hands-on TensorBoard (TensorFlow Dev Summit 2017) [https://youtu.be/eBbEDRsCmv4](https://youtu.be/eBbEDRsCmv4) "]},{"cell_type":"markdown","metadata":{"id":"aKAsIQzfmvvr"},"source":["# Installation"]},{"cell_type":"markdown","metadata":{"id":"M0iLf5yWmvvr"},"source":["See https://www.tensorflow.org/install/ for instructions on how to install our release binaries or how to build from source.\n","\n","People who are a little more adventurous can also try nightly binaries:\n","\n","#### Nightly pip packages\n","\n","* TensorFlow now offers nightly pip packages under the tf-nightly and tf-nightly-gpu project on pypi. Simply run **pip install tf-nightly** or **pip install tf-nightly-gpu** in a clean environment to install the nightly TensorFlow build. We support CPU and GPU packages on Linux, Mac, and Windows.\n","\n","\n","#### Individual whl files\n","\n","1. Linux CPU-only: Python 2 (build history) / Python 3.4 (build history) / Python 3.5 (build history) / Python 3.6 (build history)\n","2. Linux GPU: Python 2 (build history) / Python 3.4 (build history) / Python 3.5 (build history) / Python 3.6 (build history)\n","3. Mac CPU-only: Python 2 (build history) / Python 3 (build history)\n","4. Windows CPU-only: Python 3.5 64-bit (build history) / Python 3.6 64-bit (build history)\n","5. Windows GPU: Python 3.5 64-bit (build history) / Python 3.6 64-bit (build history)\n","6. Android: demo APK, native libs (build history)"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"pQ9G0EJmmvvs","outputId":"d05c57d5-2e1d-4bdb-d704-d7bbc47f4290"},"source":["!pip install tensorflow"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow in /Users/bear/anaconda/lib/python3.6/site-packages\r\n","Requirement already satisfied: protobuf>=3.2.0 in /Users/bear/anaconda/lib/python3.6/site-packages (from tensorflow)\r\n","Requirement already satisfied: werkzeug>=0.11.10 in /Users/bear/anaconda/lib/python3.6/site-packages (from tensorflow)\r\n","Requirement already satisfied: numpy>=1.11.0 in /Users/bear/anaconda/lib/python3.6/site-packages (from tensorflow)\r\n","Requirement already satisfied: six>=1.10.0 in /Users/bear/anaconda/lib/python3.6/site-packages (from tensorflow)\r\n","Requirement already satisfied: wheel>=0.26 in /Users/bear/anaconda/lib/python3.6/site-packages (from tensorflow)\r\n","Requirement already satisfied: setuptools in /Users/bear/anaconda/lib/python3.6/site-packages (from protobuf>=3.2.0->tensorflow)\r\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jbqySKKEmvvv"},"source":["### 1. A Simple TensorFlow example\n","\n","Let make TensorFlow perform a simple calculation – a =(b+c)∗(c+2). But before that lets introduce ourselves to TensorFlow variables and constants."]},{"cell_type":"code","metadata":{"id":"7fO19u9Kmvvv"},"source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","import tensorflow as tf #importing library\n","import numpy as np\n","\n","# first, create a TensorFlow constant\n","const = tf.constant(2.0, name=\"const\") \n","    \n","# create TensorFlow variables\n","b = tf.Variable(2.0, name='b')\n","c = tf.Variable(1.0, name='c')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bEyl_uNkmvvx"},"source":["TensorFlow constants can be declared using tf.constant function, and variables with the tf.Variable function.  The first element in both is the value to be assigned the constant / variable when it is initialised.  The second is an optional name string which can be used to label the constant / variable – this is handy for when we want to do visualisations (will be discussed  later).  TensorFlow will infer the type of the constant / variable from the initialised value, but it can also be set explicitly using the optional dtype argument.  TensorFlow has many of its own types like tf.float32, tf.int32 etc. – see https://www.tensorflow.org/api_docs/python/tf/DType.\n"]},{"cell_type":"markdown","metadata":{"id":"DTOwcnj_mvvy"},"source":["As the Python code runs through these commands, the variables haven’t actually been declared as they would have been if you just had a standard Python declaration (i.e. b = 2.0).  Instead, all the constants, variables, operations and the computational graph are only created when the initialisation commands are run.\n","\n","* Creating TensorFlow operations:"]},{"cell_type":"code","metadata":{"id":"2hhNSFEamvvy"},"source":["# creating some operations\n","d = tf.add(b, c, name='d')\n","e = tf.add(c, const, name='e')\n","a = tf.multiply(d, e, name='a')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KwRKBU5Jmvv1"},"source":["THe operations above are obvious and they instantiate the operations  b + c,  c + 2.0 and  d∗e.\n","\n","=>> setup an object to initialise the variables and the graph structure:"]},{"cell_type":"code","metadata":{"id":"m2uEi4Q_mvv1"},"source":["# setup the variable initialisation\n","init_op = tf.global_variables_initializer()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8kZ6j44Fmvv4"},"source":["To run the operations between the variables, we need to start a TensorFlow session – tf.Session.  The TensorFlow session is an object where all operations are run.  Using the with Python syntax, we can run the graph with the following code:"]},{"cell_type":"code","metadata":{"id":"ZY8xg8prmvv5","outputId":"1ecc6b7a-a51a-465d-a471-bfaf2edc6886"},"source":["# start of the session\n","with tf.Session() as sess:\n","    # initialise the variables\n","    sess.run(init_op)\n","    # compute the output of the graph\n","    a_out = sess.run(a)\n","    print(\"Variable a is {}\".format(a_out))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Variable a is 9.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Q4DYbVvnmvv8"},"source":["The first command within the with block is the initialisation, which is run with the, well, run command.  Next we want to figure out what the variable a should be.  All we have to do is run the operation which calculates a i.e. a = tf.multiply(d, e, name=’a’).  Note that a is an operation, not a variable and therefore it can be run.  We do just that with the sess.run(a) command and assign the output to a_out, the value of which we then print out.\n","\n","**Writing and running programs in TensorFlow has the following steps:**\n","\n","1. Create Tensors (variables) that are not yet executed/evaluated.\n","2. Write operations between those Tensors.\n","3. Initialize your Tensors.\n","4. Create a Session.\n","5. Run the Session. This will run the operations you'd written above.\n","\n","### Let's take an another easy example. Run the cell below:\n"]},{"cell_type":"code","metadata":{"id":"wCi2kgY2mvv8","outputId":"b833f860-5e64-432b-8a6a-14305e73dcd4"},"source":["a = tf.constant(500)\n","b = tf.constant(2)\n","c = tf.multiply(a,b)\n","print(c)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tensor(\"Mul:0\", shape=(), dtype=int32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5Us1DoQamvv_"},"source":["The expected output was 1000 but we got a tensor saying that the result is a tensor that does not have the shape attribute, and is of type \"int32\". All we did was put in the 'computation graph', but didn't run this computation yet. In order to actually multiply the two numbers, we will have to create a session and run it."]},{"cell_type":"code","metadata":{"id":"s1gxndC3mvv_","outputId":"bb7d111d-cf97-4a41-f5b7-7c5b8c52e569"},"source":["sess = tf.Session()\n","print(sess.run(c))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tZNIab2VmvwB"},"source":["**Lets take another example where we compute the loss of one training example.**"]},{"cell_type":"code","metadata":{"id":"_G6krgB-mvwC","outputId":"44668284-1031-48a6-fad7-eff2c0f5b3d0"},"source":["y_hat = tf.constant(36, name='y_hat')            # Define y_hat constant. Set to 36.\n","y = tf.constant(39, name='y')                    # Define y. Set to 39\n","\n","loss = tf.Variable((y - y_hat)**2, name='loss')  # Create a variable for the loss\n","\n","init = tf.global_variables_initializer()         # When init is run later (session.run(init)),\n","                                                 # the loss variable will be initialized and ready to be computed\n","with tf.Session() as session:                    # Create a session and print the output\n","    session.run(init)                            # Initializes the variables\n","    print(session.run(loss))                     # Prints the loss"],"execution_count":null,"outputs":[{"output_type":"stream","text":["9\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ouu6Wo_FmvwE"},"source":["Therefore, when we created a variable for the loss, we simply defined the loss as a function of other quantities, but did not evaluate its value. To evaluate it, we had to run **init = tf.global_variables_initializer()**. That initialized the loss variable, and in the last line we were finally able to evaluate the value of loss and print its value."]},{"cell_type":"markdown","metadata":{"id":"qN9WJnmPmvwE"},"source":["# The TensorFlow placeholder\n","\n","**A placeholder is an object whose value we can specify only later. To specify values for a placeholder, we can pass in values by using a \"feed dictionary\" (feed_dict variable). Below, we created a placeholder for x. This allows us to pass in a number later when we run the session.**\n","\n"]},{"cell_type":"code","metadata":{"id":"PJfy6BqimvwF","outputId":"d6c2e558-b6a5-4be4-87f0-e5119f73d6bc"},"source":["# Changing the value of x in the feed_dict\n","\n","X = tf.placeholder(tf.int64, name = 'X')\n","print(sess.run(2 * X, feed_dict = {X: 50}))\n","sess.close()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hR0Ra5zimvwH"},"source":["while defining X we didnt specify value for it. A placeholder is simply a variable that we will assign data to only later, when running the session. "]},{"cell_type":"markdown","metadata":{"id":"tRoomsx3mvwI"},"source":["### 1.1 Linear function\n","\n","\n","Lets computing the following equation: $Y = WX + b$, where $W$ and $X$ are random matrices and b is a random vector.\n","\n","**Exercise** : Compute $WX + b$ where $W, X$, and $b$ are drawn from a random normal distribution. W is of shape (4, 3), X is (3,1) and b is (4,1). As an example, here is how you would define a constant X that has shape (3,1):\n","\n","X = tf.constant(np.random.randn(3,1), name = \"X\")\n","\n","Useful Functions below:\n","\n","    tf.matmul(..., ...) to do a matrix multiplication\n","    tf.add(..., ...) to do an addition\n","    np.random.randn(...) to initialize randomly"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"bOBlkpOMmvwI"},"source":["def linear_function():\n","    \"\"\"\n","    Implements a linear function: \n","            Initializes W to be a random tensor of shape (4,3)\n","            Initializes X to be a random tensor of shape (3,1)\n","            Initializes b to be a random tensor of shape (4,1)\n","    Returns: \n","    result -- runs the session for Y = WX + b \n","    \"\"\"\n","    \n","    np.random.seed(1)  \n","    \n","    X = np.random.randn(3, 1)\n","    W = np.random.randn(4, 3)\n","    b = np.random.randn(4, 1)\n","    Y = tf.add(tf.matmul(W, X), b)\n","   \n","    \n","    # Creating the session using tf.Session() and running it with sess.run(...) on the variable we want to calculate \n","    \n","    sess = tf.Session()\n","    result = sess.run(Y)\n","   \n","    # closing the session \n","    sess.close()\n","\n","    return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C3HTy8RCmvwN","outputId":"fc300104-be3e-4588-9918-c4044af3f864"},"source":["print( \"result = \" + str(linear_function()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["result = [[-2.15657382]\n"," [ 2.95891446]\n"," [-1.08926781]\n"," [-0.84538042]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"t6wf3D49mvwP"},"source":["Expected Output :\n","\n","**result**\t[[-2.15657382] [ 2.95891446] [-1.08926781] [-0.84538042]]"]},{"cell_type":"markdown","metadata":{"id":"XYpnt_5QmvwQ"},"source":["### **1.2 - Computing the sigmoid**\n","\n","We have implemented a linear function. Tensorflow offers a variety of commonly used neural network functions like tf.sigmoid and tf.softmax. Lets compute the sigmoid function of an input.\n","\n","We will be doing another exercise using a placeholder variable x. When running the session, we will use the feed dictionary to pass in the input z. In this exercise, we will have to\n","    (i) create a placeholder x,\n","    (ii) define the operations needed to compute the sigmoid using tf.sigmoid, and then\n","    (iii) run the session.\n","\n","**Exercise** : Implementing the sigmoid function below using the following:\n","\n","       > tf.placeholder(tf.float32, name = \"...\")\n","       > tf.sigmoid(...)\n","       > sess.run(..., feed_dict = {x: z})\n","There are two typical ways to create and use sessions in tensorflow:\n","\n","**Method 1:**\n","![image.png](attachment:image.png)\n"]},{"cell_type":"markdown","metadata":{"id":"tnnpDTaDmvwQ"},"source":["**Method 2:**\n","![image.png](attachment:image.png)\n"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"GQ7kiEM_mvwQ"},"source":["def sigmoid(z):\n","    \"\"\"\n","    Computes the sigmoid of z\n","    \n","    Arguments:\n","    z -- input value, scalar or vector\n","    \n","    Returns: \n","    results -- the sigmoid of z\n","    \"\"\"\n","    \n","    # Creating a placeholder for x. Naming it 'x'.\n","    x = tf.placeholder(tf.float32, name=\"x\")\n","\n","    # computing sigmoid(x)\n","    sigmoid = tf.sigmoid(x)\n","    \n","    # Creating a session, and running it. Using the method 2 explained above. \n","    # we should use a feed_dict to pass z's value to x. \n","    with tf.Session() as sess: \n","        result = result = sess.run(sigmoid, feed_dict = {x: z})\n","    \n","    return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ROm9_UuymvwT","outputId":"31d52922-28bd-42fb-b9cb-a1488feaba06"},"source":["print (\"sigmoid(0) = \" + str(sigmoid(0)))\n","print (\"sigmoid(12) = \" + str(sigmoid(12)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["sigmoid(0) = 0.5\n","sigmoid(12) = 0.999994\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qcxeDTIVmvwV"},"source":["Expected Output :\n","\n","**sigmoid(0)**\t0.5\n","**sigmoid(12)**\t0.999994\n","\n","    1. Create placeholders\n","    2. Specify the computation graph corresponding to operations you want to compute\n","    3. Create the session\n","    4. Run the session, using a feed dictionary if necessary to specify placeholder variables' values.\n","\n","### 1.3 - Computing the Cost\n","Lets use a built-in function to compute the cost of your neural network. So instead of needing to write code to compute this as a function of $a^{[2](i)}$ and $y^{(i)}$ for i=1...m: $$ J = - \\frac{1}{m}  \\sum_{i = 1}^m  \\large ( \\small y^{(i)} \\log a^{ [2] (i)} + (1-y^{(i)})\\log (1-a^{ [2] (i)} )\\large )\\small\\tag{2}$$\n","\n","We can do it in one line of code in tensorflow!\n","\n","Exercise: Implement the cross entropy loss. The function you will use is:\n","\n","tf.nn.sigmoid_cross_entropy_with_logits(logits = ...,  labels = ...)\n","Your code should input z, compute the sigmoid (to get a) and then compute the cross entropy cost $J$. All this can be done using one call to tf.nn.sigmoid_cross_entropy_with_logits, which computes\n","\n","$$- \\frac{1}{m}  \\sum_{i = 1}^m  \\large ( \\small y^{(i)} \\log \\sigma(z^{[2](i)}) + (1-y^{(i)})\\log (1-\\sigma(z^{[2](i)})\\large )\\small\\tag{2}$$"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"pNoUnQm7mvwV"},"source":["def cost(logits, labels):\n","    \"\"\"\n","    Computes the cost using the sigmoid cross entropy\n","    \n","    Arguments:\n","    logits -- vector containing z, output of the last linear unit (before the final sigmoid activation)\n","    labels -- vector of labels y (1 or 0) \n","    \n","    Note: What we've been calling \"z\" and \"y\" in this class are respectively called \"logits\" and \"labels\" \n","    in the TensorFlow documentation. So logits will feed into z, and labels into y. \n","    \n","    Returns:\n","    cost -- runs the session of the cost (formula (2))\n","    \"\"\"\n","    \n","    # Creating the placeholders for \"logits\" (z) and \"labels\" (y) \n","    z = tf.placeholder(tf.float32, name=\"z\")\n","    y = tf.placeholder(tf.float32, name=\"y\")\n","    \n","    # Using the loss function\n","    cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=z, labels=y)\n","    \n","    # Create a session. refer method 1 above.\n","    sess = tf.Session()\n","    cost = sess.run(cost, feed_dict={z: logits, y: labels})\n","    \n","    # Closing the session\n","    sess.close()\n","    \n","    return cost"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rmu45vcFmvwX","outputId":"b161cced-2d51-4549-fff0-5da404016cdb"},"source":["logits = sigmoid(np.array([0.2, 0.4, 0.7, 0.9]))\n","cost = cost(logits, np.array([0, 0, 1, 1]))\n","print (\"cost = \" + str(cost))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cost = [ 1.00538719  1.03664088  0.41385433  0.39956614]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WsFsQxdLmvwa"},"source":["### 1.4 - Using One Hot encodings\n","Many times in deep learning we will have a y vector with numbers ranging from **0** to **C-1**, where **C** is the number of classes. If **C** is for example 4, then you might have the following y vector which we will need to convert as follows:\n","\n","![image.png](attachment:image.png)\n","\n","This is called a \"one hot\" encoding, because in the converted representation exactly one element of each column is \"hot\" (meaning set to 1). To do this conversion in numpy, you might have to write a few lines of code. In tensorflow, you can use one line of code:\n","\n","tf.one_hot(labels, depth, axis)\n","Exercise: Implement the function below to take one vector of labels and the total number of classes $C$, and return the one hot encoding. Use tf.one_hot() to do this."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"inEdvzK6mvwa"},"source":["def one_hot_matrix(labels, C):\n","    \"\"\"\n","    Creates a matrix where the i-th row corresponds to the ith class number and the jth column\n","                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) \n","                     will be 1. \n","                     \n","    Arguments:\n","    labels -- vector containing the labels \n","    C -- number of classes, the depth of the one hot dimension\n","    \n","    Returns: \n","    one_hot -- one hot matrix\n","    \"\"\"\n","       \n","    # Creating a tf.constant equal to C (depth), naming it 'C'.\n","    C = tf.constant(C, name='C')\n","    \n","    # Using tf.one_hot, be careful with the axis\n","    one_hot_matrix = tf.one_hot(indices=labels, depth=C, axis=0)\n","    \n","    # Creating the session\n","    sess = tf.Session()\n","\n","    one_hot = sess.run(one_hot_matrix)\n","    \n","    # Closing the session\n","    sess.close()\n","    \n","    return one_hot"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2ZCvArbGmvwc","outputId":"9917fcab-d3df-4390-814b-8fb6914d268b"},"source":["labels = np.array([1,2,3,0,2,1])\n","one_hot = one_hot_matrix(labels, C=4)\n","print (\"one_hot = \" + str(one_hot))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["one_hot = [[ 0.  0.  0.  1.  0.  0.]\n"," [ 1.  0.  0.  0.  0.  1.]\n"," [ 0.  1.  0.  0.  1.  0.]\n"," [ 0.  0.  1.  0.  0.  0.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rO4EYusbmvwe"},"source":["### 1.5 - Initialize with zeros and ones\n","Initializing a vector of zeros and ones. The function we will be calling is tf.ones(). To initialize with zeros we will use tf.zeros() instead. These functions take in a shape and return an array of dimension shape full of zeros and ones respectively.\n","\n","**Exercise:** Implement the function below to take in a shape and to return an array (of the shape's dimension of ones).\n","\n","tf.ones(shape)"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"ff_qrrRImvwe"},"source":["def ones(shape):\n","    \"\"\"\n","    Creates an array of ones of dimension shape\n","    \n","    Arguments:\n","    shape -- shape of the array you want to create\n","        \n","    Returns: \n","    ones -- array containing only ones\n","    \"\"\"\n","\n","    # Create \"ones\" tensor using tf.ones(...)\n","    ones = tf.ones(shape)\n","    \n","    # Creating & Running the session\n","    sess = tf.Session()\n","    ones = sess.run(ones)\n","    \n","    # Close the session\n","    sess.close()\n","    \n","    return ones"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"15rLEUYtmvwi","outputId":"2eedb31b-e7b1-4747-99ba-54edc036a387"},"source":["print (\"ones = \" + str(ones([3])))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["ones = [ 1.  1.  1.]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"d5yhiMOkmvwk"},"source":["# 2. Building Neural Network in TensorFlow\n","\n","we’ll be using the MNIST dataset (and its associated loader) that the TensorFlow package provides.  This MNIST dataset is a set of 28×28 pixel grayscale images which represent hand-written digits.  It has 55,000 training rows, 10,000 testing rows and 5,000 validation rows.\n","\n","We can load the data by running:"]},{"cell_type":"code","metadata":{"id":"38aARBuEmvwk","outputId":"c5e80791-6537-4334-dc16-fad5ba8bb5cf"},"source":["from tensorflow.examples.tutorials.mnist import input_data\n","mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Extracting MNIST_data/train-images-idx3-ubyte.gz\n","Extracting MNIST_data/train-labels-idx1-ubyte.gz\n","Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n","Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Nk2CG3Yimvwm","outputId":"4e9deebd-54be-45d2-dcb1-fdd5c9cfbc50"},"source":["# Displaying a random image from dataset\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","batch = mnist.train.next_batch(1)\n","plotData = batch[0]\n","plotData = plotData.reshape(28, 28)\n","plt.gray()\n","plt.imshow(plotData)\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n","text/plain":["<matplotlib.figure.Figure at 0x119858908>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"9cOfXeW2mvwo"},"source":["### 2.1 Setting things up\n","Next, we can set-up the placeholder variables for the training data (and some training parameters):"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"eqG2OLxDmvwp"},"source":["# Python optimisation variables\n","learning_rate = 0.5\n","epochs = 10\n","batch_size = 100\n","\n","# declaring the training data placeholders\n","# input x - for 28 x 28 pixels = 784\n","x = tf.placeholder(tf.float32, [None, 784])\n","# declaring the output data placeholder - 10 digits\n","y = tf.placeholder(tf.float32, [None, 10])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JHhvn4Edmvwr"},"source":["The **x** input layer is **784** nodes corresponding to the **28 x 28 (=784) pixels**, and the **y** output layer is **10 nodes** corresponding to the 10 possible digits.  Again, the size of x is (? x 784), where the ***?*** stands for an as yet unspecified number of samples to be input – this is the function of the placeholder variable.\n","\n","Let's setup the weight and bias variables for the three layer neural network.  There are always L-1 number of weights/bias tensors, where L is the number of layers.  So in this case, we need to setup two tensors for each:"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"wEfcuDibmvwr"},"source":["# declaring the weights connecting the input to the hidden layer\n","W1 = tf.Variable(tf.random_normal([784, 300], stddev=0.03), name='W1')\n","b1 = tf.Variable(tf.random_normal([300]), name='b1')\n","# and the weights connecting the hidden layer to the output layer\n","W2 = tf.Variable(tf.random_normal([300, 10], stddev=0.03), name='W2')\n","b2 = tf.Variable(tf.random_normal([10]), name='b2')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9n7e8nrxmvwt"},"source":["First, we declare some variables for W1 and b1, the weights and bias for the connections between the input and hidden layer.  This neural network will have 300 nodes in the hidden layer, so the size of the weight tensor W1 is [784, 300].  We initialise the values of the weights using a random normal distribution with a mean of zero and a standard deviation of 0.03.  TensorFlow has a replicated version of the numpy random normal function, which allows us to create a matrix of a given size populated with random samples drawn from a given distribution.  Likewise, we create W2 and b2 variables to connect the hidden layer to the output layer of the neural network.\n","\n","Next, we have to setup node inputs and activation functions of the hidden layer nodes:"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"m4uZKiIWmvwu"},"source":["# calculating the output of the hidden layer\n","hidden_out = tf.add(tf.matmul(x, W1), b1)\n","hidden_out = tf.nn.relu(hidden_out)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"By7xhbecmvwx"},"source":["In the first line, we execute the standard matrix multiplication of the weights (W1) by the input vector x and we add the bias b1.  The matrix multiplication is executed using the tf.matmul operation.  Next, we finalise the hidden_out operation by applying a rectified linear unit activation function to the matrix multiplication plus bias.  Note that TensorFlow has a rectified linear unit activation already setup for us, tf.nn.relu.\n","\n","This is to execute the following equations, as detailed in the neural networks tutorial:\n","![image.png](attachment:image.png)\n","Now, let’s setup the output layer, y_: "]},{"cell_type":"code","metadata":{"collapsed":true,"id":"YPcluMYQmvwx"},"source":["# calculating the hidden layer output using softmax activated\n","# output layer\n","y_ = tf.nn.softmax(tf.add(tf.matmul(hidden_out, W2), b2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6QDTfEaomvw1"},"source":["Again we perform the weight multiplication with the output from the hidden layer (hidden_out) and add the bias, b2.  In this case, we are going to use a softmax activation for the output layer – we can use the included TensorFlow softmax function tf.nn.softmax.\n","\n","We also have to include a cost or loss function for the optimisation / backpropagation to work on. Here we’ll use the cross entropy cost function, represented by:\n","\n","![image.png](attachment:image.png)\n","\n","Where y(i)j is the ith training label for output node j, yj_(i) is the ith predicted label for output node j, m is the number of training / batch samples and n is the number .  There are two operations occurring in the above equation.  The first is the summation of the logarithmic products and additions across all the output nodes.  The second is taking a mean of this summation across all the training samples.  We can implement this cross entropy cost function in TensorFlow with the following code:\n"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"26yQ0Czcmvw1"},"source":["y_clipped = tf.clip_by_value(y_, 1e-10, 0.9999999)\n","cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped)\n","                        + (1 - y) * tf.log(1 - y_clipped), axis=1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TlJDKy6Nmvw3"},"source":["Explanation: The first line is an operation converting the output y_ to a clipped version, limited between 1e-10 to 0.999999.  This is to make sure that we never get a case were we have a log(0) operation occurring during training – this would return NaN and break the training process.  The second line is the cross entropy calculation.\n","\n","To perform this calculation, first we use TensorFlow’s tf.reduce_sum function – this function basically takes the sum of a given axis of the tensor you supply.  In this case, the tensor that is supplied is the element-wise cross-entropy calculation for a single node and training sample i.e.:\n","y(i)jlog(yj_(i))+(1–y(i)j)log(1–yj_(i)).  \n","\n","Remember that y and y_clipped in the above calculation are (m x 10) tensors – therefore we need to perform the first sum over the second axis.  This is specified using the axis=1 argument, where “1” actually refers to the second axis when we have a zero-based indices system like Python.\n","\n","After this operation, we have an (m x 1) tensor.  To take the mean of this tensor and complete our cross entropy cost calculation (i.e. execute this part 1m∑mi=1), we use TensorFlow’s tf.reduce_mean function.  This function simply takes the mean of whatever tensor you provide it.  So now we have a cost function that we can use in the training process.\n","\n","Let’s setup the optimiser in TensorFlow:"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"iqOfokxamvw4"},"source":["# adding an optimiser\n","optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_QMbhBIPmvw6"},"source":["Here we are just using the gradient descent optimiser provided by TensorFlow.  We initialize it with a learning rate, then specify what we want it to do – i.e. minimise the cross entropy cost operation we created.  This function will then perform the gradient descent (for more details on gradient descent see http://adventuresinmachinelearning.com/neural-networks-tutorial/ and http://adventuresinmachinelearning.com/stochastic-gradient-descent/) and the backpropagation for us.  How easy is that?  TensorFlow has a library of popular neural network training optimisers, see https://www.tensorflow.org/api_guides/python/train.\n","\n","Finally, let’s setup the variable initialisation operation and an operation to measure the accuracy of our predictions:"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"Cl4r9JsTmvw7"},"source":["# setup the initialisation operator\n","init_op = tf.global_variables_initializer()\n","\n","# defining an accuracy assessment operation\n","correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n","accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"piY35K6xmvw9"},"source":["The correct prediction operation correct_prediction makes use of the TensorFlow tf.equal function which returns True or False depending on whether to arguments supplied to it are equal.  The **tf.argmax** function is the same as the numpy argmax function(https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html), which returns the index of the maximum value in a vector / tensor.  Therefore, the correct_prediction operation returns a tensor of size **(m x 1)** of True and False values designating whether the neural network has correctly predicted the digit.  We will calculate the mean accuracy from this tensor – first we have to cast the type of the correct_prediction operation from a Boolean to a TensorFlow float in order to perform the reduce_mean operation.  Once we’ve done that, we now have an accuracy operation ready to assess the performance of our neural network.\n","\n","### 3.2 Setting up the training\n","We now have everything we need to setup the training process of our neural network. \n","\n","Stepping through the lines below, the first couple relate to setting up the with statement and running the initialisation operation.  The third line relates to our mini-batch training scheme that we are going to run for this neural network.  If you want to know about mini-batch gradient descent, check out this post.  In the third line, we are calculating the number of batches to run through in each training epoch.  After that, we loop through each training epoch and initialise an avg_cost variable to keep track of the average cross entropy cost for each epoch.  The next line is where we extract a randomised batch of samples, batch_x and batch_y, from the MNIST training dataset.  The TensorFlow provided MNIST dataset has a handy **utility function**, **next_batch** that makes it easy to extract batches of data for training.\n","\n","The following line is where we run two operations.The sess.run is capable of taking a list of operations to run as its first argument.  In this case, supplying [optimiser, cross_entropy] as the list means that both these operations will be performed.  As such, we get two outputs, which we have assigned to the variables _ and c.  We don’t really care too much about the output from the optimiser operation but we want to know the output from the cross_entropy operation – which we have assigned to the variable c.  Note, we run the optimiser (and cross_entropy) operation on the batch samples.  In the following line, we use c to calculate the average cost for the epoch.\n","\n","Finally, we print out our progress in the average cost, and after the training is complete, we run the accuracy operation to print out the accuracy of our trained network on the test set.  Running this program produces the following output:"]},{"cell_type":"code","metadata":{"id":"vUk6ud-Emvw9","outputId":"7c0f09a7-2067-4845-9e1a-98bf266d8a84"},"source":["# start the session\n","with tf.Session() as sess:\n","   # initialise the variables\n","   sess.run(init_op)\n","   total_batch = int(len(mnist.train.labels) / batch_size)\n","   for epoch in range(epochs):\n","        avg_cost = 0\n","        for i in range(total_batch):\n","            batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)\n","            _ , c = sess.run([optimiser, cross_entropy], \n","                         feed_dict={x: batch_x, y: batch_y})\n","            avg_cost += c / total_batch\n","        test_acc = sess.run(accuracy,feed_dict={x: mnist.test.images, y: mnist.test.labels})\n","        print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(avg_cost), \"test accuracy: {:.3f}\".format(test_acc))\n","   print(\"\\nTraining complete!\")     \n","   print(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch: 1 cost = 0.571 test accuracy: 0.940\n","Epoch: 2 cost = 0.213 test accuracy: 0.970\n","Epoch: 3 cost = 0.152 test accuracy: 0.972\n","Epoch: 4 cost = 0.119 test accuracy: 0.975\n","Epoch: 5 cost = 0.098 test accuracy: 0.977\n","Epoch: 6 cost = 0.080 test accuracy: 0.978\n","Epoch: 7 cost = 0.063 test accuracy: 0.977\n","Epoch: 8 cost = 0.053 test accuracy: 0.978\n","Epoch: 9 cost = 0.045 test accuracy: 0.980\n","Epoch: 10 cost = 0.036 test accuracy: 0.980\n","\n","Training complete!\n","0.9801\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lC4W8DGNmvw_"},"source":["We got approximately 98% accuracy on the test set, not bad.  We could do a number of things to improve the model, such as regularisation, but we are just interested in exploring TensorFlow\n"," \n"," **Summary:**\n","\n","* Tensorflow is a programming framework used in deep learning\n","* The two main object classes in tensorflow are Tensors and Operators.\n","* When you code in tensorflow you have to take the following steps:\n","    * Create a graph containing Tensors (Variables, Placeholders ...) and Operations (tf.matmul, tf.add, ...)\n","    * Create a session\n","    * Initialize the session\n","    * Run the session to execute the graph\n","* You can execute the graph multiple times as you've seen in model()\n","* The backpropagation and optimization is automatically done when running the session on the \"optimizer\" object."]},{"cell_type":"markdown","metadata":{"id":"gdyCCs4gmvxA"},"source":["### Reference\n","\n","1. TensorFlow Tutorial -**from Kulbear** \n","https://github.com/Kulbear/deep-learning-coursera/blob/master/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Tensorflow%20Tutorial.ipynb\n","\n","2. Python TensorFlow Tutorial – Build a Neural Network **from Andy**\n","http://adventuresinmachinelearning.com/python-tensorflow-tutorial/\n","\n","\n","\n"]}]}