{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"colab":{"name":"NBB_Naïve_Bayes.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"ef-v1XxdO-1k"},"source":["## Naive Bayes\n","\n","[Naive Bayes classifiers](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features.\n","\n","\n","Abstractly, naive Bayes is a conditional probability model given a problem instance to be classified, represented by a vector $\\mathbf{x} = (x_1, \\dots, x_n)$ representing some $n$ features (independent variables), it assigns to this instance probabilities\n","\n","$$p(C_k \\mid x_1, \\dots, x_n)\\,$$\n","\n","for each of $k$ possible outcomes or classes $C_k$.\n","\n","The problem with the above formulation is that if the number of features $n$ is large or if a feature can take on a large number of values, then basing such a model on probability tables is infeasible.  We therefore reformulate the model to make it more tractable.  Using Bayes' theorem, the conditional probability can be decomposed as\n","\n","$$p(C_k \\mid \\mathbf{x}) = \\frac{p(C_k) \\ p(\\mathbf{x} \\mid C_k)}{p(\\mathbf{x})} \\,$$\n","\n","In plain English, using Bayesian probability terminology, the above equation can be written as\n","\n","$$\\mbox{posterior} = \\frac{\\mbox{prior} \\times \\mbox{likelihood}}{\\mbox{evidence}} \\,$$\n","\n","In practice, there is interest only in the numerator of that fraction, because the denominator does not depend on $C$ and the values of the features $F_i$ are given, so that the denominator is effectively constant.\n","The numerator is equivalent to the joint probability model\n","\n","$$p(C_k, x_1, \\dots, x_n)\\,$$\n","\n","which can be rewritten as follows, using the Chain rule for repeated applications of the definition of conditional probability\n","\n","$$\n","\\begin{align}\n","p(C_k, x_1, \\dots, x_n) & = p(x_1, \\dots, x_n, C_k) \\\\\n","                        & = p(x_1 \\mid x_2, \\dots, x_n, C_k) p(x_2, \\dots, x_n, C_k) \\\\\n","                        & = p(x_1 \\mid x_2, \\dots, x_n, C_k) p(x_2 \\mid x_3, \\dots, x_n, C_k) p(x_3, \\dots, x_n, C_k) \\\\\n","                        & = \\dots \\\\\n","                        & = p(x_1 \\mid x_2, \\dots, x_n, C_k) p(x_2 \\mid x_3, \\dots, x_n, C_k) \\dots   p(x_{n-1} \\mid x_n, C_k) p(x_n \\mid C_k) p(C_k)  \\\\\n","\\end{align}\n","$$\n","\n","Now the \"naive\" conditional independence assumptions come into play assume that each feature $F_i$ is conditionally statistical independence|independent of every other feature $F_j$ for $j\\neq i$, given the category $C$.  This means that\n","\n","$$p(x_i \\mid x_{i+1}, \\dots ,x_{n}, C_k ) = p(x_i \\mid C_k)\\,$$.\n","\n","Thus, the joint model can be expressed as\n","\n","$$\n","\\begin{align}\n","p(C_k \\mid x_1, \\dots, x_n) & \\varpropto p(C_k, x_1, \\dots, x_n) \\\\\n","                            & \\varpropto p(C_k) \\ p(x_1 \\mid C_k) \\ p(x_2\\mid C_k) \\ p(x_3\\mid C_k) \\ \\cdots \\\\\n","                            & \\varpropto p(C_k) \\prod_{i=1}^n p(x_i \\mid C_k)\\,.\n","\\end{align}\n","$$\n","\n","This means that under the above independence assumptions, the conditional distribution over the class variable $C$ is\n","\n","$$p(C_k \\mid x_1, \\dots, x_n) = \\frac{1}{Z} p(C_k) \\prod_{i=1}^n p(x_i \\mid C_k)$$\n","\n","where the evidence $Z = p(\\mathbf{x})$ is a scaling factor dependent only on $x_1, \\dots, x_n$, that is, a constant if the values of the feature variables are known.\n","\n","### Constructing a classifier from the probability model   \n","\n","The discussion so far has derived the independent feature model, that is, the naive Bayes probability model.  The naive Bayes classifier combines this model with a decision rule.  One common rule is to pick the hypothesis that is most probable; this is known as the 'maximum a posteriori' or 'MAP' decision rule.  The corresponding classifier, a Bayes classifier, is the function that assigns a class label $\\hat{y} = C_k$ for some $k$ as follows\n","\n","$$\\hat{y} = \\underset{k \\in \\{1, \\dots, K\\}}{\\operatorname{argmax}} \\ p(C_k) \\displaystyle\\prod_{i=1}^n p(x_i \\mid C_k).$$"]},{"cell_type":"markdown","metadata":{"id":"-UzluK1AO-1l"},"source":["## Gaussian naive Bayes\n","\n","When dealing with continuous data, a typical assumption is that the continuous values associated with each class are distributed according to a Normal distribution. For example, suppose the training data contains a continuous attribute, $x$. We first segment the data by the class, and then compute the mean and variance of $x$ in each class. Let $\\mu_c$ be the mean of the values in $x$ associated with class c, and let $\\sigma^2_c$ be the variance of the values in $x$ associated with class c. Suppose we have collected some observation value $v$. Then, the probability distribution of $v$ given a class $c$, $p(x=v \\mid c)$, can be computed by plugging $v$ into the equation for a Normal distribution parameterized by $\\mu_c$ and $\\sigma^2_c$. That is,\n","\n","$$\n","p(x=v \\mid c)=\\frac{1}{\\sqrt{2\\pi\\sigma^2_c}}\\,e^{ -\\frac{(v-\\mu_c)^2}{2\\sigma^2_c} }\n","$$\n","\n","Another common technique for handling continuous values is to use binning  the feature values, to obtain a new set of Bernoulli-distributed features; some literature in fact suggests that this is necessary to apply naive Bayes, but it is not, and the discretization may throw away discriminative information.\n","\n","\n","## Multinomial Naive Bayes \n","\n","In probability theory, the multinomial distribution is a generalization of the binomial distribution. For example, it models the probability of counts for rolling a k-sided dice n times. For n independent trials each of which leads to a success for exactly one of $k$ categories, with each category having a given fixed success probability, the multinomial distribution gives the probability of any particular combination of numbers of successes for the various categories.\n","\n","With a multinomial event model, samples (feature vectors) represent the frequencies with which certain events have been generated by a multinomial $(p_1, \\dots, p_n)$ where $p_i$ is the probability that event $i$ occurs $K$ such multinomials in the multiclass case). A feature vector $\\mathbf{x} = (x_1, \\dots, x_n)$ is then a histogram, with $x_i$ counting the number of times event $i$ was observed in a particular instance. This is the event model typically used for document classification, with events representing the occurrence of a word in a single document (see bag of words assumption). The likelihood of observing a histogram $x$ is given by\n","\n","$$\n","p(\\mathbf{x} \\mid C_k) = \\frac{(\\sum_i x_i)!}{\\prod_i x_i !} \\prod_i {p_{ki}}^{x_i}\n","$$\n","\n","The multinomial naive Bayes classifier becomes a linear classifier when expressed in log-space\n","\n","$$\n","\\begin{align}\n","\\log p(C_k \\mid \\mathbf{x}) & \\varpropto \\log \\left( p(C_k) \\prod_{i=1}^n {p_{ki}}^{x_i} \\right) \\\\\n"," & = \\log p(C_k) + \\sum_{i=1}^n x_i \\cdot \\log p_{ki} \\\\\n"," & = b + \\mathbf{w}_k^\\top \\mathbf{x}\n","\\end{align}\n","$$\n","\n","where $b = \\log p(C_k)$ and $w_{ki} = \\log p_{ki}$.\n","\n","If a given class and feature value never occur together in the training data, then the frequency-based probability estimate will be zero. This is problematic because it will wipe out all information in the other probabilities when they are multiplied. Therefore, it is often desirable to incorporate a small-sample correction, called pseudocount, in all probability estimates such that no probability is ever set to be exactly zero. This way of regularizing naive Bayes is called Laplace smoothing when the pseudocount is one, and Lidstone smoothing in the general case.\n","\n","Rennie et al. discuss problems with the multinomial assumption in the context of document classification and possible ways to alleviate those problems, including the use of tf–idf weights instead of raw term frequencies and document length normalization, to produce a naive Bayes classifier that is competitive with support vector machines.\n","\n","\n","## Bernoulli naive Bayes\n","\n","In probability theory and statistics, the Bernoulli distribution, named after Swiss scientist Jacob Bernoulli, is the probability distribution of a random variable which takes the value 1 with probability $p$ and the value 0 with probability $q=1-p$ —i.e., the probability distribution of any single experiment that asks a yes–no question; the question results in a boolean-valued function, a single bit of information whose value is success/yes/true/one with probability p and failure/no/false/zero with probability q. It can be used to represent a coin toss where 1 and 0 would represent \"head\" and \"tail\" (or vice versa), respectively. In particular, unfair coins would have $p \\neq 0.5$. \n","\n","The Bernoulli distribution is a special case of the binomial distribution where a single experiment/trial is conducted (n=1). It is also a special case of the two-point distribution, for which the outcome need not be a bit, i.e., the two possible outcomes need not be 0 and 1.\n","\n","In the multivariate Bernoulli event model, features are independent boolean data type describing inputs. Like the multinomial model, this model is popular for document classification tasks, where binary term occurrence features are used rather than term frequencies. If $x_i$ is a boolean expressing the occurrence or absence of the $i$'th term from the vocabulary, then the likelihood of a document given a class $C_k$ is given by\n","\n","$$\n","p(\\mathbf{x} \\mid C_k) = \\prod_{i=1}^n p_{ki}^{x_i} (1 - p_{ki})^{(1-x_i)}\n","$$\n","\n","where $p_{ki}$ is the probability of class $C_k$ generating the term $w_i$. This event model is especially popular for classifying short texts. It has the benefit of explicitly modelling the absence of terms. Note that a naive Bayes classifier with a Bernoulli event model is not the same as a multinomial NB classifier with frequency counts truncated to one.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"d9NlLyyBO-1m"},"source":["## Bayesian Probability\n","\n","In this lesson we will disucss Bayesian probability theory. There are no data sets, or libraires to be installed.\n","\n","## Probability and Statistics\n","\n","Probability is a measure of the likelihood of a random phenomenon or chance behavior.  Probability describes the long-term proportion with which a certain outcome will occur in situations with short-term uncertainty. \n","\n","### The Axioms of Probability\n","\n","\n","#### First axiom - The probability of an event is a non-negative real number:\n","$$\n","P(E)\\in\\mathbb{R}, P(E)\\geq 0 \\qquad \\forall E\\in F\n","$$\n","where $F$ is the event space\n","\n","#### Second axiom -  unit measure:\n","\n","The probability that some elementary event in the entire sample space will occur is 1.\n","\n","$$\n","P(\\Omega) = 1.\n","$$  \n","\n","#### Third axiom - the assumption of $\\sigma$-additivity:\n","\n","Any countable sequence of disjoint (synonymous with mutually exclusive) events $E_1, E_2, ...$ satisfies\n","\n","$$\n","P\\left(\\bigcup_{i = 1}^\\infty E_i\\right) = \\sum_{i=1}^\\infty P(E_i).\n","$$\n","\n","The total probability of all possible event always sums to 1. \n","\n","### Consequences of these axioms\n","\n","The probability of the empty set:\n","$$\n","P(\\varnothing)=0.\n","$$\n","\n","Monotonicity   \n","$$\n","\\quad\\text{if}\\quad A\\subseteq B\\quad\\text{then}\\quad P(A)\\leq P(B).\n","$$\n","\n","The numeric bound between 0 and 1:  \n","\n","$$\n","0\\leq P(E)\\leq 1\\qquad \\forall E\\in F.\n","$$\n","\n","\n","![Probability is expressed in numbers between 0 and 1](http://nikbearbrown.com/YouTube/MachineLearning/M10/Probability_0_1.png)    \n","*Probability is expressed in numbers between 0 and 1.*   \n","\n","\n","Probabilty of a certain event is 1:\n","\n","$$\n","P(True) = 1\n","$$\n","\n","Probability = 1 means it always happens.\n","\n","\n","Probabilty of an impossible event is 0:\n","\n","$$\n","P(False) = 0\n","$$\n","\n","Probability = 0 means the event never happens.  \n","\n","Probabilty of A or B:\n","\n","$$\n","P(A \\quad or \\quad B) = P(A) + P(A) - P(A \\quad and \\quad B) \n","$$\n","\n","or\n","\n","$$\n","P(A \\cup B) = P(A) + P(A) - P(A \\cap B) \n","$$\n","\n","\n","Probabilty of not A:\n","\n","$$\n","P(not  \\quad A) = 1- P(A) \n","$$\n","\n","\n","## Conditional Probability\n","\n","In probability theory, a [conditional probability](https://en.wikipedia.org/wiki/Conditional_probability) measures the probability of an event given that another event has occurred. That is,  \"the conditional probability of A given B.\"   \n","\n"," the conditional probability of A given B is defined as the quotient of the probability of the joint of events A and B, and the probability of B:\n"," \n","$$ \n","P(A|B) = \\frac{P(A \\cap B)}{P(B)}\n","$$\n","\n","This may be visualized using a Venn diagram. \n","\n","![P(A and B) Venn](http://nikbearbrown.com/YouTube/MachineLearning/M10/Conditional_Probability_Venn_Diagram.png)     \n","*$P(A \\cap B)$*\n","\n","### Corollary of Conditional Probability is The Chain Rule\n","\n","If we multiply both sides by $P(B)$ then\n","\n","$$ \n","P(A|B) = \\frac{P(A \\cap B)}{P(B)}\n","$$\n","\n","becomes\n","\n","$$ \n","P(A|B) P(B) = P(A \\cap B) \n","$$\n","\n","\n","\n","\n","\n","### Statistical independence\n","\n","Events A and B are defined to be statistically independent if:\n","\n","$$\n","\\begin{align}\n","             P(A \\cap B) &= P(A) P(B) \\\\\n","  \\Leftrightarrow P(A|B) &= P(A) \\\\\n","  \\Leftrightarrow P(B|A) &= P(B)\n","\\end{align}\n","$$\n","\n","That is, the occurrence of A does not affect the probability of B, and vice versa\n","\n","\n","Probabilty of A or B for independent events $P(A and B) is 0$:\n","\n","$$\n","P(A or B) = P(A) + P(A) \n","$$\n","\n","## Bayes Rule\n","\n","[Bayes' theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem) (alternatively Bayes' law or Bayes' rule) describes the probability of an event, given prior events. That is, a conditional probability.\n","\n","$$\n","P(A|B) = \\frac{P(A)\\, P(B | A)}{P(B)},\n","$$\n","\n","where A and B are events.\n","\n","* P(A) and P(B) are the independent probabilities of A and B.  \n","* P(A | B), a conditional probability, is the probability of observing event A given that B is true.  \n","* P(B | A), is the probability of observing event B given that A is true.  \n","\n","\n","## Bayesian inference\n","\n","[Bayesian inference](https://en.wikipedia.org/wiki/Bayesian_inference) is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as evidence. Bayesian inference derives the posterior probability as a consequence of two antecedents, a prior probability and a \"likelihood function\" derived from a statistical model for the observed data. \n","\n","Bayesian inference computes the posterior probability according to Bayes' theorem:\n","\n","$$\n","P(H\\mid E) = \\frac{P(E\\mid H) \\cdot P(H)}{P(E)}\n","$$\n","\n","where,    \n","\n","$P(H\\mid E)$ the posterior probability, denotes a conditional probability of $\\textstyle H$ (the hypothesis) whose probability may be affected by the evidence $\\textstyle E$.   \n","\n","$\\textstyle P(H)$, the prior probability, is an estimate of the probability that a hypothesis is true, before observing the current evidence.   \n","\n","$\\textstyle P(E\\mid H)$ is the probability of observing $\\textstyle E$ given $\\textstyle H$. It indicates the compatibility of the evidence with the given hypothesis.   \n","\n","$\\textstyle P(E)$ is sometimes termed the marginal likelihood or \"model evidence\". This factor is the same for all possible hypotheses being considered. \n","\n","Note that Bayes' rule can also be written as follows:\n","$$\n","P(H\\mid E) = \\frac{P(E\\mid H)}{P(E)} \\cdot P(H)\n","$$\n","where the factor $\\textstyle \\frac{P(E\\mid H)}{P(E)}$ represents the impact of $E$ on the probability of $H$.   \n","\n","## Bayesian probability example\n","\n","Suppose a certain disease has an incidence rate of 0.01% (that is, it afflicts 0.01% of the population).  A test has been devised to detect this disease.  The test does not produce false negatives (that is, anyone who has the disease will test positive for it), but the false positive rate is 1% (that is, about 1% of people who take the test will test positive, even though they do not have the disease).  Suppose a randomly selected person takes the test and tests positive.  What is the probability that this person actually has the disease?\n","\n","Bayes theorem would ask the question, what is the probability of disease given a postive result, or $P(disease\\mid positive))$. \n","\n","What do we know?  \n","\n","$P(positive\\mid disease)=1$ (i.e. The test does not produce false negatives.)     \n","$P(disease)=0.0001$ (i.e.  1/10,000 have the disease)      \n","$P(positive\\mid no disease)=0.01$ (i.e. he false positive rate is 1%. This means 1% of people who take the test will test positive, even though they do not have the disease)      \n","\n","Bayes’ Theorem\n","\n","$$\n","P(A|B) = \\frac{P(A)\\, P(B | A)}{P(B)},\n","$$\n","\n","which can be rewritten as  \n","\n","$$\n","P(A|B) = \\frac{P(A)\\, P(B | A)}{P(A)P(B|A)+P(\\bar{A})P(B|\\bar{A})},\n","$$\n","\n","which in our example is\n","$$\n","P(disease|positive) = \\frac{P(disease)\\, P(positive | disease)}{P(disease)P(positive|disease)+P(no \\quad  disease)P(positive|no \\quad disease)},\n","$$\n","\n","plugging in the numbers gives\n","\n","$$\n","P(disease|positive)= \\frac{(0.0001)\\, (1)}{(0.0001)(1)+(0.9999)(0.01)}, \\approx 0.01\n","$$\n","\n","So even though the test is 99% accurate, of all people who test positive, over 99% do not have the disease.  \n","\n","\n","# Bayesians versus Frequentists\n","\n"," \n","[Frequentist inference](https://en.wikipedia.org/wiki/Frequentist_inference) or frequentist statistics is a scheme for making statistical inference based on the frequency or proportion of the data. This effectively requires that conclusions should only be drawn with a set of repetitions.    \n","\n","Frequentists will only generate statistical inference given a large enough set of repetitions. In contrast, a Bayesian approach to inference does allow probabilities to be associated with unknown parameters.   \n","\n","![Count Von Count](http://nikbearbrown.com/YouTube/MachineLearning/M10/Count_von_Count_kneeling.png)   \n","*Count Von Count*   \n","- from https://en.wikipedia.org/wiki/File:Count_von_Count_kneeling.png  \n","\n","While \"probabilities\" are involved in both approaches to inference, frequentist probability is essentially equivelent to counting. The Bayesian approach allows these estimates of probabilities to be based upon counting but also allows for subjective estimates (i.e. guesses) of prior probabilities.\n","\n","Bayesian probability, also called evidential probability, or subjectivist probability, can be assigned to any statement whatsoever, even when no random process is involved. Evidential probabilities are considered to be degrees of belief, and a Bayesian can even use an un-informative prior (also called a non-informative or Jeffreys prior).\n","\n","In Bayesian probability, the [Jeffreys prior](https://en.wikipedia.org/wiki/Jeffreys_prior), named after Sir Harold Jeffreys, is a non-informative (objective) prior distribution for a parameter space.  The crucial idea behind the Jeffreys prior is the Jeffreys posterior. This posterior aims to reflect as best as possible the information about the parameters brought by the data, in effect  \"representing ignorance\" about the prior. This is sometimes called the \"principle of indifference.\" Jeffreys prior is proportional to the square root of the determinant of the Fisher information:\n","\n","$$\n","p\\left(\\vec\\theta\\right) \\propto \\sqrt{\\det \\mathcal{I}\\left(\\vec\\theta\\right)}.\\,\n","$$\n","\n","It has the key feature that it is invariant under reparameterization of the parameter vector $\\vec\\theta.$  \n","\n","At its essence the Bayesian can be vague or subjective about an inital guess at a prior probability. and the the posterior probability be updated data point by data point. A Bayesian defines a \"probability\" in the same way that many non-statisticians do - namely an indication of the plausibility or belief of a proposition.\n","\n","A Frequentist is someone that believes probabilities represent long run frequencies with which events occur; he or she will have a model (e.g. Guassian, uniform, etc.) of how the sample popluation was generated. The observed counts are considered a random sample the estimate the true parameters of the model.   \n","\n","It is important to note that most Frequentist methods have a Bayesian equivalent (that is, they give the same results) when there are enough repeated trails. That is, they converge the the same result given enough data.  \n","\n"]},{"cell_type":"markdown","metadata":{"id":"rdCdoE9TO-1n"},"source":["## Applying Bayes' theorem to iris classification\n","\n","Can **Bayes' theorem** predict the species of an iris?"]},{"cell_type":"code","metadata":{"id":"h_GLuQxmO-1n"},"source":["from __future__ import print_function\n","%matplotlib inline\n","import random\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import seaborn as sns\n","import scipy as sp\n","from sklearn.naive_bayes import MultinomialNB, GaussianNB\n","from sklearn.model_selection import KFold, cross_val_score, train_test_split\n","from sklearn import metrics\n","import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N45G2l4aO-1r","outputId":"627a8c60-1582-45db-e8c1-f162487ed626"},"source":["iris = sns.load_dataset(\"iris\")\n","iris.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style>\n","    .dataframe thead tr:only-child th {\n","        text-align: right;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: left;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sepal_length</th>\n","      <th>sepal_width</th>\n","      <th>petal_length</th>\n","      <th>petal_width</th>\n","      <th>species</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5.1</td>\n","      <td>3.5</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","      <td>setosa</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4.9</td>\n","      <td>3.0</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","      <td>setosa</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4.7</td>\n","      <td>3.2</td>\n","      <td>1.3</td>\n","      <td>0.2</td>\n","      <td>setosa</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4.6</td>\n","      <td>3.1</td>\n","      <td>1.5</td>\n","      <td>0.2</td>\n","      <td>setosa</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5.0</td>\n","      <td>3.6</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","      <td>setosa</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   sepal_length  sepal_width  petal_length  petal_width species\n","0           5.1          3.5           1.4          0.2  setosa\n","1           4.9          3.0           1.4          0.2  setosa\n","2           4.7          3.2           1.3          0.2  setosa\n","3           4.6          3.1           1.5          0.2  setosa\n","4           5.0          3.6           1.4          0.2  setosa"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"5w0dOApdO-1u","outputId":"8043563c-c535-45ab-fbbd-773360c83a5c"},"source":["# apply the ceiling function to the numeric columns\n","iris.loc[:, 'sepal_length':'petal_width'] = iris.loc[:, 'sepal_length':'petal_width'].apply(np.ceil)\n","iris.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style>\n","    .dataframe thead tr:only-child th {\n","        text-align: right;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: left;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sepal_length</th>\n","      <th>sepal_width</th>\n","      <th>petal_length</th>\n","      <th>petal_width</th>\n","      <th>species</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>6.0</td>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>setosa</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>setosa</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5.0</td>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>setosa</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5.0</td>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>setosa</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5.0</td>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>setosa</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   sepal_length  sepal_width  petal_length  petal_width species\n","0           6.0          4.0           2.0          1.0  setosa\n","1           5.0          3.0           2.0          1.0  setosa\n","2           5.0          4.0           2.0          1.0  setosa\n","3           5.0          4.0           2.0          1.0  setosa\n","4           5.0          4.0           2.0          1.0  setosa"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"JgdjaR04O-1x","outputId":"bdbe0f71-3ad1-4768-8297-5b132f48d608"},"source":["iris.describe()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style>\n","    .dataframe thead tr:only-child th {\n","        text-align: right;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: left;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sepal_length</th>\n","      <th>sepal_width</th>\n","      <th>petal_length</th>\n","      <th>petal_width</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>150.000000</td>\n","      <td>150.000000</td>\n","      <td>150.00000</td>\n","      <td>150.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>6.273333</td>\n","      <td>3.460000</td>\n","      <td>4.22000</td>\n","      <td>1.773333</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.889134</td>\n","      <td>0.551143</td>\n","      <td>1.74506</td>\n","      <td>0.696556</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>5.000000</td>\n","      <td>2.000000</td>\n","      <td>1.00000</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>6.000000</td>\n","      <td>3.000000</td>\n","      <td>2.00000</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>6.000000</td>\n","      <td>3.000000</td>\n","      <td>5.00000</td>\n","      <td>2.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>7.000000</td>\n","      <td>4.000000</td>\n","      <td>6.00000</td>\n","      <td>2.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>8.000000</td>\n","      <td>5.000000</td>\n","      <td>7.00000</td>\n","      <td>3.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       sepal_length  sepal_width  petal_length  petal_width\n","count    150.000000   150.000000     150.00000   150.000000\n","mean       6.273333     3.460000       4.22000     1.773333\n","std        0.889134     0.551143       1.74506     0.696556\n","min        5.000000     2.000000       1.00000     1.000000\n","25%        6.000000     3.000000       2.00000     1.000000\n","50%        6.000000     3.000000       5.00000     2.000000\n","75%        7.000000     4.000000       6.00000     2.000000\n","max        8.000000     5.000000       7.00000     3.000000"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"kku27ZiKO-1z","outputId":"f7510145-0bde-4910-f82d-ba7af70ef835"},"source":["iris['sepal_length'].value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6.0    57\n","7.0    49\n","5.0    32\n","8.0    12\n","Name: sepal_length, dtype: int64"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"RgTZprQ8O-12","outputId":"e25873e9-5567-4401-a07c-6c1f2b4fcba2"},"source":["iris['sepal_width'].value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3.0    82\n","4.0    64\n","5.0     3\n","2.0     1\n","Name: sepal_width, dtype: int64"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"9oQwLLroO-14","outputId":"df5a7299-026d-43d8-d986-9af11af07900"},"source":["iris['petal_length'].value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2.0    49\n","5.0    42\n","6.0    33\n","4.0    15\n","7.0     9\n","3.0     1\n","1.0     1\n","Name: petal_length, dtype: int64"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"guh_O__tO-19","outputId":"132f9cb7-b534-4e15-bb4f-1c7f5cda4207"},"source":["iris['petal_width'].value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2.0    70\n","1.0    57\n","3.0    23\n","Name: petal_width, dtype: int64"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"2gzWPEuxO-1_","outputId":"bac21d08-a411-4d38-a743-0b0c0641ec12"},"source":["iris['species'].value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["setosa        50\n","versicolor    50\n","virginica     50\n","Name: species, dtype: int64"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"rjg_uG0BO-2B"},"source":["## Making a Bayesian prediction\n","\n","Let's say that I have an **out-of-sample iris** with the following measurements (Highest counts of each column): **7, 3, 5, 2**. How might I predict the species?"]},{"cell_type":"code","metadata":{"id":"iT1eH4aRO-2C","outputId":"3a0ff86d-2abd-43a6-ee71-cb30fb6d4c4e"},"source":["iris[(iris.sepal_length==7) & (iris.sepal_width==3) & (iris.petal_length==5) & (iris.petal_width==2)]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style>\n","    .dataframe thead tr:only-child th {\n","        text-align: right;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: left;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sepal_length</th>\n","      <th>sepal_width</th>\n","      <th>petal_length</th>\n","      <th>petal_width</th>\n","      <th>species</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>54</th>\n","      <td>7.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>versicolor</td>\n","    </tr>\n","    <tr>\n","      <th>58</th>\n","      <td>7.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>versicolor</td>\n","    </tr>\n","    <tr>\n","      <th>63</th>\n","      <td>7.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>versicolor</td>\n","    </tr>\n","    <tr>\n","      <th>68</th>\n","      <td>7.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>versicolor</td>\n","    </tr>\n","    <tr>\n","      <th>72</th>\n","      <td>7.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>versicolor</td>\n","    </tr>\n","    <tr>\n","      <th>73</th>\n","      <td>7.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>versicolor</td>\n","    </tr>\n","    <tr>\n","      <th>74</th>\n","      <td>7.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>versicolor</td>\n","    </tr>\n","    <tr>\n","      <th>75</th>\n","      <td>7.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>versicolor</td>\n","    </tr>\n","    <tr>\n","      <th>76</th>\n","      <td>7.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>versicolor</td>\n","    </tr>\n","    <tr>\n","      <th>77</th>\n","      <td>7.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>versicolor</td>\n","    </tr>\n","    <tr>\n","      <th>87</th>\n","      <td>7.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>versicolor</td>\n","    </tr>\n","    <tr>\n","      <th>91</th>\n","      <td>7.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>versicolor</td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>7.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>versicolor</td>\n","    </tr>\n","    <tr>\n","      <th>123</th>\n","      <td>7.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>virginica</td>\n","    </tr>\n","    <tr>\n","      <th>126</th>\n","      <td>7.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>virginica</td>\n","    </tr>\n","    <tr>\n","      <th>127</th>\n","      <td>7.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>virginica</td>\n","    </tr>\n","    <tr>\n","      <th>146</th>\n","      <td>7.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>virginica</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     sepal_length  sepal_width  petal_length  petal_width     species\n","54            7.0          3.0           5.0          2.0  versicolor\n","58            7.0          3.0           5.0          2.0  versicolor\n","63            7.0          3.0           5.0          2.0  versicolor\n","68            7.0          3.0           5.0          2.0  versicolor\n","72            7.0          3.0           5.0          2.0  versicolor\n","73            7.0          3.0           5.0          2.0  versicolor\n","74            7.0          3.0           5.0          2.0  versicolor\n","75            7.0          3.0           5.0          2.0  versicolor\n","76            7.0          3.0           5.0          2.0  versicolor\n","77            7.0          3.0           5.0          2.0  versicolor\n","87            7.0          3.0           5.0          2.0  versicolor\n","91            7.0          3.0           5.0          2.0  versicolor\n","97            7.0          3.0           5.0          2.0  versicolor\n","123           7.0          3.0           5.0          2.0   virginica\n","126           7.0          3.0           5.0          2.0   virginica\n","127           7.0          3.0           5.0          2.0   virginica\n","146           7.0          3.0           5.0          2.0   virginica"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"vF-lYSl5O-2E","outputId":"28e9e967-9586-475b-c2d4-9bfe28b19879"},"source":["# count the species for these observations\n","iris[(iris.sepal_length==7) & (iris.sepal_width==3) & (iris.petal_length==5) & (iris.petal_width==2)].species.value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["versicolor    13\n","virginica      4\n","Name: species, dtype: int64"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"FhFHnWcPO-2G"},"source":["## What is the probability of some particular species, given the measurements 7, 3, 5, and 2?\n","\n","$$P(species \\ | \\ 7352)$$\n","\n","We could calculate the conditional probability for **each of the three species**, and then predict the species with the **highest probability**:\n","\n","$$P(setosa \\ | \\ 7352)$$\n","$$P(versicolor \\ | \\ 7352)$$\n","$$P(virginica \\ | \\ 7352)$$\n","\n","### Calculating the conditional probability of versicolor\n","\n","**Bayes' theorem** gives us a way to calculate these conditional probabilities.\n","\n","$$P(setosa) = P(versicolor) = P(virginica) = 50/150 = 1/3$$\n","\n","For **versicolor**:\n","\n","$$P(versicolor \\ | \\ 7352) = \\frac {P(7352 \\ | \\ versicolor) \\times P(versicolor)} {P(7352)}$$\n","\n","We can calculate each of the terms on the right side of the equation:\n","\n","$$P(7352 \\ | \\ versicolor) = \\frac {13} {50} = 0.26$$\n","\n","$$P(versicolor) = \\frac {50} {150} = 0.33$$\n","\n","$$P(7352) = \\frac {17} {150} = 0.11$$\n","\n","Bayes' theorem says:\n","\n","$$P(versicolor \\ | \\ 7352) = \\frac {0.26 \\times 0.33} {0.11} = 0.765$$\n","\n"]},{"cell_type":"code","metadata":{"id":"01S46yuUO-2H","outputId":"e8ecfee8-6cbb-4202-b3ef-2ba7685b8600"},"source":["((13/50)*(50/150))/(17/150)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7647058823529412"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"X8JIzIR_O-2J"},"source":["### Quiz calculate P(virginica  | 7352) and P(setosa |  7352)\n","\n","\n","**Calculate $P(virginica \\ | \\ 7352)$ and $P(setosa \\ | \\ 7352)$** \n","\n","\n","### $P(virginica \\ | \\ 7352)$\n","\n","$$P(virginica \\ | \\ 7352) = \\frac {P(7352 \\ | \\ virginica) \\times P(virginica)} {P(7352)}$$\n","\n","We can calculate each of the terms on the right side of the equation:\n","\n","$$P(7352 \\ | \\ virginica) = \\frac {4} {50} = 0.26$$\n","\n","$$P(virginica) = \\frac {50} {150} = 0.33$$\n","\n","$$P(7352) = \\frac {17} {150} = 0.11$$\n","\n","Bayes' theorem says:\n","\n","$$P(virginica \\ | \\ 7352) = \\frac {0.08 \\times 0.33} {0.11} = 0.25$$\n","\n","\n","### $P(setosa \\ | \\ 7352)$\n","\n","\n","$$P(setosa \\ | \\ 7352) = \\frac {P(7352 \\ | \\ setosa) \\times P(setosa)} {P(7352)}$$\n","\n","We can calculate each of the terms on the right side of the equation:\n","\n","$$P(7352 \\ | \\ setosa) = \\frac {0} {50} = 0$$\n","\n","$$P(setosa) = \\frac {50} {150} = 0.33$$\n","\n","$$P(7352) = \\frac {17} {150} = 0.11$$\n","\n","Bayes' theorem says:\n","\n","$$P(virginica \\ | \\ 7352) = \\frac {0.0 \\times 0.33} {0.11} = 0$$"]},{"cell_type":"code","metadata":{"id":"OUafTLzzO-2J","outputId":"437d03d8-e63c-44cd-bb23-26254901ebf0"},"source":["((4/50)*(50/150))/(17/150)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.23529411764705882"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"3IAlDTsxO-2M","outputId":"e8f94c64-bcb6-4342-d80f-df7358955851"},"source":["((0/50)*(50/150))/(17/150)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.0"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"-UNRM8aBO-2O"},"source":["## Predict that the iris is a versicolor\n","\n","Bayes' theorem says:\n","\n","$$P(versicolor \\ | \\ 7352) = \\frac {0.26 \\times 0.33} {0.11} = 0.76$$\n","\n","\n","$$P(virginica \\ | \\ 7352) = \\frac {0.08 \\times 0.33} {0.11} = 0.25$$\n","\n","$$P(setosa \\ | \\ 7352) = \\frac {0 \\times 0.33} {0.11} = 0$$\n","\n","We predict that the iris is a versicolor, given it has the **highest conditional probability**."]},{"cell_type":"markdown","metadata":{"id":"dBUe1bpBO-2O"},"source":["## Building a Naive Bayes model\n"]},{"cell_type":"markdown","metadata":{"id":"NkvEtuSQO-2P"},"source":["## Comparing Multinomial and Gaussian Naive Bayes\n","\n","scikit-learn documentation: [MultinomialNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) and [GaussianNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)\n","\n","\n","* Bernoulli Naive Bayes The binomial model is useful if your feature vectors are binary (i.e., 0s and 1s).  One application would be text classification with ‘bag of words’ model where the 1s & 0s are “word occurs in the document” and “word does not occur in the document” respectively.\n","\n","* Multinomial Naive Bayes The multinomial naive Bayes model is typically used for discrete counts. That is , the \"number of times outcome number  $x_i$  is observed over the n trials\".  For example, the  “count how often word occurs in the document.”\n","\n","* Gaussian Naive Bayes Here, we assume that the features follow a normal distribution. Instead of discrete counts, we have continuous features.\n","\n","\n","Dataset: [Pima Indians Diabetes](https://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes) from the UCI Machine Learning Repository"]},{"cell_type":"code","metadata":{"id":"qbOJcE-zO-2P"},"source":["data = 'data/pima-indians-diabetes.csv'\n","# col_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']\n","pima = pd.read_csv(data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0SEVjT-5O-2R","outputId":"4f7a2eeb-387b-49e7-dd51-a85e6e9adc42"},"source":["# notice that all features are continuous\n","pima.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style>\n","    .dataframe thead tr:only-child th {\n","        text-align: right;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: left;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Pregnancies</th>\n","      <th>Glucose</th>\n","      <th>BloodPressure</th>\n","      <th>SkinThickness</th>\n","      <th>Insulin</th>\n","      <th>BMI</th>\n","      <th>DiabetesPedigreeFunction</th>\n","      <th>Age</th>\n","      <th>Outcome</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>6</td>\n","      <td>148</td>\n","      <td>72</td>\n","      <td>35</td>\n","      <td>0</td>\n","      <td>33.6</td>\n","      <td>0.627</td>\n","      <td>50</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>85</td>\n","      <td>66</td>\n","      <td>29</td>\n","      <td>0</td>\n","      <td>26.6</td>\n","      <td>0.351</td>\n","      <td>31</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>8</td>\n","      <td>183</td>\n","      <td>64</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>23.3</td>\n","      <td>0.672</td>\n","      <td>32</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>89</td>\n","      <td>66</td>\n","      <td>23</td>\n","      <td>94</td>\n","      <td>28.1</td>\n","      <td>0.167</td>\n","      <td>21</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>137</td>\n","      <td>40</td>\n","      <td>35</td>\n","      <td>168</td>\n","      <td>43.1</td>\n","      <td>2.288</td>\n","      <td>33</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n","0            6      148             72             35        0  33.6   \n","1            1       85             66             29        0  26.6   \n","2            8      183             64              0        0  23.3   \n","3            1       89             66             23       94  28.1   \n","4            0      137             40             35      168  43.1   \n","\n","   DiabetesPedigreeFunction  Age  Outcome  \n","0                     0.627   50        1  \n","1                     0.351   31        0  \n","2                     0.672   32        1  \n","3                     0.167   21        0  \n","4                     2.288   33        1  "]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"DEm-ruHiO-2U"},"source":["# create X and y\n","y = pima.Outcome\n","X = pima.drop('Outcome', axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C-uv7fM4O-2W"},"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bmr48cyMO-2Y","outputId":"160bf0a9-3b08-4b60-8c38-1a6f83662b67"},"source":["mnb = MultinomialNB()\n","mnb.fit(X_train, y_train)\n","y_pred_class = mnb.predict(X_test)\n","print (metrics.accuracy_score(y_test, y_pred_class))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.5416666666666666\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"ExINrnAuO-2a","outputId":"751a95ee-fa1e-47fd-e8c9-51922310f594"},"source":["# testing accuracy of Gaussian Naive Bayes\n","gnb = GaussianNB()\n","gnb.fit(X_train, y_train)\n","y_pred_class = gnb.predict(X_test)\n","print (metrics.accuracy_score(y_test, y_pred_class))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.7916666666666666\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kGsI1jz8O-2c"},"source":["### Discusion Multinomial and Gaussian Naive Bayes\n","\n","Note the both the multinomial and Gaussian Naive Bayes work in the sense that the python code runs. The multinomial Naive Bayes is appropriate and gives terrible (coin flip level) results.\n"]},{"cell_type":"markdown","metadata":{"id":"AXOpg5WsO-2c"},"source":["Last update July 8, 2019"]}]}