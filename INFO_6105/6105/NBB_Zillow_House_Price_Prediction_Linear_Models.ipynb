{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zillow House Price Prediction Linear Models\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "Zillow as one of the largest marketplaces for real estate information in the U.S, it’s Zestimate is a prediction of a property value. Zillow hosted a [Kaggle Competition](https://www.kaggle.com/c/zillow-prize-1/data) with over a million in prizes to get ideas to improve their Zestimate.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "import random, os, sys\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import logging\n",
    "import csv\n",
    "import optparse\n",
    "import time\n",
    "import json\n",
    "from distutils.util import strtobool\n",
    "import psutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import h5py\n",
    "import warnings\n",
    "from tensorflow.python.framework import ops\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target='logerror'\n",
    "min_mem_size=6 \n",
    "run_time=333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "pct_memory=0.5\n",
    "virtual_memory=psutil.virtual_memory()\n",
    "min_mem_size=int(round(int(pct_memory*virtual_memory.available)/1073741824,0))\n",
    "print(min_mem_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:19372..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"1.8.0_121\"; OpenJDK Runtime Environment (Zulu 8.20.0.5-macosx) (build 1.8.0_121-b15); OpenJDK 64-Bit Server VM (Zulu 8.20.0.5-macosx) (build 25.121-b15, mixed mode)\n",
      "  Starting server from /Users/bear/anaconda/lib/python3.6/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /var/folders/lh/42j8mfjx069d1bkc2wlf2pw40000gn/T/tmp3p_k25uk\n",
      "  JVM stdout: /var/folders/lh/42j8mfjx069d1bkc2wlf2pw40000gn/T/tmp3p_k25uk/h2o_bear_started_from_python.out\n",
      "  JVM stderr: /var/folders/lh/42j8mfjx069d1bkc2wlf2pw40000gn/T/tmp3p_k25uk/h2o_bear_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:19372\n",
      "Connecting to H2O server at http://127.0.0.1:19372... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>03 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>America/New_York</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.22.1.3</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>25 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_bear_114ib0</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>3.556 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:19372</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.5 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ----------------------------------------\n",
       "H2O cluster uptime:         03 secs\n",
       "H2O cluster timezone:       America/New_York\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.22.1.3\n",
       "H2O cluster version age:    25 days\n",
       "H2O cluster name:           H2O_from_python_bear_114ib0\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    3.556 Gb\n",
       "H2O cluster total cores:    8\n",
       "H2O cluster allowed cores:  8\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:19372\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.6.5 final\n",
       "--------------------------  ----------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 65535 Highest port no\n",
    "port_no=random.randint(5555,55555)\n",
    "\n",
    "#  h2o.init(strict_version_check=False,min_mem_size_GB=min_mem_size,port=port_no) # start h2o\n",
    "try:\n",
    "  h2o.init(strict_version_check=False,min_mem_size_GB=min_mem_size,port=port_no) # start h2o\n",
    "except:\n",
    "  logging.critical('h2o.init')\n",
    "  h2o.download_all_logs(dirname=logs_path, filename=logfile)      \n",
    "  h2o.cluster().shutdown()\n",
    "  sys.exit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the property data from properties_2016 file\n",
    "data_file='kaggle_data/properties_2016_scrubbed_good.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the properties file, we can see there are 58 colunms. According to zillow data dictionary each field is as follows:\n",
    "- 'airconditioningtypeid'\t Type of cooling system present in the home (if any)\n",
    "- 'architecturalstyletypeid'\t Architectural style of the home (i.e. ranch, colonial, split-level, etc…)\n",
    "- 'basementsqft'\t Finished living area below or partially below ground level\n",
    "- 'bathroomcnt'\t Number of bathrooms in home including fractional bathrooms\n",
    "- 'bedroomcnt'\t Number of bedrooms in home \n",
    "- 'buildingqualitytypeid'\t Overall assessment of condition of the building from best (lowest) to worst (highest)\n",
    "- 'buildingclasstypeid'\tThe building framing type (steel frame, wood frame, concrete/brick) \n",
    "- 'calculatedbathnbr'\t Number of bathrooms in home including fractional bathroom\n",
    "- 'decktypeid'\tType of deck (if any) present on parcel\n",
    "- 'threequarterbathnbr'\t Number of 3/4 bathrooms in house (shower + sink + toilet)\n",
    "- 'finishedfloor1squarefeet'\t Size of the finished living area on the first (entry) floor of the home\n",
    "- 'calculatedfinishedsquarefeet'\t Calculated total finished living area of the home \n",
    "- 'finishedsquarefeet6'\tBase unfinished and finished area\n",
    "- 'finishedsquarefeet12'\tFinished living area\n",
    "- 'finishedsquarefeet13'\tPerimeter  living area\n",
    "- 'finishedsquarefeet15'\tTotal area\n",
    "- 'finishedsquarefeet50'\t Size of the finished living area on the first (entry) floor of the home\n",
    "- 'fips'\t Federal Information Processing Standard code -  see https://en.wikipedia.org/wiki/FIPS_county_code for more details\n",
    "- 'fireplacecnt'\t Number of fireplaces in a home (if any)\n",
    "- 'fireplaceflag'\t Is a fireplace present in this home \n",
    "- 'fullbathcnt'\t Number of full bathrooms (sink, shower + bathtub, and toilet) present in home\n",
    "- 'garagecarcnt'\t Total number of garages on the lot including an attached garage\n",
    "- 'garagetotalsqft'\t Total number of square feet of all garages on lot including an attached garage\n",
    "- 'hashottuborspa'\t Does the home have a hot tub or spa\n",
    "- 'heatingorsystemtypeid'\t Type of home heating system\n",
    "- 'latitude'\t Latitude of the middle of the parcel multiplied by 10e6\n",
    "- 'longitude'\t Longitude of the middle of the parcel multiplied by 10e6\n",
    "- 'lotsizesquarefeet'\t Area of the lot in square feet\n",
    "- 'numberofstories'\t Number of stories or levels the home has\n",
    "- 'parcelid'\t Unique identifier for parcels (lots) \n",
    "- 'poolcnt'\t Number of pools on the lot (if any)\n",
    "- 'poolsizesum'\t Total square footage of all pools on property\n",
    "- 'pooltypeid10'\t Spa or Hot Tub\n",
    "- 'pooltypeid2'\t Pool with Spa/Hot Tub\n",
    "- 'pooltypeid7'\t Pool without hot tub\n",
    "- 'propertycountylandusecode'\t County land use code i.e. it's zoning at the county level\n",
    "- 'propertylandusetypeid'\t Type of land use the property is zoned for\n",
    "- 'propertyzoningdesc'\t Description of the allowed land uses (zoning) for that property\n",
    "- 'rawcensustractandblock'\t Census tract and block ID combined - also contains blockgroup assignment by extension\n",
    "- 'censustractandblock'\t Census tract and block ID combined - also contains blockgroup assignment by extension\n",
    "- 'regionidcounty'\tCounty in which the property is located\n",
    "- 'regionidcity'\t City in which the property is located (if any)\n",
    "- 'regionidzip'\t Zip code in which the property is located\n",
    "- 'regionidneighborhood'\tNeighborhood in which the property is located\n",
    "- 'roomcnt'\t Total number of rooms in the principal residence\n",
    "- 'storytypeid'\t Type of floors in a multi-story house (i.e. basement and main level, split-level, attic, etc.).  See tab for details.\n",
    "- 'typeconstructiontypeid'\t What type of construction material was used to construct the home\n",
    "- 'unitcnt'\t Number of units the structure is built into (i.e. 2 = duplex, 3 = triplex, etc...)\n",
    "- 'yardbuildingsqft17'\tPatio in  yard\n",
    "- 'yardbuildingsqft26'\tStorage shed/building in yard\n",
    "- 'yearbuilt'\t The Year the principal residence was built \n",
    "- 'taxvaluedollarcnt'\tThe total tax assessed value of the parcel\n",
    "- 'structuretaxvaluedollarcnt'\tThe assessed value of the built structure on the parcel\n",
    "- 'landtaxvaluedollarcnt'\tThe assessed value of the land area of the parcel\n",
    "- 'taxamount'\tThe total property tax assessed for that assessment year\n",
    "- 'assessmentyear'\tThe year of the property tax assessment \n",
    "- 'taxdelinquencyflag'\tProperty taxes for this parcel are past due as of 2015\n",
    "- 'taxdelinquencyyear'\tYear for which the unpaid propert taxes were due \n",
    "\n",
    "Obviously, we can not use every column of data. Because some data has too many missing values, some data is not suitable for the prediction and some data can be calculated by some other data. So we need to do the data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "hf = h2o.import_file(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  logerror</th><th style=\"text-align: right;\">  regionidneighborhood</th><th style=\"text-align: right;\">  garagetotalsqft</th><th style=\"text-align: right;\">  finishedsquarefeet12</th><th style=\"text-align: right;\">  structuretaxvaluedollarcnt</th><th style=\"text-align: right;\">  yearbuilt</th><th style=\"text-align: right;\">  taxamount</th><th style=\"text-align: right;\">  calculatedfinishedsquarefeet</th><th style=\"text-align: right;\">  taxvaluedollarcnt</th><th style=\"text-align: right;\">  landtaxvaluedollarcnt</th><th style=\"text-align: right;\">   latitude</th><th style=\"text-align: right;\">  finishedsquarefeet15</th><th style=\"text-align: right;\">  lotsizesquarefeet</th><th style=\"text-align: right;\">  propertyzoningdesc</th><th style=\"text-align: right;\">  bedroomcnt</th><th style=\"text-align: right;\">   longitude</th><th style=\"text-align: right;\">  yardbuildingsqft17</th><th style=\"text-align: right;\">  finishedsquarefeet6</th><th style=\"text-align: right;\">  bathroomcnt</th><th style=\"text-align: right;\">  calculatedbathnbr</th><th style=\"text-align: right;\">  buildingqualitytypeid</th><th style=\"text-align: right;\">  fullbathcnt</th><th style=\"text-align: right;\">  regionidcity</th><th style=\"text-align: right;\">  propertycountylandusecode</th><th style=\"text-align: right;\">  heatingorsystemtypeid</th><th style=\"text-align: right;\">   parcelid</th><th style=\"text-align: right;\">  regionidzip</th><th style=\"text-align: right;\">  taxdelinquencyyear</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">    0.0953</td><td style=\"text-align: right;\">                   nan</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">                  1264</td><td style=\"text-align: right;\">                      115087</td><td style=\"text-align: right;\">       1986</td><td style=\"text-align: right;\">    2015.06</td><td style=\"text-align: right;\">                          1264</td><td style=\"text-align: right;\">             191811</td><td style=\"text-align: right;\">                  76724</td><td style=\"text-align: right;\">3.43036e+07</td><td style=\"text-align: right;\">                   nan</td><td style=\"text-align: right;\">               1735</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">-1.19287e+08</td><td style=\"text-align: right;\">                 128</td><td style=\"text-align: right;\">                  nan</td><td style=\"text-align: right;\">          2.5</td><td style=\"text-align: right;\">                2.5</td><td style=\"text-align: right;\">                    nan</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">         34543</td><td style=\"text-align: right;\">                       1128</td><td style=\"text-align: right;\">                    nan</td><td style=\"text-align: right;\">1.70738e+07</td><td style=\"text-align: right;\">        97081</td><td style=\"text-align: right;\">                 nan</td></tr>\n",
       "<tr><td style=\"text-align: right;\">    0.0198</td><td style=\"text-align: right;\">                   nan</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">                   777</td><td style=\"text-align: right;\">                      143809</td><td style=\"text-align: right;\">       1990</td><td style=\"text-align: right;\">    2581.3 </td><td style=\"text-align: right;\">                           777</td><td style=\"text-align: right;\">             239679</td><td style=\"text-align: right;\">                  95870</td><td style=\"text-align: right;\">3.42729e+07</td><td style=\"text-align: right;\">                   nan</td><td style=\"text-align: right;\">                nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">-1.19199e+08</td><td style=\"text-align: right;\">                 198</td><td style=\"text-align: right;\">                  nan</td><td style=\"text-align: right;\">          1  </td><td style=\"text-align: right;\">                1  </td><td style=\"text-align: right;\">                    nan</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         34543</td><td style=\"text-align: right;\">                       1129</td><td style=\"text-align: right;\">                    nan</td><td style=\"text-align: right;\">1.7089e+07 </td><td style=\"text-align: right;\">        97083</td><td style=\"text-align: right;\">                 nan</td></tr>\n",
       "<tr><td style=\"text-align: right;\">    0.006 </td><td style=\"text-align: right;\">                   nan</td><td style=\"text-align: right;\">              441</td><td style=\"text-align: right;\">                  1101</td><td style=\"text-align: right;\">                       33619</td><td style=\"text-align: right;\">       1956</td><td style=\"text-align: right;\">     591.64</td><td style=\"text-align: right;\">                          1101</td><td style=\"text-align: right;\">              47853</td><td style=\"text-align: right;\">                  14234</td><td style=\"text-align: right;\">3.43408e+07</td><td style=\"text-align: right;\">                   nan</td><td style=\"text-align: right;\">               6569</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">-1.1908e+08 </td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                  nan</td><td style=\"text-align: right;\">          2  </td><td style=\"text-align: right;\">                2  </td><td style=\"text-align: right;\">                    nan</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">         26965</td><td style=\"text-align: right;\">                       1111</td><td style=\"text-align: right;\">                    nan</td><td style=\"text-align: right;\">1.71004e+07</td><td style=\"text-align: right;\">        97113</td><td style=\"text-align: right;\">                 nan</td></tr>\n",
       "<tr><td style=\"text-align: right;\">   -0.0566</td><td style=\"text-align: right;\">                   nan</td><td style=\"text-align: right;\">              460</td><td style=\"text-align: right;\">                  1554</td><td style=\"text-align: right;\">                       45609</td><td style=\"text-align: right;\">       1965</td><td style=\"text-align: right;\">     682.78</td><td style=\"text-align: right;\">                          1554</td><td style=\"text-align: right;\">              62914</td><td style=\"text-align: right;\">                  17305</td><td style=\"text-align: right;\">3.43543e+07</td><td style=\"text-align: right;\">                   nan</td><td style=\"text-align: right;\">               7400</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">-1.19076e+08</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                  nan</td><td style=\"text-align: right;\">          1.5</td><td style=\"text-align: right;\">                1.5</td><td style=\"text-align: right;\">                    nan</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         26965</td><td style=\"text-align: right;\">                       1110</td><td style=\"text-align: right;\">                    nan</td><td style=\"text-align: right;\">1.71024e+07</td><td style=\"text-align: right;\">        97113</td><td style=\"text-align: right;\">                 nan</td></tr>\n",
       "<tr><td style=\"text-align: right;\">    0.0573</td><td style=\"text-align: right;\">                   nan</td><td style=\"text-align: right;\">              665</td><td style=\"text-align: right;\">                  2415</td><td style=\"text-align: right;\">                      277000</td><td style=\"text-align: right;\">       1984</td><td style=\"text-align: right;\">    5886.92</td><td style=\"text-align: right;\">                          2415</td><td style=\"text-align: right;\">             554000</td><td style=\"text-align: right;\">                 277000</td><td style=\"text-align: right;\">3.42666e+07</td><td style=\"text-align: right;\">                   nan</td><td style=\"text-align: right;\">               6326</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">-1.19165e+08</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                  nan</td><td style=\"text-align: right;\">          2.5</td><td style=\"text-align: right;\">                2.5</td><td style=\"text-align: right;\">                    nan</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">         34543</td><td style=\"text-align: right;\">                       1111</td><td style=\"text-align: right;\">                    nan</td><td style=\"text-align: right;\">1.71096e+07</td><td style=\"text-align: right;\">        97084</td><td style=\"text-align: right;\">                 nan</td></tr>\n",
       "<tr><td style=\"text-align: right;\">    0.0564</td><td style=\"text-align: right;\">                   nan</td><td style=\"text-align: right;\">              473</td><td style=\"text-align: right;\">                  2882</td><td style=\"text-align: right;\">                      222070</td><td style=\"text-align: right;\">       1980</td><td style=\"text-align: right;\">    3110.44</td><td style=\"text-align: right;\">                          2882</td><td style=\"text-align: right;\">             289609</td><td style=\"text-align: right;\">                  67539</td><td style=\"text-align: right;\">3.424e+07  </td><td style=\"text-align: right;\">                   nan</td><td style=\"text-align: right;\">              10000</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">-1.19025e+08</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                  nan</td><td style=\"text-align: right;\">          2.5</td><td style=\"text-align: right;\">                2.5</td><td style=\"text-align: right;\">                    nan</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">         51239</td><td style=\"text-align: right;\">                       1111</td><td style=\"text-align: right;\">                    nan</td><td style=\"text-align: right;\">1.71258e+07</td><td style=\"text-align: right;\">        97089</td><td style=\"text-align: right;\">                 nan</td></tr>\n",
       "<tr><td style=\"text-align: right;\">    0.0315</td><td style=\"text-align: right;\">                   nan</td><td style=\"text-align: right;\">              467</td><td style=\"text-align: right;\">                  1772</td><td style=\"text-align: right;\">                      185000</td><td style=\"text-align: right;\">       1978</td><td style=\"text-align: right;\">    5632.2 </td><td style=\"text-align: right;\">                          1772</td><td style=\"text-align: right;\">             526000</td><td style=\"text-align: right;\">                 341000</td><td style=\"text-align: right;\">3.42268e+07</td><td style=\"text-align: right;\">                   nan</td><td style=\"text-align: right;\">               8059</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">-1.1906e+08 </td><td style=\"text-align: right;\">                1045</td><td style=\"text-align: right;\">                  nan</td><td style=\"text-align: right;\">          2  </td><td style=\"text-align: right;\">                2  </td><td style=\"text-align: right;\">                    nan</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">         51239</td><td style=\"text-align: right;\">                       1111</td><td style=\"text-align: right;\">                    nan</td><td style=\"text-align: right;\">1.71329e+07</td><td style=\"text-align: right;\">        97089</td><td style=\"text-align: right;\">                 nan</td></tr>\n",
       "<tr><td style=\"text-align: right;\">    0.0257</td><td style=\"text-align: right;\">                   nan</td><td style=\"text-align: right;\">              440</td><td style=\"text-align: right;\">                  2632</td><td style=\"text-align: right;\">                      342611</td><td style=\"text-align: right;\">       1971</td><td style=\"text-align: right;\">    6109.94</td><td style=\"text-align: right;\">                          2632</td><td style=\"text-align: right;\">             571086</td><td style=\"text-align: right;\">                 228475</td><td style=\"text-align: right;\">3.42298e+07</td><td style=\"text-align: right;\">                   nan</td><td style=\"text-align: right;\">               7602</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">           5</td><td style=\"text-align: right;\">-1.1905e+08 </td><td style=\"text-align: right;\">                 180</td><td style=\"text-align: right;\">                  nan</td><td style=\"text-align: right;\">          2.5</td><td style=\"text-align: right;\">                2.5</td><td style=\"text-align: right;\">                    nan</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">         51239</td><td style=\"text-align: right;\">                       1111</td><td style=\"text-align: right;\">                    nan</td><td style=\"text-align: right;\">1.71349e+07</td><td style=\"text-align: right;\">        97089</td><td style=\"text-align: right;\">                 nan</td></tr>\n",
       "<tr><td style=\"text-align: right;\">    0.002 </td><td style=\"text-align: right;\">                   nan</td><td style=\"text-align: right;\">              494</td><td style=\"text-align: right;\">                  1292</td><td style=\"text-align: right;\">                      231297</td><td style=\"text-align: right;\">       1979</td><td style=\"text-align: right;\">    5026.4 </td><td style=\"text-align: right;\">                          1292</td><td style=\"text-align: right;\">             462594</td><td style=\"text-align: right;\">                 231297</td><td style=\"text-align: right;\">3.42264e+07</td><td style=\"text-align: right;\">                   nan</td><td style=\"text-align: right;\">               7405</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">-1.18984e+08</td><td style=\"text-align: right;\">                 304</td><td style=\"text-align: right;\">                  nan</td><td style=\"text-align: right;\">          2  </td><td style=\"text-align: right;\">                2  </td><td style=\"text-align: right;\">                    nan</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">         51239</td><td style=\"text-align: right;\">                       1111</td><td style=\"text-align: right;\">                    nan</td><td style=\"text-align: right;\">1.714e+07  </td><td style=\"text-align: right;\">        97091</td><td style=\"text-align: right;\">                 nan</td></tr>\n",
       "<tr><td style=\"text-align: right;\">   -0.0576</td><td style=\"text-align: right;\">                   nan</td><td style=\"text-align: right;\">              253</td><td style=\"text-align: right;\">                  1385</td><td style=\"text-align: right;\">                      134251</td><td style=\"text-align: right;\">       1950</td><td style=\"text-align: right;\">    3217.06</td><td style=\"text-align: right;\">                          1385</td><td style=\"text-align: right;\">             268502</td><td style=\"text-align: right;\">                 134251</td><td style=\"text-align: right;\">3.41793e+07</td><td style=\"text-align: right;\">                   nan</td><td style=\"text-align: right;\">               6000</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">-1.19169e+08</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                  nan</td><td style=\"text-align: right;\">          1  </td><td style=\"text-align: right;\">                1  </td><td style=\"text-align: right;\">                    nan</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         13150</td><td style=\"text-align: right;\">                       1111</td><td style=\"text-align: right;\">                    nan</td><td style=\"text-align: right;\">1.71674e+07</td><td style=\"text-align: right;\">        97104</td><td style=\"text-align: right;\">                 nan</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows:90275\n",
      "Cols:28\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>       </th><th>logerror            </th><th>regionidneighborhood  </th><th>garagetotalsqft  </th><th>finishedsquarefeet12  </th><th>structuretaxvaluedollarcnt  </th><th>yearbuilt        </th><th>taxamount        </th><th>calculatedfinishedsquarefeet  </th><th>taxvaluedollarcnt  </th><th>landtaxvaluedollarcnt  </th><th>latitude          </th><th>finishedsquarefeet15  </th><th>lotsizesquarefeet  </th><th>propertyzoningdesc  </th><th>bedroomcnt        </th><th>longitude          </th><th>yardbuildingsqft17  </th><th>finishedsquarefeet6  </th><th>bathroomcnt       </th><th>calculatedbathnbr  </th><th>buildingqualitytypeid  </th><th>fullbathcnt       </th><th>regionidcity      </th><th>propertycountylandusecode  </th><th>heatingorsystemtypeid  </th><th>parcelid          </th><th>regionidzip      </th><th>taxdelinquencyyear  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>type   </td><td>real                </td><td>enum                  </td><td>enum             </td><td>int                   </td><td>int                         </td><td>int              </td><td>real             </td><td>int                           </td><td>int                </td><td>int                    </td><td>int               </td><td>enum                  </td><td>real               </td><td>enum                </td><td>int               </td><td>int                </td><td>enum                </td><td>enum                 </td><td>real              </td><td>real               </td><td>int                    </td><td>int               </td><td>int               </td><td>int                        </td><td>int                    </td><td>int               </td><td>int              </td><td>enum                </td></tr>\n",
       "<tr><td>mins   </td><td>-4.605              </td><td>                      </td><td>                 </td><td>2.0                   </td><td>100.0                       </td><td>1885.0           </td><td>49.08            </td><td>2.0                           </td><td>22.0               </td><td>22.0                   </td><td>33339295.0        </td><td>                      </td><td>167.0              </td><td>                    </td><td>0.0               </td><td>-119447865.0       </td><td>                    </td><td>                     </td><td>0.0               </td><td>1.0                </td><td>1.0                    </td><td>1.0               </td><td>3491.0            </td><td>0.0                        </td><td>1.0                    </td><td>10711738.0        </td><td>95982.0          </td><td>                    </td></tr>\n",
       "<tr><td>mean   </td><td>0.011457219606757128</td><td>                      </td><td>                 </td><td>1745.454530585541     </td><td>180093.39374826188          </td><td>1968.532870116959</td><td>5983.975926730108</td><td>1773.1859865645993            </td><td>457672.627356714   </td><td>278335.3250216009      </td><td>34005410.6936915  </td><td>                      </td><td>29110.164176624017 </td><td>                    </td><td>3.0318692882857943</td><td>-118198868.30685131</td><td>                    </td><td>                     </td><td>2.2794738299639983</td><td>2.3092162122725703 </td><td>5.565406875392232      </td><td>2.2412310731482834</td><td>33761.332851071515</td><td>202.69972340281998         </td><td>3.9269793152639085     </td><td>12984656.108712273</td><td>96586.13118351066</td><td>                    </td></tr>\n",
       "<tr><td>maxs   </td><td>4.737               </td><td>                      </td><td>                 </td><td>20013.0               </td><td>9948100.0                   </td><td>2015.0           </td><td>321936.09        </td><td>22741.0                       </td><td>27750000.0         </td><td>24500000.0             </td><td>34816009.0        </td><td>                      </td><td>6971010.0          </td><td>                    </td><td>16.0              </td><td>-117554924.0       </td><td>                    </td><td>                     </td><td>20.0              </td><td>20.0               </td><td>12.0                   </td><td>20.0              </td><td>396556.0          </td><td>8800.0                     </td><td>24.0                   </td><td>162960842.0       </td><td>399675.0         </td><td>                    </td></tr>\n",
       "<tr><td>sigma  </td><td>0.16107883536718667 </td><td>                      </td><td>                 </td><td>909.9411657489275     </td><td>209129.8881686003           </td><td>23.76347471454321</td><td>6838.876956292306</td><td>928.1623927208238             </td><td>554884.3989425416  </td><td>400495.4648926157      </td><td>264965.37555690215</td><td>                      </td><td>121721.30865440045 </td><td>                    </td><td>1.156435519867552 </td><td>360603.19970355305 </td><td>                    </td><td>                     </td><td>1.0042709877071896</td><td>0.9761724992343215 </td><td>1.9006016285969718     </td><td>0.9631416217273225</td><td>46672.3938629122  </td><td>307.471526829543           </td><td>3.68438205053138       </td><td>2504510.488301769 </td><td>3661.339093751153</td><td>                    </td></tr>\n",
       "<tr><td>zeros  </td><td>847                 </td><td>                      </td><td>                 </td><td>0                     </td><td>0                           </td><td>0                </td><td>0                </td><td>0                             </td><td>0                  </td><td>0                      </td><td>0                 </td><td>                      </td><td>0                  </td><td>                    </td><td>1421              </td><td>0                  </td><td>                    </td><td>                     </td><td>1165              </td><td>0                  </td><td>0                      </td><td>0                 </td><td>0                 </td><td>1                          </td><td>0                      </td><td>0                 </td><td>0                </td><td>                    </td></tr>\n",
       "<tr><td>missing</td><td>0                   </td><td>0                     </td><td>0                </td><td>4679                  </td><td>380                         </td><td>756              </td><td>6                </td><td>661                           </td><td>1                  </td><td>1                      </td><td>0                 </td><td>0                     </td><td>10150              </td><td>0                   </td><td>0                 </td><td>0                  </td><td>0                   </td><td>0                    </td><td>0                 </td><td>1182               </td><td>32911                  </td><td>1182              </td><td>1803              </td><td>16160                      </td><td>34195                  </td><td>0                 </td><td>35               </td><td>0                   </td></tr>\n",
       "<tr><td>0      </td><td>0.0953              </td><td>nan                   </td><td>0.0              </td><td>1264.0                </td><td>115087.0                    </td><td>1986.0           </td><td>2015.06          </td><td>1264.0                        </td><td>191811.0           </td><td>76724.0                </td><td>34303597.0        </td><td>nan                   </td><td>1735.0             </td><td>nan                 </td><td>3.0               </td><td>-119287236.0       </td><td>128.0               </td><td>nan                  </td><td>2.5               </td><td>2.5                </td><td>nan                    </td><td>2.0               </td><td>34543.0           </td><td>1128.0                     </td><td>nan                    </td><td>17073783.0        </td><td>97081.0          </td><td>nan                 </td></tr>\n",
       "<tr><td>1      </td><td>0.0198              </td><td>nan                   </td><td>0.0              </td><td>777.0                 </td><td>143809.0                    </td><td>1990.0           </td><td>2581.3           </td><td>777.0                         </td><td>239679.0           </td><td>95870.0                </td><td>34272866.0        </td><td>nan                   </td><td>nan                </td><td>nan                 </td><td>2.0               </td><td>-119198911.0       </td><td>198.0               </td><td>nan                  </td><td>1.0               </td><td>1.0                </td><td>nan                    </td><td>1.0               </td><td>34543.0           </td><td>1129.0                     </td><td>nan                    </td><td>17088994.0        </td><td>97083.0          </td><td>nan                 </td></tr>\n",
       "<tr><td>2      </td><td>0.006               </td><td>nan                   </td><td>441.0            </td><td>1101.0                </td><td>33619.0                     </td><td>1956.0           </td><td>591.64           </td><td>1101.0                        </td><td>47853.0            </td><td>14234.0                </td><td>34340801.0        </td><td>nan                   </td><td>6569.0             </td><td>nan                 </td><td>3.0               </td><td>-119079610.0       </td><td>nan                 </td><td>nan                  </td><td>2.0               </td><td>2.0                </td><td>nan                    </td><td>2.0               </td><td>26965.0           </td><td>1111.0                     </td><td>nan                    </td><td>17100444.0        </td><td>97113.0          </td><td>nan                 </td></tr>\n",
       "<tr><td>3      </td><td>-0.0566             </td><td>nan                   </td><td>460.0            </td><td>1554.0                </td><td>45609.0                     </td><td>1965.0           </td><td>682.78           </td><td>1554.0                        </td><td>62914.0            </td><td>17305.0                </td><td>34354313.0        </td><td>nan                   </td><td>7400.0             </td><td>nan                 </td><td>2.0               </td><td>-119076405.0       </td><td>nan                 </td><td>nan                  </td><td>1.5               </td><td>1.5                </td><td>nan                    </td><td>1.0               </td><td>26965.0           </td><td>1110.0                     </td><td>nan                    </td><td>17102429.0        </td><td>97113.0          </td><td>nan                 </td></tr>\n",
       "<tr><td>4      </td><td>0.0573              </td><td>nan                   </td><td>665.0            </td><td>2415.0                </td><td>277000.0                    </td><td>1984.0           </td><td>5886.92          </td><td>2415.0                        </td><td>554000.0           </td><td>277000.0               </td><td>34266578.0        </td><td>nan                   </td><td>6326.0             </td><td>nan                 </td><td>4.0               </td><td>-119165392.0       </td><td>nan                 </td><td>nan                  </td><td>2.5               </td><td>2.5                </td><td>nan                    </td><td>2.0               </td><td>34543.0           </td><td>1111.0                     </td><td>nan                    </td><td>17109604.0        </td><td>97084.0          </td><td>nan                 </td></tr>\n",
       "<tr><td>5      </td><td>0.0564              </td><td>nan                   </td><td>473.0            </td><td>2882.0                </td><td>222070.0                    </td><td>1980.0           </td><td>3110.44          </td><td>2882.0                        </td><td>289609.0           </td><td>67539.0                </td><td>34240014.0        </td><td>nan                   </td><td>10000.0            </td><td>nan                 </td><td>4.0               </td><td>-119024793.0       </td><td>nan                 </td><td>nan                  </td><td>2.5               </td><td>2.5                </td><td>nan                    </td><td>2.0               </td><td>51239.0           </td><td>1111.0                     </td><td>nan                    </td><td>17125829.0        </td><td>97089.0          </td><td>nan                 </td></tr>\n",
       "<tr><td>6      </td><td>0.0315              </td><td>nan                   </td><td>467.0            </td><td>1772.0                </td><td>185000.0                    </td><td>1978.0           </td><td>5632.2           </td><td>1772.0                        </td><td>526000.0           </td><td>341000.0               </td><td>34226842.0        </td><td>nan                   </td><td>8059.0             </td><td>nan                 </td><td>3.0               </td><td>-119059815.0       </td><td>1045.0              </td><td>nan                  </td><td>2.0               </td><td>2.0                </td><td>nan                    </td><td>2.0               </td><td>51239.0           </td><td>1111.0                     </td><td>nan                    </td><td>17132911.0        </td><td>97089.0          </td><td>nan                 </td></tr>\n",
       "<tr><td>7      </td><td>0.0257              </td><td>nan                   </td><td>440.0            </td><td>2632.0                </td><td>342611.0                    </td><td>1971.0           </td><td>6109.94          </td><td>2632.0                        </td><td>571086.0           </td><td>228475.0               </td><td>34229816.0        </td><td>nan                   </td><td>7602.0             </td><td>nan                 </td><td>5.0               </td><td>-119050224.0       </td><td>180.0               </td><td>nan                  </td><td>2.5               </td><td>2.5                </td><td>nan                    </td><td>2.0               </td><td>51239.0           </td><td>1111.0                     </td><td>nan                    </td><td>17134926.0        </td><td>97089.0          </td><td>nan                 </td></tr>\n",
       "<tr><td>8      </td><td>0.002               </td><td>nan                   </td><td>494.0            </td><td>1292.0                </td><td>231297.0                    </td><td>1979.0           </td><td>5026.4           </td><td>1292.0                        </td><td>462594.0           </td><td>231297.0               </td><td>34226351.0        </td><td>nan                   </td><td>7405.0             </td><td>nan                 </td><td>3.0               </td><td>-118983853.0       </td><td>304.0               </td><td>nan                  </td><td>2.0               </td><td>2.0                </td><td>nan                    </td><td>2.0               </td><td>51239.0           </td><td>1111.0                     </td><td>nan                    </td><td>17139988.0        </td><td>97091.0          </td><td>nan                 </td></tr>\n",
       "<tr><td>9      </td><td>-0.0576             </td><td>nan                   </td><td>253.0            </td><td>1385.0                </td><td>134251.0                    </td><td>1950.0           </td><td>3217.06          </td><td>1385.0                        </td><td>268502.0           </td><td>134251.0               </td><td>34179289.0        </td><td>nan                   </td><td>6000.0             </td><td>nan                 </td><td>3.0               </td><td>-119169287.0       </td><td>nan                 </td><td>nan                  </td><td>1.0               </td><td>1.0                </td><td>nan                    </td><td>1.0               </td><td>13150.0           </td><td>1111.0                     </td><td>nan                    </td><td>17167359.0        </td><td>97104.0          </td><td>nan                 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First convert to a pandas df, then to a numpy array\n",
    "logerror_array = hf['logerror'].as_data_frame().as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAFyCAYAAADYhIJtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show the graph of logerror \n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(range(logerror_array.shape[0]), np.sort(logerror_array))\n",
    "plt.xlabel('index', fontsize=12)\n",
    "plt.ylabel('logerror', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011457219606757128\n"
     ]
    }
   ],
   "source": [
    "logerror_mean=hf['logerror'].mean()[0]\n",
    "print(logerror_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hf['logerror_highlow'] = (hf['logerror'] >= logerror_mean).ifelse(\"1\", \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  logerror_highlow</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">                 1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">                 1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">                 0</td></tr>\n",
       "<tr><td style=\"text-align: right;\">                 0</td></tr>\n",
       "<tr><td style=\"text-align: right;\">                 1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">                 1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">                 1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">                 1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">                 0</td></tr>\n",
       "<tr><td style=\"text-align: right;\">                 0</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf['logerror_highlow'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  logerror_highlow</th><th style=\"text-align: right;\">  Count</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">                 0</td><td style=\"text-align: right;\">  50087</td></tr>\n",
       "<tr><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">  40188</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf['logerror_highlow'].table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.605, 4.737]\n"
     ]
    }
   ],
   "source": [
    "logerror_range=[hf['logerror'].min(),hf['logerror'].max()]\n",
    "print(logerror_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16107883536718667\n"
     ]
    }
   ],
   "source": [
    "logerror_sd=hf['logerror'].sd()[0]\n",
    "print(logerror_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5607199999999999\n"
     ]
    }
   ],
   "source": [
    "top_by_pct=(0.66*(logerror_range[1]-logerror_range[0]))+logerror_range[0]\n",
    "print(top_by_pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.5221400000000003\n"
     ]
    }
   ],
   "source": [
    "bottom_by_pct=(0.33*(logerror_range[1]-logerror_range[0]))+logerror_range[0]\n",
    "print(bottom_by_pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hf['logerror_three']=(hf['logerror'] <= (logerror_mean-logerror_sd)).ifelse('0','1')\n",
    "hf['logerror_three'] = (hf['logerror'] >= (logerror_mean+logerror_sd)).ifelse('2', hf['logerror_three'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  logerror_three</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">               1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">               1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">               1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">               1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">               1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">               1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">               1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">               1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">               1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">               1</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf['logerror_three'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  logerror_three</th><th style=\"text-align: right;\">  Count</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">               0</td><td style=\"text-align: right;\">   3616</td></tr>\n",
       "<tr><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">  82367</td></tr>\n",
       "<tr><td style=\"text-align: right;\">               2</td><td style=\"text-align: right;\">   4292</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf['logerror_three'].table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, valid = hf.split_frame([0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_independent_variables(df, l):\n",
    "    C = [name for name in df.columns if name not in l]\n",
    "    # determine column types\n",
    "    print (C) \n",
    "    print (df.columns)     \n",
    "    ints, reals, enums = [], [], []\n",
    "    for key, val in df.types.items():\n",
    "        if key in C:\n",
    "            if val == 'enum':\n",
    "                enums.append(key)\n",
    "            elif val == 'int':\n",
    "                ints.append(key)            \n",
    "            else: \n",
    "                reals.append(key)    \n",
    "    x=ints+enums+reals\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['regionidneighborhood', 'garagetotalsqft', 'finishedsquarefeet12', 'structuretaxvaluedollarcnt', 'yearbuilt', 'taxamount', 'calculatedfinishedsquarefeet', 'taxvaluedollarcnt', 'landtaxvaluedollarcnt', 'latitude', 'finishedsquarefeet15', 'lotsizesquarefeet', 'propertyzoningdesc', 'bedroomcnt', 'longitude', 'yardbuildingsqft17', 'finishedsquarefeet6', 'bathroomcnt', 'calculatedbathnbr', 'buildingqualitytypeid', 'fullbathcnt', 'regionidcity', 'propertycountylandusecode', 'heatingorsystemtypeid', 'parcelid', 'regionidzip', 'taxdelinquencyyear', 'logerror_highlow', 'logerror_three']\n",
      "['logerror', 'regionidneighborhood', 'garagetotalsqft', 'finishedsquarefeet12', 'structuretaxvaluedollarcnt', 'yearbuilt', 'taxamount', 'calculatedfinishedsquarefeet', 'taxvaluedollarcnt', 'landtaxvaluedollarcnt', 'latitude', 'finishedsquarefeet15', 'lotsizesquarefeet', 'propertyzoningdesc', 'bedroomcnt', 'longitude', 'yardbuildingsqft17', 'finishedsquarefeet6', 'bathroomcnt', 'calculatedbathnbr', 'buildingqualitytypeid', 'fullbathcnt', 'regionidcity', 'propertycountylandusecode', 'heatingorsystemtypeid', 'parcelid', 'regionidzip', 'taxdelinquencyyear', 'logerror_highlow', 'logerror_three']\n",
      "['finishedsquarefeet12', 'structuretaxvaluedollarcnt', 'yearbuilt', 'calculatedfinishedsquarefeet', 'taxvaluedollarcnt', 'landtaxvaluedollarcnt', 'latitude', 'bedroomcnt', 'longitude', 'buildingqualitytypeid', 'fullbathcnt', 'regionidcity', 'propertycountylandusecode', 'heatingorsystemtypeid', 'parcelid', 'regionidzip', 'regionidneighborhood', 'garagetotalsqft', 'finishedsquarefeet15', 'propertyzoningdesc', 'yardbuildingsqft17', 'finishedsquarefeet6', 'taxdelinquencyyear', 'logerror_highlow', 'logerror_three', 'taxamount', 'lotsizesquarefeet', 'bathroomcnt', 'calculatedbathnbr']\n",
      "29\n",
      "logerror\n"
     ]
    }
   ],
   "source": [
    "y=target\n",
    "X=get_independent_variables(train,y)\n",
    "print(X)\n",
    "print(len(X))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLM Parameters\n",
    "\n",
    "\n",
    "_Model parameters_ \n",
    "\n",
    "**Family**\n",
    "The probability distribution of the dependent variable. For regression the choices are gaussian, poisson,gamma, and tweedie.\n",
    "\n",
    "for logistic regression (i.e., binomial classification) you must set this to binomial, and for multinomial regression you must set this to multinomial.\n",
    "\n",
    "Family defines how the deviance metric is calculated; it is only the same as MSE when the family is gaussian.\n",
    "\n",
    "\n",
    "**Link**\n",
    "\n",
    "Link defines the error model.  \n",
    "\n",
    "Can be one of “family_default,” “identity,” “logit,” “log,” “inverse,” or “tweedie.”\n",
    "\n",
    "As the name suggests, “family_default” is the default, and is usually best.  For example, when the dependent variable is gaussian the “family_default” is “identity“ and this is simple linear regression.\n",
    "\n",
    "\n",
    "See [Simple Linear Regression and Confidence Intervals (13:01)](https://www.youtube.com/watch?v=PsE9UqoWtS4)   \n",
    "and\n",
    "\n",
    "[Multiple Linear Regression and Interpreting Regression Coefficients (15:38)](https://www.youtube.com/watch?v=1hbCJyM9ccs)\n",
    "\n",
    "and\n",
    "\n",
    "[Model Selection and Qualitative Predictors (14:51)](https://www.youtube.com/watch?v=3T6RXmIHbJ4)\n",
    "\n",
    "and\n",
    "\n",
    "[Logistic Regression and Maximum Likelihood (9:07)](https://www.youtube.com/watch?v=31Q5FGRnxt4)  \n",
    "\n",
    "_Regularization parameters_  \n",
    "\n",
    "\n",
    "The other parameters are about regularization. There are two types: L1 (also called lasso regularization or lasso regression) and L2 (i.e. ridge regression).   \n",
    "\n",
    "\n",
    "\n",
    "L1 regularization will set some of your coefficients to zero (this can be useful to simplify a problem when you have a lot of predictor columns but don’t know which ones are important), whereas L2 regularization\n",
    "tries to keep all the coefficients close to zero, but nonzero, stopping any single coefficient from dominating. If your data is dense (meaning all columns are likely to explain something about the response variable), L2 regularization is likely to be better than L1.\n",
    "\n",
    "You choose lasso regression by setting alpha to 1.0, and you choose ridge regression by setting alpha to 0.0.  \n",
    "\n",
    "Or you can choose elastic net, which is “have your cake and\n",
    "eat it”: you set alpha between 0.0 and 1.0 to mix them together. The other parameter is lambda. While alpha decides what type of regularization to use, lambda decides how strong to make it.\n",
    "\n",
    "See [Shrinkage Methods and Ridge Regression (12:37)](https://www.youtube.com/watch?v=cSKzqb0EKS0)\n",
    "\n",
    "**alpha**\n",
    "\n",
    "Described earlier, it is how much L1 regularization, and 1-alpha is how much L2 regularization. The default is 0.5.\n",
    "\n",
    "**lambda**\n",
    "\n",
    "Regularization strength. The default is chosen based on lambda max (described under lambda_search) but, as already mentioned, it is often useful to try lambda search to automatically find a good value. If you want to explicitly choose values for lambda that lambda search should try, specify them as a list here.\n",
    "\n",
    "\n",
    "**lambda_search**\n",
    "\n",
    "If true, then it will try multiple values of lambda for you. It starts with the maximum value of lambda, which is a lambda value such that the regularization causes all coefficients to end up as zero. It then keeps reducing the lambda value until the minimum value (which is decided by lambda_min_ratio). Note that when setting lambda_search to true you would never also set lambda to a single value. Normally you would not set lambda at all, but if you wanted to explicitly\n",
    "give 2+ values for lambda\n",
    "\n",
    "**lambda_min_ratio**\n",
    "\n",
    "Minimum lambda used in lambda search, specified as a percentage of the starting (maximum) lambda. Defaults to 0.0001. For example, if lambda search chooses 15 as the starting value, then 0.0015 is the final lambda value it will try. Lambda search will never try a lambda of zero. If you suspect it may be best (e.g., the best value from lambda search was the final and smallest one), you will need to try it separately.\n",
    "\n",
    "**nlambdas**\n",
    "\n",
    "\n",
    "Number of lambdas to be used in a search. The default of –1 normally means it will try 100 lambdas but when doing ridge regression, i.e., alpha=0.0, it instead defaults to 30 lambdas. Consider setting this to a lower number if you need to speed things up.\n",
    "\n",
    "\n",
    "**max_active_predictors**  \n",
    "\n",
    "The default of –1 means no restriction, but you can set it to have lambda_search stop early, once it has reached this number of nonzero coefficients.\n",
    "\n",
    "\n",
    "_Solvers_  \n",
    "\n",
    "**AUTO**\n",
    "\n",
    "AUTO will set the solver based on given data and the other parameters.  \n",
    "\n",
    "**IRLSM**\n",
    "\n",
    "This stands for Iterative Re-weighted Least Squares Method. If you have only a relatively small number of columns it is usually the best choice. \n",
    "\n",
    "**L_BFGS**\n",
    "\n",
    "The L is for limited memory, and the BFGS is for the Broyden–Fletcher–Goldfarb– Shanno algorithm. \n",
    "\n",
    "_Solver parameters_  \n",
    "\n",
    "\n",
    "**max_iterations**\n",
    "\n",
    "This controls how much work the solver will do. It defaults to 50.\n",
    "\n",
    "**beta_epsilon**  \n",
    "\n",
    "This is for the IRLSM solver: if the beta changes less than this, then stop.\n",
    "\n",
    "\n",
    "**gradient_epsilon**\n",
    "\n",
    "This is for the L-BFGS solver: if the objective changes less than this, then stop.\n",
    "\n",
    "**objective_epsilon**\n",
    "\n",
    "Stop when the objective value changes less than this.\n",
    "\n",
    "\n",
    "_Data parameters_  \n",
    "\n",
    "\n",
    "**non_negative**\n",
    "\n",
    "Restrict coefficients (not intercept) to be nonnegative.\n",
    "\n",
    "**remove_collinear_columns**\n",
    "\n",
    "Remove some of the columns, if any are linearly dependent. False by default. There is no need to set this if using regularization.  \n",
    "\n",
    "\n",
    "**standardize**\n",
    "Standardize numeric columns to have zero mean and unit variance. This is true by default. You are unlikely to need to set it to false, but always make sure it is true if using regularization (i.e., lambda > 0).\n",
    "\n",
    "**missing_values_handling**\n",
    "\n",
    "The default is “MeanImputation,” which gives them the mean value from that\n",
    "column. The alternative is “Skip” to ignore rows with missing values.\n",
    "\n",
    "\n",
    "**interactions**\n",
    "\n",
    "This is a list of columns you want to interact. For example, if you give three columns, such as A, B, and C, then AB, AC, and BC will be added to your model.\n",
    "\n",
    "See [Interactions and Nonlinearity (14:16)](https://www.youtube.com/watch?v=IFzVxLv0TKQ)\n",
    "\n",
    "\n",
    "**intercept**\n",
    "\n",
    "Whether to include a constant term in the model. The default is true, so setting it to false effectively means force an intercept of zero.   \n",
    "\n",
    "**obj_reg**\n",
    "\n",
    "If you want to modify the objective function. The default of –1 means it will use 1 divided by the number of rows in your training data set.   \n",
    "\n",
    "**prior**\n",
    "\n",
    "Used for binomial classification, and it is the prior probability for the first of the two classes. Being a probability, it must be between 0.0 and 1.0.   \n",
    "\n",
    "**compute_p_values**\n",
    "\n",
    "Set this to true to have the p-values for each coefficient.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class H2OGeneralizedLinearEstimator in module h2o.estimators.glm:\n",
      "\n",
      "class H2OGeneralizedLinearEstimator(h2o.estimators.estimator_base.H2OEstimator)\n",
      " |  Generalized Linear Modeling\n",
      " |  \n",
      " |  Fits a generalized linear model, specified by a response variable, a set of predictors, and a\n",
      " |  description of the error distribution.\n",
      " |  \n",
      " |  A subclass of :class:`ModelBase` is returned. The specific subclass depends on the machine learning task\n",
      " |  at hand (if it's binomial classification, then an H2OBinomialModel is returned, if it's regression then a\n",
      " |  H2ORegressionModel is returned). The default print-out of the models is shown, but further GLM-specific\n",
      " |  information can be queried out of the object. Upon completion of the GLM, the resulting object has\n",
      " |  coefficients, normalized coefficients, residual/null deviance, aic, and a host of model metrics including\n",
      " |  MSE, AUC (for logistic regression), degrees of freedom, and confusion matrices.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      H2OGeneralizedLinearEstimator\n",
      " |      h2o.estimators.estimator_base.H2OEstimator\n",
      " |      h2o.model.model_base.ModelBase\n",
      " |      h2o.utils.backward_compatibility.BackwardsCompatibleBase\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, **kwargs)\n",
      " |      Construct a new model instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  getGLMRegularizationPath(model)\n",
      " |      Extract full regularization path explored during lambda search from glm model.\n",
      " |      \n",
      " |      :param model: source lambda search model\n",
      " |  \n",
      " |  makeGLMModel(model, coefs, threshold=0.5)\n",
      " |      Create a custom GLM model using the given coefficients.\n",
      " |      \n",
      " |      Needs to be passed source model trained on the dataset to extract the dataset information from.\n",
      " |      \n",
      " |      :param model: source model, used for extracting dataset information\n",
      " |      :param coefs: dictionary containing model coefficients\n",
      " |      :param threshold: (optional, only for binomial) decision threshold used for classification\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  Lambda\n",
      " |      DEPRECATED. Use ``self.lambda_`` instead\n",
      " |  \n",
      " |  alpha\n",
      " |      Distribution of regularization between the L1 (Lasso) and L2 (Ridge) penalties. A value of 1 for alpha\n",
      " |      represents Lasso regression, a value of 0 produces Ridge regression, and anything in between specifies the\n",
      " |      amount of mixing between the two. Default value of alpha is 0 when SOLVER = 'L-BFGS'; 0.5 otherwise.\n",
      " |      \n",
      " |      Type: ``List[float]``.\n",
      " |  \n",
      " |  balance_classes\n",
      " |      Balance training data class counts via over/under-sampling (for imbalanced data).\n",
      " |      \n",
      " |      Type: ``bool``  (default: ``False``).\n",
      " |  \n",
      " |  beta_constraints\n",
      " |      Beta constraints\n",
      " |      \n",
      " |      Type: ``H2OFrame``.\n",
      " |  \n",
      " |  beta_epsilon\n",
      " |      Converge if  beta changes less (using L-infinity norm) than beta esilon, ONLY applies to IRLSM solver\n",
      " |      \n",
      " |      Type: ``float``  (default: ``0.0001``).\n",
      " |  \n",
      " |  class_sampling_factors\n",
      " |      Desired over/under-sampling ratios per class (in lexicographic order). If not specified, sampling factors will\n",
      " |      be automatically computed to obtain class balance during training. Requires balance_classes.\n",
      " |      \n",
      " |      Type: ``List[float]``.\n",
      " |  \n",
      " |  compute_p_values\n",
      " |      Request p-values computation, p-values work only with IRLSM solver and no regularization\n",
      " |      \n",
      " |      Type: ``bool``  (default: ``False``).\n",
      " |  \n",
      " |  custom_metric_func\n",
      " |      Reference to custom evaluation function, format: `language:keyName=funcName`\n",
      " |      \n",
      " |      Type: ``str``.\n",
      " |  \n",
      " |  early_stopping\n",
      " |      Stop early when there is no more relative improvement on train or validation (if provided)\n",
      " |      \n",
      " |      Type: ``bool``  (default: ``True``).\n",
      " |  \n",
      " |  export_checkpoints_dir\n",
      " |      Automatically export generated models to this directory.\n",
      " |      \n",
      " |      Type: ``str``.\n",
      " |  \n",
      " |  family\n",
      " |      Family. Use binomial for classification with logistic regression, others are for regression problems.\n",
      " |      \n",
      " |      One of: ``\"gaussian\"``, ``\"binomial\"``, ``\"quasibinomial\"``, ``\"ordinal\"``, ``\"multinomial\"``, ``\"poisson\"``,\n",
      " |      ``\"gamma\"``, ``\"tweedie\"``  (default: ``\"gaussian\"``).\n",
      " |  \n",
      " |  fold_assignment\n",
      " |      Cross-validation fold assignment scheme, if fold_column is not specified. The 'Stratified' option will stratify\n",
      " |      the folds based on the response variable, for classification problems.\n",
      " |      \n",
      " |      One of: ``\"auto\"``, ``\"random\"``, ``\"modulo\"``, ``\"stratified\"``  (default: ``\"auto\"``).\n",
      " |  \n",
      " |  fold_column\n",
      " |      Column with cross-validation fold index assignment per observation.\n",
      " |      \n",
      " |      Type: ``str``.\n",
      " |  \n",
      " |  gradient_epsilon\n",
      " |      Converge if  objective changes less (using L-infinity norm) than this, ONLY applies to L-BFGS solver. Default\n",
      " |      indicates: If lambda_search is set to False and lambda is equal to zero, the default value of gradient_epsilon\n",
      " |      is equal to .000001, otherwise the default value is .0001. If lambda_search is set to True, the conditional\n",
      " |      values above are 1E-8 and 1E-6 respectively.\n",
      " |      \n",
      " |      Type: ``float``  (default: ``-1``).\n",
      " |  \n",
      " |  ignore_const_cols\n",
      " |      Ignore constant columns.\n",
      " |      \n",
      " |      Type: ``bool``  (default: ``True``).\n",
      " |  \n",
      " |  ignored_columns\n",
      " |      Names of columns to ignore for training.\n",
      " |      \n",
      " |      Type: ``List[str]``.\n",
      " |  \n",
      " |  interaction_pairs\n",
      " |      A list of pairwise (first order) column interactions.\n",
      " |      \n",
      " |      Type: ``List[tuple]``.\n",
      " |  \n",
      " |  interactions\n",
      " |      A list of predictor column indices to interact. All pairwise combinations will be computed for the list.\n",
      " |      \n",
      " |      Type: ``List[str]``.\n",
      " |  \n",
      " |  intercept\n",
      " |      Include constant term in the model\n",
      " |      \n",
      " |      Type: ``bool``  (default: ``True``).\n",
      " |  \n",
      " |  keep_cross_validation_fold_assignment\n",
      " |      Whether to keep the cross-validation fold assignment.\n",
      " |      \n",
      " |      Type: ``bool``  (default: ``False``).\n",
      " |  \n",
      " |  keep_cross_validation_models\n",
      " |      Whether to keep the cross-validation models.\n",
      " |      \n",
      " |      Type: ``bool``  (default: ``True``).\n",
      " |  \n",
      " |  keep_cross_validation_predictions\n",
      " |      Whether to keep the predictions of the cross-validation models.\n",
      " |      \n",
      " |      Type: ``bool``  (default: ``False``).\n",
      " |  \n",
      " |  lambda_\n",
      " |      Regularization strength\n",
      " |      \n",
      " |      Type: ``List[float]``.\n",
      " |  \n",
      " |  lambda_min_ratio\n",
      " |      Minimum lambda used in lambda search, specified as a ratio of lambda_max (the smallest lambda that drives all\n",
      " |      coefficients to zero). Default indicates: if the number of observations is greater than the number of variables,\n",
      " |      then lambda_min_ratio is set to 0.0001; if the number of observations is less than the number of variables, then\n",
      " |      lambda_min_ratio is set to 0.01.\n",
      " |      \n",
      " |      Type: ``float``  (default: ``-1``).\n",
      " |  \n",
      " |  lambda_search\n",
      " |      Use lambda search starting at lambda max, given lambda is then interpreted as lambda min\n",
      " |      \n",
      " |      Type: ``bool``  (default: ``False``).\n",
      " |  \n",
      " |  link\n",
      " |      One of: ``\"family_default\"``, ``\"identity\"``, ``\"logit\"``, ``\"log\"``, ``\"inverse\"``, ``\"tweedie\"``,\n",
      " |      ``\"ologit\"``, ``\"oprobit\"``, ``\"ologlog\"``  (default: ``\"family_default\"``).\n",
      " |  \n",
      " |  max_active_predictors\n",
      " |      Maximum number of active predictors during computation. Use as a stopping criterion to prevent expensive model\n",
      " |      building with many predictors. Default indicates: If the IRLSM solver is used, the value of\n",
      " |      max_active_predictors is set to 5000 otherwise it is set to 100000000.\n",
      " |      \n",
      " |      Type: ``int``  (default: ``-1``).\n",
      " |  \n",
      " |  max_after_balance_size\n",
      " |      Maximum relative size of the training data after balancing class counts (can be less than 1.0). Requires\n",
      " |      balance_classes.\n",
      " |      \n",
      " |      Type: ``float``  (default: ``5``).\n",
      " |  \n",
      " |  max_confusion_matrix_size\n",
      " |      [Deprecated] Maximum size (# classes) for confusion matrices to be printed in the Logs\n",
      " |      \n",
      " |      Type: ``int``  (default: ``20``).\n",
      " |  \n",
      " |  max_hit_ratio_k\n",
      " |      Maximum number (top K) of predictions to use for hit ratio computation (for multi-class only, 0 to disable)\n",
      " |      \n",
      " |      Type: ``int``  (default: ``0``).\n",
      " |  \n",
      " |  max_iterations\n",
      " |      Maximum number of iterations\n",
      " |      \n",
      " |      Type: ``int``  (default: ``-1``).\n",
      " |  \n",
      " |  max_runtime_secs\n",
      " |      Maximum allowed runtime in seconds for model training. Use 0 to disable.\n",
      " |      \n",
      " |      Type: ``float``  (default: ``0``).\n",
      " |  \n",
      " |  missing_values_handling\n",
      " |      Handling of missing values. Either MeanImputation or Skip.\n",
      " |      \n",
      " |      One of: ``\"mean_imputation\"``, ``\"skip\"``  (default: ``\"mean_imputation\"``).\n",
      " |  \n",
      " |  nfolds\n",
      " |      Number of folds for K-fold cross-validation (0 to disable or >= 2).\n",
      " |      \n",
      " |      Type: ``int``  (default: ``0``).\n",
      " |  \n",
      " |  nlambdas\n",
      " |      Number of lambdas to be used in a search. Default indicates: If alpha is zero, with lambda search set to True,\n",
      " |      the value of nlamdas is set to 30 (fewer lambdas are needed for ridge regression) otherwise it is set to 100.\n",
      " |      \n",
      " |      Type: ``int``  (default: ``-1``).\n",
      " |  \n",
      " |  non_negative\n",
      " |      Restrict coefficients (not intercept) to be non-negative\n",
      " |      \n",
      " |      Type: ``bool``  (default: ``False``).\n",
      " |  \n",
      " |  obj_reg\n",
      " |      Likelihood divider in objective value computation, default is 1/nobs\n",
      " |      \n",
      " |      Type: ``float``  (default: ``-1``).\n",
      " |  \n",
      " |  objective_epsilon\n",
      " |      Converge if  objective value changes less than this. Default indicates: If lambda_search is set to True the\n",
      " |      value of objective_epsilon is set to .0001. If the lambda_search is set to False and lambda is equal to zero,\n",
      " |      the value of objective_epsilon is set to .000001, for any other value of lambda the default value of\n",
      " |      objective_epsilon is set to .0001.\n",
      " |      \n",
      " |      Type: ``float``  (default: ``-1``).\n",
      " |  \n",
      " |  offset_column\n",
      " |      Offset column. This will be added to the combination of columns before applying the link function.\n",
      " |      \n",
      " |      Type: ``str``.\n",
      " |  \n",
      " |  prior\n",
      " |      Prior probability for y==1. To be used only for logistic regression iff the data has been sampled and the mean\n",
      " |      of response does not reflect reality.\n",
      " |      \n",
      " |      Type: ``float``  (default: ``-1``).\n",
      " |  \n",
      " |  remove_collinear_columns\n",
      " |      In case of linearly dependent columns, remove some of the dependent columns\n",
      " |      \n",
      " |      Type: ``bool``  (default: ``False``).\n",
      " |  \n",
      " |  response_column\n",
      " |      Response variable column.\n",
      " |      \n",
      " |      Type: ``str``.\n",
      " |  \n",
      " |  score_each_iteration\n",
      " |      Whether to score during each iteration of model training.\n",
      " |      \n",
      " |      Type: ``bool``  (default: ``False``).\n",
      " |  \n",
      " |  seed\n",
      " |      Seed for pseudo random number generator (if applicable)\n",
      " |      \n",
      " |      Type: ``int``  (default: ``-1``).\n",
      " |  \n",
      " |  solver\n",
      " |      AUTO will set the solver based on given data and the other parameters. IRLSM is fast on on problems with small\n",
      " |      number of predictors and for lambda-search with L1 penalty, L_BFGS scales better for datasets with many columns.\n",
      " |      \n",
      " |      One of: ``\"auto\"``, ``\"irlsm\"``, ``\"l_bfgs\"``, ``\"coordinate_descent_naive\"``, ``\"coordinate_descent\"``,\n",
      " |      ``\"gradient_descent_lh\"``, ``\"gradient_descent_sqerr\"``  (default: ``\"auto\"``).\n",
      " |  \n",
      " |  standardize\n",
      " |      Standardize numeric columns to have zero mean and unit variance\n",
      " |      \n",
      " |      Type: ``bool``  (default: ``True``).\n",
      " |  \n",
      " |  training_frame\n",
      " |      Id of the training data frame.\n",
      " |      \n",
      " |      Type: ``H2OFrame``.\n",
      " |  \n",
      " |  tweedie_link_power\n",
      " |      Tweedie link power\n",
      " |      \n",
      " |      Type: ``float``  (default: ``1``).\n",
      " |  \n",
      " |  tweedie_variance_power\n",
      " |      Tweedie variance power\n",
      " |      \n",
      " |      Type: ``float``  (default: ``0``).\n",
      " |  \n",
      " |  validation_frame\n",
      " |      Id of the validation data frame.\n",
      " |      \n",
      " |      Type: ``H2OFrame``.\n",
      " |  \n",
      " |  weights_column\n",
      " |      Column with observation weights. Giving some observation a weight of zero is equivalent to excluding it from the\n",
      " |      dataset; giving an observation a relative weight of 2 is equivalent to repeating that row twice. Negative\n",
      " |      weights are not allowed. Note: Weights are per-row observation weights and do not increase the size of the data\n",
      " |      frame. This is typically the number of times a row is repeated, but non-integer values are supported as well.\n",
      " |      During training, rows with higher weights matter more, due to the larger loss function pre-factor.\n",
      " |      \n",
      " |      Type: ``str``.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  algo = 'glm'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from h2o.estimators.estimator_base.H2OEstimator:\n",
      " |  \n",
      " |  convert_H2OXGBoostParams_2_XGBoostParams(self)\n",
      " |      In order to use convert_H2OXGBoostParams_2_XGBoostParams and convert_H2OFrame_2_DMatrix, you must import\n",
      " |      the following toolboxes: xgboost, pandas, numpy and scipy.sparse.\n",
      " |      \n",
      " |      Given an H2OXGBoost model, this method will generate the corresponding parameters that should be used by\n",
      " |      native XGBoost in order to give exactly the same result, assuming that the same dataset\n",
      " |      (derived from h2oFrame) is used to train the native XGBoost model.\n",
      " |      \n",
      " |      Follow the steps below to compare H2OXGBoost and native XGBoost:\n",
      " |      \n",
      " |      1. Train the H2OXGBoost model with H2OFrame trainFile and generate a prediction:\n",
      " |      h2oModelD = H2OXGBoostEstimator(**h2oParamsD) # parameters specified as a dict()\n",
      " |      h2oModelD.train(x=myX, y=y, training_frame=trainFile) # train with H2OFrame trainFile\n",
      " |      h2oPredict = h2oPredictD = h2oModelD.predict(trainFile)\n",
      " |      \n",
      " |      2. Derive the DMatrix from H2OFrame:\n",
      " |      nativeDMatrix = trainFile.convert_H2OFrame_2_DMatrix(myX, y, h2oModelD)\n",
      " |      \n",
      " |      3. Derive the parameters for native XGBoost:\n",
      " |      nativeParams = h2oModelD.convert_H2OXGBoostParams_2_XGBoostParams()\n",
      " |      \n",
      " |      4. Train your native XGBoost model and generate a prediction:\n",
      " |      nativeModel = xgb.train(params=nativeParams[0], dtrain=nativeDMatrix, num_boost_round=nativeParams[1])\n",
      " |      nativePredict = nativeModel.predict(data=nativeDMatrix, ntree_limit=nativeParams[1].\n",
      " |      \n",
      " |      5. Compare the predictions h2oPredict from H2OXGBoost, nativePredict from native XGBoost.\n",
      " |      \n",
      " |      :return: nativeParams, num_boost_round\n",
      " |  \n",
      " |  fit(self, X, y=None, **params)\n",
      " |      Fit an H2O model as part of a scikit-learn pipeline or grid search.\n",
      " |      \n",
      " |      A warning will be issued if a caller other than sklearn attempts to use this method.\n",
      " |      \n",
      " |      :param H2OFrame X: An H2OFrame consisting of the predictor variables.\n",
      " |      :param H2OFrame y: An H2OFrame consisting of the response variable.\n",
      " |      :param params: Extra arguments.\n",
      " |      :returns: The current instance of H2OEstimator for method chaining.\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Obtain parameters for this estimator.\n",
      " |      \n",
      " |      Used primarily for sklearn Pipelines and sklearn grid search.\n",
      " |      \n",
      " |      :param deep: If True, return parameters of all sub-objects that are estimators.\n",
      " |      \n",
      " |      :returns: A dict of parameters\n",
      " |  \n",
      " |  join(self)\n",
      " |      Wait until job's completion.\n",
      " |  \n",
      " |  set_params(self, **parms)\n",
      " |      Used by sklearn for updating parameters during grid search.\n",
      " |      \n",
      " |      :param parms: A dictionary of parameters that will be set on this model.\n",
      " |      :returns: self, the current estimator object with the parameters all set as desired.\n",
      " |  \n",
      " |  start(self, x, y=None, training_frame=None, offset_column=None, fold_column=None, weights_column=None, validation_frame=None, **params)\n",
      " |      Train the model asynchronously (to block for results call :meth:`join`).\n",
      " |      \n",
      " |      :param x: A list of column names or indices indicating the predictor columns.\n",
      " |      :param y: An index or a column name indicating the response column.\n",
      " |      :param H2OFrame training_frame: The H2OFrame having the columns indicated by x and y (as well as any\n",
      " |          additional columns specified by fold, offset, and weights).\n",
      " |      :param offset_column: The name or index of the column in training_frame that holds the offsets.\n",
      " |      :param fold_column: The name or index of the column in training_frame that holds the per-row fold\n",
      " |          assignments.\n",
      " |      :param weights_column: The name or index of the column in training_frame that holds the per-row weights.\n",
      " |      :param validation_frame: H2OFrame with validation data to be scored on while training.\n",
      " |  \n",
      " |  train(self, x=None, y=None, training_frame=None, offset_column=None, fold_column=None, weights_column=None, validation_frame=None, max_runtime_secs=None, ignored_columns=None, model_id=None, verbose=False)\n",
      " |      Train the H2O model.\n",
      " |      \n",
      " |      :param x: A list of column names or indices indicating the predictor columns.\n",
      " |      :param y: An index or a column name indicating the response column.\n",
      " |      :param H2OFrame training_frame: The H2OFrame having the columns indicated by x and y (as well as any\n",
      " |          additional columns specified by fold, offset, and weights).\n",
      " |      :param offset_column: The name or index of the column in training_frame that holds the offsets.\n",
      " |      :param fold_column: The name or index of the column in training_frame that holds the per-row fold\n",
      " |          assignments.\n",
      " |      :param weights_column: The name or index of the column in training_frame that holds the per-row weights.\n",
      " |      :param validation_frame: H2OFrame with validation data to be scored on while training.\n",
      " |      :param float max_runtime_secs: Maximum allowed runtime in seconds for model training. Use 0 to disable.\n",
      " |      :param bool verbose: Print scoring history to stdout. Defaults to False.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from h2o.estimators.estimator_base.H2OEstimator:\n",
      " |  \n",
      " |  mixin(obj, cls)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from h2o.model.model_base.ModelBase:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  aic(self, train=False, valid=False, xval=False)\n",
      " |      Get the AIC (Akaike Information Criterium).\n",
      " |      \n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\",\n",
      " |      \"valid\", and \"xval\".\n",
      " |      \n",
      " |      :param bool train: If train is True, then return the AIC value for the training data.\n",
      " |      :param bool valid: If valid is True, then return the AIC value for the validation data.\n",
      " |      :param bool xval:  If xval is True, then return the AIC value for the validation data.\n",
      " |      \n",
      " |      :returns: The AIC.\n",
      " |  \n",
      " |  auc(self, train=False, valid=False, xval=False)\n",
      " |      Get the AUC (Area Under Curve).\n",
      " |      \n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\",\n",
      " |      \"valid\", and \"xval\".\n",
      " |      \n",
      " |      :param bool train: If train is True, then return the AUC value for the training data.\n",
      " |      :param bool valid: If valid is True, then return the AUC value for the validation data.\n",
      " |      :param bool xval:  If xval is True, then return the AUC value for the validation data.\n",
      " |      \n",
      " |      :returns: The AUC.\n",
      " |  \n",
      " |  biases(self, vector_id=0)\n",
      " |      Return the frame for the respective bias vector.\n",
      " |      \n",
      " |      :param: vector_id: an integer, ranging from 0 to number of layers, that specifies the bias vector to return.\n",
      " |      \n",
      " |      :returns: an H2OFrame which represents the bias vector identified by vector_id\n",
      " |  \n",
      " |  catoffsets(self)\n",
      " |      Categorical offsets for one-hot encoding.\n",
      " |  \n",
      " |  coef(self)\n",
      " |      Return the coefficients which can be applied to the non-standardized data.\n",
      " |      \n",
      " |      Note: standardize = True by default, if set to False then coef() return the coefficients which are fit directly.\n",
      " |  \n",
      " |  coef_norm(self)\n",
      " |      Return coefficients fitted on the standardized data (requires standardize = True, which is on by default).\n",
      " |      \n",
      " |      These coefficients can be used to evaluate variable importance.\n",
      " |  \n",
      " |  cross_validation_fold_assignment(self)\n",
      " |      Obtain the cross-validation fold assignment for all rows in the training data.\n",
      " |      \n",
      " |      :returns: H2OFrame\n",
      " |  \n",
      " |  cross_validation_holdout_predictions(self)\n",
      " |      Obtain the (out-of-sample) holdout predictions of all cross-validation models on the training data.\n",
      " |      \n",
      " |      This is equivalent to summing up all H2OFrames returned by cross_validation_predictions.\n",
      " |      \n",
      " |      :returns: H2OFrame\n",
      " |  \n",
      " |  cross_validation_metrics_summary(self)\n",
      " |      Retrieve Cross-Validation Metrics Summary.\n",
      " |      \n",
      " |      :returns: The cross-validation metrics summary as an H2OTwoDimTable\n",
      " |  \n",
      " |  cross_validation_models(self)\n",
      " |      Obtain a list of cross-validation models.\n",
      " |      \n",
      " |      :returns: list of H2OModel objects.\n",
      " |  \n",
      " |  cross_validation_predictions(self)\n",
      " |      Obtain the (out-of-sample) holdout predictions of all cross-validation models on their holdout data.\n",
      " |      \n",
      " |      Note that the predictions are expanded to the full number of rows of the training data, with 0 fill-in.\n",
      " |      \n",
      " |      :returns: list of H2OFrame objects.\n",
      " |  \n",
      " |  deepfeatures(self, test_data, layer)\n",
      " |      Return hidden layer details.\n",
      " |      \n",
      " |      :param test_data: Data to create a feature space on\n",
      " |      :param layer: 0 index hidden layer\n",
      " |  \n",
      " |  download_mojo(self, path='.', get_genmodel_jar=False, genmodel_name='')\n",
      " |      Download the model in MOJO format.\n",
      " |      \n",
      " |      :param path: the path where MOJO file should be saved.\n",
      " |      :param get_genmodel_jar: if True, then also download h2o-genmodel.jar and store it in folder ``path``.\n",
      " |      :param genmodel_name Custom name of genmodel jar\n",
      " |      :returns: name of the MOJO file written.\n",
      " |  \n",
      " |  download_pojo(self, path='', get_genmodel_jar=False, genmodel_name='')\n",
      " |      Download the POJO for this model to the directory specified by path.\n",
      " |      \n",
      " |      If path is an empty string, then dump the output to screen.\n",
      " |      \n",
      " |      :param path:  An absolute path to the directory where POJO should be saved.\n",
      " |      :param get_genmodel_jar: if True, then also download h2o-genmodel.jar and store it in folder ``path``.\n",
      " |      :param genmodel_name Custom name of genmodel jar\n",
      " |      :returns: name of the POJO file written.\n",
      " |  \n",
      " |  get_xval_models(self, key=None)\n",
      " |      Return a Model object.\n",
      " |      \n",
      " |      :param key: If None, return all cross-validated models; otherwise return the model that key points to.\n",
      " |      \n",
      " |      :returns: A model or list of models.\n",
      " |  \n",
      " |  gini(self, train=False, valid=False, xval=False)\n",
      " |      Get the Gini coefficient.\n",
      " |      \n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\",\n",
      " |      \"valid\", and \"xval\"\n",
      " |      \n",
      " |      :param bool train: If train is True, then return the Gini Coefficient value for the training data.\n",
      " |      :param bool valid: If valid is True, then return the Gini Coefficient value for the validation data.\n",
      " |      :param bool xval:  If xval is True, then return the Gini Coefficient value for the cross validation data.\n",
      " |      \n",
      " |      :returns: The Gini Coefficient for this binomial model.\n",
      " |  \n",
      " |  is_cross_validated(self)\n",
      " |      Return True if the model was cross-validated.\n",
      " |  \n",
      " |  logloss(self, train=False, valid=False, xval=False)\n",
      " |      Get the Log Loss.\n",
      " |      \n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\",\n",
      " |      \"valid\", and \"xval\".\n",
      " |      \n",
      " |      :param bool train: If train is True, then return the log loss value for the training data.\n",
      " |      :param bool valid: If valid is True, then return the log loss value for the validation data.\n",
      " |      :param bool xval:  If xval is True, then return the log loss value for the cross validation data.\n",
      " |      \n",
      " |      :returns: The log loss for this regression model.\n",
      " |  \n",
      " |  mae(self, train=False, valid=False, xval=False)\n",
      " |      Get the Mean Absolute Error.\n",
      " |      \n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\",\n",
      " |      \"valid\", and \"xval\".\n",
      " |      \n",
      " |      :param bool train: If train is True, then return the MAE value for the training data.\n",
      " |      :param bool valid: If valid is True, then return the MAE value for the validation data.\n",
      " |      :param bool xval:  If xval is True, then return the MAE value for the cross validation data.\n",
      " |      \n",
      " |      :returns: The MAE for this regression model.\n",
      " |  \n",
      " |  mean_residual_deviance(self, train=False, valid=False, xval=False)\n",
      " |      Get the Mean Residual Deviances.\n",
      " |      \n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\",\n",
      " |      \"valid\", and \"xval\".\n",
      " |      \n",
      " |      :param bool train: If train is True, then return the Mean Residual Deviance value for the training data.\n",
      " |      :param bool valid: If valid is True, then return the Mean Residual Deviance value for the validation data.\n",
      " |      :param bool xval:  If xval is True, then return the Mean Residual Deviance value for the cross validation data.\n",
      " |      \n",
      " |      :returns: The Mean Residual Deviance for this regression model.\n",
      " |  \n",
      " |  model_performance(self, test_data=None, train=False, valid=False, xval=False)\n",
      " |      Generate model metrics for this model on test_data.\n",
      " |      \n",
      " |      :param H2OFrame test_data: Data set for which model metrics shall be computed against. All three of train,\n",
      " |          valid and xval arguments are ignored if test_data is not None.\n",
      " |      :param bool train: Report the training metrics for the model.\n",
      " |      :param bool valid: Report the validation metrics for the model.\n",
      " |      :param bool xval: Report the cross-validation metrics for the model. If train and valid are True, then it\n",
      " |          defaults to True.\n",
      " |      \n",
      " |      :returns: An object of class H2OModelMetrics.\n",
      " |  \n",
      " |  mse(self, train=False, valid=False, xval=False)\n",
      " |      Get the Mean Square Error.\n",
      " |      \n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\",\n",
      " |      \"valid\", and \"xval\".\n",
      " |      \n",
      " |      :param bool train: If train is True, then return the MSE value for the training data.\n",
      " |      :param bool valid: If valid is True, then return the MSE value for the validation data.\n",
      " |      :param bool xval:  If xval is True, then return the MSE value for the cross validation data.\n",
      " |      \n",
      " |      :returns: The MSE for this regression model.\n",
      " |  \n",
      " |  normmul(self)\n",
      " |      Normalization/Standardization multipliers for numeric predictors.\n",
      " |  \n",
      " |  normsub(self)\n",
      " |      Normalization/Standardization offsets for numeric predictors.\n",
      " |  \n",
      " |  null_degrees_of_freedom(self, train=False, valid=False, xval=False)\n",
      " |      Retreive the null degress of freedom if this model has the attribute, or None otherwise.\n",
      " |      \n",
      " |      :param bool train: Get the null dof for the training set. If both train and valid are False, then train is\n",
      " |          selected by default.\n",
      " |      :param bool valid: Get the null dof for the validation set. If both train and valid are True, then train is\n",
      " |          selected by default.\n",
      " |      \n",
      " |      :returns: Return the null dof, or None if it is not present.\n",
      " |  \n",
      " |  null_deviance(self, train=False, valid=False, xval=False)\n",
      " |      Retreive the null deviance if this model has the attribute, or None otherwise.\n",
      " |      \n",
      " |      :param bool train: Get the null deviance for the training set. If both train and valid are False, then train\n",
      " |          is selected by default.\n",
      " |      :param bool valid: Get the null deviance for the validation set. If both train and valid are True, then train\n",
      " |          is selected by default.\n",
      " |      \n",
      " |      :returns: Return the null deviance, or None if it is not present.\n",
      " |  \n",
      " |  partial_plot(self, data, cols, destination_key=None, nbins=20, weight_column=None, plot=True, plot_stddev=True, figsize=(7, 10), server=False, include_na=False, user_splits=None, save_to_file=None)\n",
      " |      Create partial dependence plot which gives a graphical depiction of the marginal effect of a variable on the\n",
      " |      response. The effect of a variable is measured in change in the mean response.\n",
      " |      \n",
      " |      :param H2OFrame data: An H2OFrame object used for scoring and constructing the plot.\n",
      " |      :param cols: Feature(s) for which partial dependence will be calculated.\n",
      " |      :param destination_key: An key reference to the created partial dependence tables in H2O.\n",
      " |      :param nbins: Number of bins used. For categorical columns make sure the number of bins exceed the level count. If you enable add_missing_NA, the returned length will be nbin+1.\n",
      " |      :param weight_column: A string denoting which column of data should be used as the weight column.\n",
      " |      :param plot: A boolean specifying whether to plot partial dependence table.\n",
      " |      :param plot_stddev: A boolean specifying whether to add std err to partial dependence plot.\n",
      " |      :param figsize: Dimension/size of the returning plots, adjust to fit your output cells.\n",
      " |      :param server: ?\n",
      " |      :param include_na: A boolean specifying whether missing value should be included in the Feature values.\n",
      " |      :param user_splits: a dictionary containing column names as key and user defined split values as value in a list.\n",
      " |      :param save_to_file Fully qualified name to an image file the resulting plot should be saved to, e.g. '/home/user/pdpplot.png'. The 'png' postfix might be omitted. If the file already exists, it will be overridden. Plot is only saved if plot = True.\n",
      " |      :returns: Plot and list of calculated mean response tables for each feature requested.\n",
      " |  \n",
      " |  pprint_coef(self)\n",
      " |      Pretty print the coefficents table (includes normalized coefficients).\n",
      " |  \n",
      " |  predict(self, test_data, custom_metric=None, custom_metric_func=None)\n",
      " |      Predict on a dataset.\n",
      " |      \n",
      " |      :param H2OFrame test_data: Data on which to make predictions.\n",
      " |      :param custom_metric:  custom evaluation function defined as class reference, the class get uploaded\n",
      " |      into cluster\n",
      " |      :param custom_metric_func: custom evaluation function reference, e.g, result of upload_custom_metric\n",
      " |      \n",
      " |      :returns: A new H2OFrame of predictions.\n",
      " |  \n",
      " |  predict_leaf_node_assignment(self, test_data, type='Path')\n",
      " |      Predict on a dataset and return the leaf node assignment (only for tree-based models).\n",
      " |      \n",
      " |      :param H2OFrame test_data: Data on which to make predictions.\n",
      " |      :param Enum type: How to identify the leaf node. Nodes can be either identified by a path from to the root node\n",
      " |      of the tree to the node or by H2O's internal node id. One of: ``\"Path\"``, ``\"Node_ID\"`` (default: ``\"Path\"``).\n",
      " |      \n",
      " |      :returns: A new H2OFrame of predictions.\n",
      " |  \n",
      " |  r2(self, train=False, valid=False, xval=False)\n",
      " |      Return the R squared for this regression model.\n",
      " |      \n",
      " |      Will return R^2 for GLM Models and will return NaN otherwise.\n",
      " |      \n",
      " |      The R^2 value is defined to be 1 - MSE/var, where var is computed as sigma*sigma.\n",
      " |      \n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\",\n",
      " |      \"valid\", and \"xval\".\n",
      " |      \n",
      " |      :param bool train: If train is True, then return the R^2 value for the training data.\n",
      " |      :param bool valid: If valid is True, then return the R^2 value for the validation data.\n",
      " |      :param bool xval:  If xval is True, then return the R^2 value for the cross validation data.\n",
      " |      \n",
      " |      :returns: The R squared for this regression model.\n",
      " |  \n",
      " |  residual_degrees_of_freedom(self, train=False, valid=False, xval=False)\n",
      " |      Retreive the residual degress of freedom if this model has the attribute, or None otherwise.\n",
      " |      \n",
      " |      :param bool train: Get the residual dof for the training set. If both train and valid are False, then train\n",
      " |          is selected by default.\n",
      " |      :param bool valid: Get the residual dof for the validation set. If both train and valid are True, then train\n",
      " |          is selected by default.\n",
      " |      \n",
      " |      :returns: Return the residual dof, or None if it is not present.\n",
      " |  \n",
      " |  residual_deviance(self, train=False, valid=False, xval=None)\n",
      " |      Retreive the residual deviance if this model has the attribute, or None otherwise.\n",
      " |      \n",
      " |      :param bool train: Get the residual deviance for the training set. If both train and valid are False, then\n",
      " |          train is selected by default.\n",
      " |      :param bool valid: Get the residual deviance for the validation set. If both train and valid are True, then\n",
      " |          train is selected by default.\n",
      " |      \n",
      " |      :returns: Return the residual deviance, or None if it is not present.\n",
      " |  \n",
      " |  respmul(self)\n",
      " |      Normalization/Standardization multipliers for numeric response.\n",
      " |  \n",
      " |  respsub(self)\n",
      " |      Normalization/Standardization offsets for numeric response.\n",
      " |  \n",
      " |  rmse(self, train=False, valid=False, xval=False)\n",
      " |      Get the Root Mean Square Error.\n",
      " |      \n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\",\n",
      " |      \"valid\", and \"xval\".\n",
      " |      \n",
      " |      :param bool train: If train is True, then return the RMSE value for the training data.\n",
      " |      :param bool valid: If valid is True, then return the RMSE value for the validation data.\n",
      " |      :param bool xval:  If xval is True, then return the RMSE value for the cross validation data.\n",
      " |      \n",
      " |      :returns: The RMSE for this regression model.\n",
      " |  \n",
      " |  rmsle(self, train=False, valid=False, xval=False)\n",
      " |      Get the Root Mean Squared Logarithmic Error.\n",
      " |      \n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\",\n",
      " |      \"valid\", and \"xval\".\n",
      " |      \n",
      " |      :param bool train: If train is True, then return the RMSLE value for the training data.\n",
      " |      :param bool valid: If valid is True, then return the RMSLE value for the validation data.\n",
      " |      :param bool xval:  If xval is True, then return the RMSLE value for the cross validation data.\n",
      " |      \n",
      " |      :returns: The RMSLE for this regression model.\n",
      " |  \n",
      " |  rotation(self)\n",
      " |      Obtain the rotations (eigenvectors) for a PCA model\n",
      " |      \n",
      " |      :return: H2OFrame\n",
      " |  \n",
      " |  save_model_details(self, path='', force=False)\n",
      " |      Save Model Details of an H2O Model in JSON Format to disk.\n",
      " |      \n",
      " |      :param model: The model object to save.\n",
      " |      :param path: a path to save the model details at (hdfs, s3, local)\n",
      " |      :param force: if True overwrite destination directory in case it exists, or throw exception if set to False.\n",
      " |      \n",
      " |      :returns str: the path of the saved model details\n",
      " |  \n",
      " |  save_mojo(self, path='', force=False)\n",
      " |      Save an H2O Model as MOJO (Model Object, Optimized) to disk.\n",
      " |      \n",
      " |      :param model: The model object to save.\n",
      " |      :param path: a path to save the model at (hdfs, s3, local)\n",
      " |      :param force: if True overwrite destination directory in case it exists, or throw exception if set to False.\n",
      " |      \n",
      " |      :returns str: the path of the saved model\n",
      " |  \n",
      " |  score_history(self)\n",
      " |      DEPRECATED. Use :meth:`scoring_history` instead.\n",
      " |  \n",
      " |  scoring_history(self)\n",
      " |      Retrieve Model Score History.\n",
      " |      \n",
      " |      :returns: The score history as an H2OTwoDimTable or a Pandas DataFrame.\n",
      " |  \n",
      " |  show(self)\n",
      " |      Print innards of model, without regards to type.\n",
      " |  \n",
      " |  staged_predict_proba(self, test_data)\n",
      " |      Predict class probabilities at each stage of an H2O Model (only GBM models).\n",
      " |      \n",
      " |      The output structure is analogous to the output of function predict_leaf_node_assignment. For each tree t and\n",
      " |      class c there will be a column Tt.Cc (eg. T3.C1 for tree 3 and class 1). The value will be the corresponding\n",
      " |      predicted probability of this class by combining the raw contributions of trees T1.Cc,..,TtCc. Binomial models\n",
      " |      build the trees just for the first class and values in columns Tx.C1 thus correspond to the the probability p0.\n",
      " |      \n",
      " |      :param H2OFrame test_data: Data on which to make predictions.\n",
      " |      \n",
      " |      :returns: A new H2OFrame of staged predictions.\n",
      " |  \n",
      " |  std_coef_plot(self, num_of_features=None, server=False)\n",
      " |      Plot a GLM model\"s standardized coefficient magnitudes.\n",
      " |      \n",
      " |      :param num_of_features: the number of features shown in the plot.\n",
      " |      :param server: ?\n",
      " |      \n",
      " |      :returns: None.\n",
      " |  \n",
      " |  summary(self)\n",
      " |      Print a detailed summary of the model.\n",
      " |  \n",
      " |  varimp(self, use_pandas=False)\n",
      " |      Pretty print the variable importances, or return them in a list.\n",
      " |      \n",
      " |      :param use_pandas: If True, then the variable importances will be returned as a pandas data frame.\n",
      " |      \n",
      " |      :returns: A list or Pandas DataFrame.\n",
      " |  \n",
      " |  varimp_plot(self, num_of_features=None, server=False)\n",
      " |      Plot the variable importance for a trained model.\n",
      " |      \n",
      " |      :param num_of_features: the number of features shown in the plot (default is 10 or all if less than 10).\n",
      " |      :param server: ?\n",
      " |      \n",
      " |      :returns: None.\n",
      " |  \n",
      " |  weights(self, matrix_id=0)\n",
      " |      Return the frame for the respective weight matrix.\n",
      " |      \n",
      " |      :param: matrix_id: an integer, ranging from 0 to number of layers, that specifies the weight matrix to return.\n",
      " |      \n",
      " |      :returns: an H2OFrame which represents the weight matrix identified by matrix_id\n",
      " |  \n",
      " |  xval_keys(self)\n",
      " |      Return model keys for the cross-validated model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from h2o.model.model_base.ModelBase:\n",
      " |  \n",
      " |  actual_params\n",
      " |      Dictionary of actual parameters of the model.\n",
      " |  \n",
      " |  default_params\n",
      " |      Dictionary of the default parameters of the model.\n",
      " |  \n",
      " |  end_time\n",
      " |      Timestamp (milliseconds since 1970) when the model training was ended.\n",
      " |  \n",
      " |  full_parameters\n",
      " |      Dictionary of the full specification of all parameters.\n",
      " |  \n",
      " |  have_mojo\n",
      " |      True, if export to MOJO is possible\n",
      " |  \n",
      " |  have_pojo\n",
      " |      True, if export to POJO is possible\n",
      " |  \n",
      " |  model_id\n",
      " |      Model identifier.\n",
      " |  \n",
      " |  params\n",
      " |      Get the parameters and the actual/default values only.\n",
      " |      \n",
      " |      :returns: A dictionary of parameters used to build this model.\n",
      " |  \n",
      " |  run_time\n",
      " |      Model training time in milliseconds\n",
      " |  \n",
      " |  start_time\n",
      " |      Timestamp (milliseconds since 1970) when the model training was started.\n",
      " |  \n",
      " |  type\n",
      " |      The type of model built: ``\"classifier\"`` or ``\"regressor\"`` or ``\"unsupervised\"``\n",
      " |  \n",
      " |  xvals\n",
      " |      Return a list of the cross-validated models.\n",
      " |      \n",
      " |      :returns: A list of models.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from h2o.utils.backward_compatibility.BackwardsCompatibleBase:\n",
      " |  \n",
      " |  __getattr__(self, item)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from h2o.utils.backward_compatibility.BackwardsCompatibleBase:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n",
    "help(H2OGeneralizedLinearEstimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glm_gaussian_v1 = H2OGeneralizedLinearEstimator(\n",
    "                    model_id='glm_gaussian_v1',           \n",
    "                    family='gaussian',\n",
    "                    solver='L_BFGS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glm Model Build progress: |███████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "glm_gaussian_v1.train(X,y, training_frame=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details\n",
      "=============\n",
      "H2OGeneralizedLinearEstimator :  Generalized Linear Modeling\n",
      "Model Key:  glm_gaussian_v1\n",
      "\n",
      "\n",
      "ModelMetricsRegressionGLM: glm\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.012269539491957344\n",
      "RMSE: 0.11076795336177944\n",
      "MAE: 0.0421434922559707\n",
      "RMSLE: NaN\n",
      "R^2: 0.5221880582325444\n",
      "Mean Residual Deviance: 0.012269539491957344\n",
      "Null degrees of freedom: 81198\n",
      "Residual degrees of freedom: 75205\n",
      "Null deviance: 2085.076261430717\n",
      "Residual deviance: 996.2743372074444\n",
      "AIC: -114904.42628620921\n",
      "Scoring History: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>timestamp</b></td>\n",
       "<td><b>duration</b></td>\n",
       "<td><b>iterations</b></td>\n",
       "<td><b>negative_log_likelihood</b></td>\n",
       "<td><b>objective</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:24</td>\n",
       "<td> 0.000 sec</td>\n",
       "<td>0</td>\n",
       "<td>2085.0762605</td>\n",
       "<td>0.0256786</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:24</td>\n",
       "<td> 0.222 sec</td>\n",
       "<td>1</td>\n",
       "<td>1779.2651238</td>\n",
       "<td>0.0219201</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:24</td>\n",
       "<td> 0.342 sec</td>\n",
       "<td>2</td>\n",
       "<td>1561.9075890</td>\n",
       "<td>0.0192609</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:24</td>\n",
       "<td> 0.497 sec</td>\n",
       "<td>3</td>\n",
       "<td>1519.7272957</td>\n",
       "<td>0.0187417</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:24</td>\n",
       "<td> 0.557 sec</td>\n",
       "<td>4</td>\n",
       "<td>1491.8459734</td>\n",
       "<td>0.0184080</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:25</td>\n",
       "<td> 0.648 sec</td>\n",
       "<td>5</td>\n",
       "<td>1329.8148690</td>\n",
       "<td>0.0164578</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:25</td>\n",
       "<td> 0.697 sec</td>\n",
       "<td>6</td>\n",
       "<td>1118.5878503</td>\n",
       "<td>0.0139849</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:25</td>\n",
       "<td> 0.721 sec</td>\n",
       "<td>7</td>\n",
       "<td>1048.7107655</td>\n",
       "<td>0.0131789</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:25</td>\n",
       "<td> 0.773 sec</td>\n",
       "<td>8</td>\n",
       "<td>1039.7860432</td>\n",
       "<td>0.0130861</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:25</td>\n",
       "<td> 0.803 sec</td>\n",
       "<td>9</td>\n",
       "<td>1018.3239000</td>\n",
       "<td>0.0128346</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:25</td>\n",
       "<td> 0.829 sec</td>\n",
       "<td>10</td>\n",
       "<td>1012.8626405</td>\n",
       "<td>0.0127684</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:25</td>\n",
       "<td> 0.841 sec</td>\n",
       "<td>11</td>\n",
       "<td>1005.2647022</td>\n",
       "<td>0.0126977</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:25</td>\n",
       "<td> 0.856 sec</td>\n",
       "<td>12</td>\n",
       "<td>1005.7540186</td>\n",
       "<td>0.0126839</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:25</td>\n",
       "<td> 0.878 sec</td>\n",
       "<td>13</td>\n",
       "<td>999.8186388</td>\n",
       "<td>0.0126269</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:25</td>\n",
       "<td> 0.903 sec</td>\n",
       "<td>14</td>\n",
       "<td>996.5404870</td>\n",
       "<td>0.0125960</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:25</td>\n",
       "<td> 0.979 sec</td>\n",
       "<td>15</td>\n",
       "<td>996.8052108</td>\n",
       "<td>0.0125937</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:25</td>\n",
       "<td> 1.005 sec</td>\n",
       "<td>16</td>\n",
       "<td>996.3725005</td>\n",
       "<td>0.0125895</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:25</td>\n",
       "<td> 1.075 sec</td>\n",
       "<td>17</td>\n",
       "<td>996.2743361</td>\n",
       "<td>0.0125888</td></tr></table></div>"
      ],
      "text/plain": [
       "    timestamp            duration    iterations    negative_log_likelihood    objective\n",
       "--  -------------------  ----------  ------------  -------------------------  -----------\n",
       "    2019-02-20 01:34:24  0.000 sec   0             2085.08                    0.0256786\n",
       "    2019-02-20 01:34:24  0.222 sec   1             1779.27                    0.0219201\n",
       "    2019-02-20 01:34:24  0.342 sec   2             1561.91                    0.0192609\n",
       "    2019-02-20 01:34:24  0.497 sec   3             1519.73                    0.0187417\n",
       "    2019-02-20 01:34:24  0.557 sec   4             1491.85                    0.018408\n",
       "    2019-02-20 01:34:25  0.648 sec   5             1329.81                    0.0164578\n",
       "    2019-02-20 01:34:25  0.697 sec   6             1118.59                    0.0139849\n",
       "    2019-02-20 01:34:25  0.721 sec   7             1048.71                    0.0131789\n",
       "    2019-02-20 01:34:25  0.773 sec   8             1039.79                    0.0130861\n",
       "    2019-02-20 01:34:25  0.803 sec   9             1018.32                    0.0128346\n",
       "    2019-02-20 01:34:25  0.829 sec   10            1012.86                    0.0127684\n",
       "    2019-02-20 01:34:25  0.841 sec   11            1005.26                    0.0126977\n",
       "    2019-02-20 01:34:25  0.856 sec   12            1005.75                    0.0126839\n",
       "    2019-02-20 01:34:25  0.878 sec   13            999.819                    0.0126269\n",
       "    2019-02-20 01:34:25  0.903 sec   14            996.54                     0.012596\n",
       "    2019-02-20 01:34:25  0.979 sec   15            996.805                    0.0125937\n",
       "    2019-02-20 01:34:25  1.005 sec   16            996.373                    0.0125895\n",
       "    2019-02-20 01:34:25  1.075 sec   17            996.274                    0.0125888"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm_gaussian_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows:81199\n",
      "Cols:30\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>       </th><th>logerror            </th><th>regionidneighborhood  </th><th>garagetotalsqft  </th><th>finishedsquarefeet12  </th><th>structuretaxvaluedollarcnt  </th><th>yearbuilt         </th><th>taxamount        </th><th>calculatedfinishedsquarefeet  </th><th>taxvaluedollarcnt  </th><th>landtaxvaluedollarcnt  </th><th>latitude          </th><th>finishedsquarefeet15  </th><th>lotsizesquarefeet  </th><th>propertyzoningdesc  </th><th>bedroomcnt        </th><th>longitude          </th><th>yardbuildingsqft17  </th><th>finishedsquarefeet6  </th><th>bathroomcnt       </th><th>calculatedbathnbr  </th><th>buildingqualitytypeid  </th><th>fullbathcnt       </th><th>regionidcity     </th><th>propertycountylandusecode  </th><th>heatingorsystemtypeid  </th><th>parcelid          </th><th>regionidzip      </th><th>taxdelinquencyyear  </th><th>logerror_highlow  </th><th>logerror_three  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>type   </td><td>real                </td><td>enum                  </td><td>enum             </td><td>int                   </td><td>int                         </td><td>int               </td><td>real             </td><td>int                           </td><td>int                </td><td>int                    </td><td>int               </td><td>enum                  </td><td>real               </td><td>enum                </td><td>int               </td><td>int                </td><td>enum                </td><td>enum                 </td><td>real              </td><td>real               </td><td>int                    </td><td>int               </td><td>int              </td><td>int                        </td><td>int                    </td><td>int               </td><td>int              </td><td>enum                </td><td>enum              </td><td>enum            </td></tr>\n",
       "<tr><td>mins   </td><td>-4.605              </td><td>                      </td><td>                 </td><td>2.0                   </td><td>100.0                       </td><td>1885.0            </td><td>49.08            </td><td>2.0                           </td><td>22.0               </td><td>22.0                   </td><td>33339295.0        </td><td>                      </td><td>167.0              </td><td>                    </td><td>0.0               </td><td>-119447865.0       </td><td>                    </td><td>                     </td><td>0.0               </td><td>1.0                </td><td>1.0                    </td><td>1.0               </td><td>3491.0           </td><td>0.0                        </td><td>1.0                    </td><td>10711738.0        </td><td>95982.0          </td><td>                    </td><td>                  </td><td>                </td></tr>\n",
       "<tr><td>mean   </td><td>0.011216225569280413</td><td>                      </td><td>                 </td><td>1745.3842687582785    </td><td>180189.65661912953          </td><td>1968.5372693177019</td><td>5986.219545773648</td><td>1772.9539240255067            </td><td>457948.330660854   </td><td>278533.1535875269      </td><td>34005442.26407961 </td><td>                      </td><td>29062.913969636433 </td><td>                    </td><td>3.031798421162824 </td><td>-118198834.11816648</td><td>                    </td><td>                     </td><td>2.279750982154953 </td><td>2.3096141366744862 </td><td>5.565504935901165      </td><td>2.24156391953277  </td><td>33683.95432097216</td><td>202.74086518269638         </td><td>3.9226390018447623     </td><td>12985977.637507854</td><td>96584.28873803388</td><td>                    </td><td>                  </td><td>                </td></tr>\n",
       "<tr><td>maxs   </td><td>4.737               </td><td>                      </td><td>                 </td><td>20013.0               </td><td>9948100.0                   </td><td>2015.0            </td><td>321936.09        </td><td>22741.0                       </td><td>27750000.0         </td><td>24500000.0             </td><td>34816009.0        </td><td>                      </td><td>6971010.0          </td><td>                    </td><td>16.0              </td><td>-117554924.0       </td><td>                    </td><td>                     </td><td>20.0              </td><td>20.0               </td><td>12.0                   </td><td>20.0              </td><td>396556.0         </td><td>6050.0                     </td><td>24.0                   </td><td>162960842.0       </td><td>399675.0         </td><td>                    </td><td>                  </td><td>                </td></tr>\n",
       "<tr><td>sigma  </td><td>0.16024640886809977 </td><td>                      </td><td>                 </td><td>908.989755305141      </td><td>208067.1426526782           </td><td>23.765149134160815</td><td>6868.49559518842 </td><td>927.5453860910411             </td><td>556440.1117803449  </td><td>402603.4461440696      </td><td>265327.54569498595</td><td>                      </td><td>120887.84072477915 </td><td>                    </td><td>1.1566394976045355</td><td>360725.5288288521  </td><td>                    </td><td>                     </td><td>1.0037803036340833</td><td>0.9755370582708923 </td><td>1.9017417146509352     </td><td>0.9624688037648107</td><td>46433.35966844541</td><td>306.31475103913937         </td><td>3.67833621976241       </td><td>2505539.8944113203</td><td>3552.660167815567</td><td>                    </td><td>                  </td><td>                </td></tr>\n",
       "<tr><td>zeros  </td><td>762                 </td><td>                      </td><td>                 </td><td>0                     </td><td>0                           </td><td>0                 </td><td>0                </td><td>0                             </td><td>0                  </td><td>0                      </td><td>0                 </td><td>                      </td><td>0                  </td><td>                    </td><td>1282              </td><td>0                  </td><td>                    </td><td>                     </td><td>1052              </td><td>0                  </td><td>0                      </td><td>0                 </td><td>0                </td><td>1                          </td><td>0                      </td><td>0                 </td><td>0                </td><td>                    </td><td>                  </td><td>                </td></tr>\n",
       "<tr><td>missing</td><td>0                   </td><td>0                     </td><td>0                </td><td>4193                  </td><td>350                         </td><td>677               </td><td>6                </td><td>593                           </td><td>1                  </td><td>1                      </td><td>0                 </td><td>0                     </td><td>9139               </td><td>0                   </td><td>0                 </td><td>0                  </td><td>0                   </td><td>0                    </td><td>0                 </td><td>1067               </td><td>29638                  </td><td>1067              </td><td>1622             </td><td>14531                      </td><td>30786                  </td><td>0                 </td><td>32               </td><td>0                   </td><td>0                 </td><td>0               </td></tr>\n",
       "<tr><td>0      </td><td>0.0953              </td><td>nan                   </td><td>0.0              </td><td>1264.0                </td><td>115087.0                    </td><td>1986.0            </td><td>2015.06          </td><td>1264.0                        </td><td>191811.0           </td><td>76724.0                </td><td>34303597.0        </td><td>nan                   </td><td>1735.0             </td><td>nan                 </td><td>3.0               </td><td>-119287236.0       </td><td>128.0               </td><td>nan                  </td><td>2.5               </td><td>2.5                </td><td>nan                    </td><td>2.0               </td><td>34543.0          </td><td>1128.0                     </td><td>nan                    </td><td>17073783.0        </td><td>97081.0          </td><td>nan                 </td><td>1                 </td><td>1               </td></tr>\n",
       "<tr><td>1      </td><td>0.0198              </td><td>nan                   </td><td>0.0              </td><td>777.0                 </td><td>143809.0                    </td><td>1990.0            </td><td>2581.3           </td><td>777.0                         </td><td>239679.0           </td><td>95870.0                </td><td>34272866.0        </td><td>nan                   </td><td>nan                </td><td>nan                 </td><td>2.0               </td><td>-119198911.0       </td><td>198.0               </td><td>nan                  </td><td>1.0               </td><td>1.0                </td><td>nan                    </td><td>1.0               </td><td>34543.0          </td><td>1129.0                     </td><td>nan                    </td><td>17088994.0        </td><td>97083.0          </td><td>nan                 </td><td>1                 </td><td>1               </td></tr>\n",
       "<tr><td>2      </td><td>0.006               </td><td>nan                   </td><td>441.0            </td><td>1101.0                </td><td>33619.0                     </td><td>1956.0            </td><td>591.64           </td><td>1101.0                        </td><td>47853.0            </td><td>14234.0                </td><td>34340801.0        </td><td>nan                   </td><td>6569.0             </td><td>nan                 </td><td>3.0               </td><td>-119079610.0       </td><td>nan                 </td><td>nan                  </td><td>2.0               </td><td>2.0                </td><td>nan                    </td><td>2.0               </td><td>26965.0          </td><td>1111.0                     </td><td>nan                    </td><td>17100444.0        </td><td>97113.0          </td><td>nan                 </td><td>0                 </td><td>1               </td></tr>\n",
       "<tr><td>3      </td><td>-0.0566             </td><td>nan                   </td><td>460.0            </td><td>1554.0                </td><td>45609.0                     </td><td>1965.0            </td><td>682.78           </td><td>1554.0                        </td><td>62914.0            </td><td>17305.0                </td><td>34354313.0        </td><td>nan                   </td><td>7400.0             </td><td>nan                 </td><td>2.0               </td><td>-119076405.0       </td><td>nan                 </td><td>nan                  </td><td>1.5               </td><td>1.5                </td><td>nan                    </td><td>1.0               </td><td>26965.0          </td><td>1110.0                     </td><td>nan                    </td><td>17102429.0        </td><td>97113.0          </td><td>nan                 </td><td>0                 </td><td>1               </td></tr>\n",
       "<tr><td>4      </td><td>0.0573              </td><td>nan                   </td><td>665.0            </td><td>2415.0                </td><td>277000.0                    </td><td>1984.0            </td><td>5886.92          </td><td>2415.0                        </td><td>554000.0           </td><td>277000.0               </td><td>34266578.0        </td><td>nan                   </td><td>6326.0             </td><td>nan                 </td><td>4.0               </td><td>-119165392.0       </td><td>nan                 </td><td>nan                  </td><td>2.5               </td><td>2.5                </td><td>nan                    </td><td>2.0               </td><td>34543.0          </td><td>1111.0                     </td><td>nan                    </td><td>17109604.0        </td><td>97084.0          </td><td>nan                 </td><td>1                 </td><td>1               </td></tr>\n",
       "<tr><td>5      </td><td>0.0564              </td><td>nan                   </td><td>473.0            </td><td>2882.0                </td><td>222070.0                    </td><td>1980.0            </td><td>3110.44          </td><td>2882.0                        </td><td>289609.0           </td><td>67539.0                </td><td>34240014.0        </td><td>nan                   </td><td>10000.0            </td><td>nan                 </td><td>4.0               </td><td>-119024793.0       </td><td>nan                 </td><td>nan                  </td><td>2.5               </td><td>2.5                </td><td>nan                    </td><td>2.0               </td><td>51239.0          </td><td>1111.0                     </td><td>nan                    </td><td>17125829.0        </td><td>97089.0          </td><td>nan                 </td><td>1                 </td><td>1               </td></tr>\n",
       "<tr><td>6      </td><td>0.0315              </td><td>nan                   </td><td>467.0            </td><td>1772.0                </td><td>185000.0                    </td><td>1978.0            </td><td>5632.2           </td><td>1772.0                        </td><td>526000.0           </td><td>341000.0               </td><td>34226842.0        </td><td>nan                   </td><td>8059.0             </td><td>nan                 </td><td>3.0               </td><td>-119059815.0       </td><td>1045.0              </td><td>nan                  </td><td>2.0               </td><td>2.0                </td><td>nan                    </td><td>2.0               </td><td>51239.0          </td><td>1111.0                     </td><td>nan                    </td><td>17132911.0        </td><td>97089.0          </td><td>nan                 </td><td>1                 </td><td>1               </td></tr>\n",
       "<tr><td>7      </td><td>0.0257              </td><td>nan                   </td><td>440.0            </td><td>2632.0                </td><td>342611.0                    </td><td>1971.0            </td><td>6109.94          </td><td>2632.0                        </td><td>571086.0           </td><td>228475.0               </td><td>34229816.0        </td><td>nan                   </td><td>7602.0             </td><td>nan                 </td><td>5.0               </td><td>-119050224.0       </td><td>180.0               </td><td>nan                  </td><td>2.5               </td><td>2.5                </td><td>nan                    </td><td>2.0               </td><td>51239.0          </td><td>1111.0                     </td><td>nan                    </td><td>17134926.0        </td><td>97089.0          </td><td>nan                 </td><td>1                 </td><td>1               </td></tr>\n",
       "<tr><td>8      </td><td>0.002               </td><td>nan                   </td><td>494.0            </td><td>1292.0                </td><td>231297.0                    </td><td>1979.0            </td><td>5026.4           </td><td>1292.0                        </td><td>462594.0           </td><td>231297.0               </td><td>34226351.0        </td><td>nan                   </td><td>7405.0             </td><td>nan                 </td><td>3.0               </td><td>-118983853.0       </td><td>304.0               </td><td>nan                  </td><td>2.0               </td><td>2.0                </td><td>nan                    </td><td>2.0               </td><td>51239.0          </td><td>1111.0                     </td><td>nan                    </td><td>17139988.0        </td><td>97091.0          </td><td>nan                 </td><td>0                 </td><td>1               </td></tr>\n",
       "<tr><td>9      </td><td>-0.0576             </td><td>nan                   </td><td>253.0            </td><td>1385.0                </td><td>134251.0                    </td><td>1950.0            </td><td>3217.06          </td><td>1385.0                        </td><td>268502.0           </td><td>134251.0               </td><td>34179289.0        </td><td>nan                   </td><td>6000.0             </td><td>nan                 </td><td>3.0               </td><td>-119169287.0       </td><td>nan                 </td><td>nan                  </td><td>1.0               </td><td>1.0                </td><td>nan                    </td><td>1.0               </td><td>13150.0          </td><td>1111.0                     </td><td>nan                    </td><td>17167359.0        </td><td>97104.0          </td><td>nan                 </td><td>0                 </td><td>1               </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lambda',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_bc',\n",
       " '_bcin',\n",
       " '_check_and_save_parm',\n",
       " '_check_targets',\n",
       " '_compute_algo',\n",
       " '_end_time',\n",
       " '_estimator_type',\n",
       " '_future',\n",
       " '_get_metrics',\n",
       " '_have_mojo',\n",
       " '_have_pojo',\n",
       " '_id',\n",
       " '_is_xvalidated',\n",
       " '_job',\n",
       " '_keyify_if_h2oframe',\n",
       " '_make_model',\n",
       " '_metrics_class',\n",
       " '_model_json',\n",
       " '_parms',\n",
       " '_plot',\n",
       " '_requires_training_frame',\n",
       " '_resolve_model',\n",
       " '_run_time',\n",
       " '_start_time',\n",
       " '_verify_training_frame_params',\n",
       " '_xval_keys',\n",
       " 'actual_params',\n",
       " 'aic',\n",
       " 'algo',\n",
       " 'alpha',\n",
       " 'auc',\n",
       " 'balance_classes',\n",
       " 'beta_constraints',\n",
       " 'beta_epsilon',\n",
       " 'biases',\n",
       " 'catoffsets',\n",
       " 'class_sampling_factors',\n",
       " 'coef',\n",
       " 'coef_norm',\n",
       " 'compute_p_values',\n",
       " 'convert_H2OXGBoostParams_2_XGBoostParams',\n",
       " 'cross_validation_fold_assignment',\n",
       " 'cross_validation_holdout_predictions',\n",
       " 'cross_validation_metrics_summary',\n",
       " 'cross_validation_models',\n",
       " 'cross_validation_predictions',\n",
       " 'custom_metric_func',\n",
       " 'deepfeatures',\n",
       " 'default_params',\n",
       " 'download_mojo',\n",
       " 'download_pojo',\n",
       " 'early_stopping',\n",
       " 'end_time',\n",
       " 'export_checkpoints_dir',\n",
       " 'family',\n",
       " 'fit',\n",
       " 'fold_assignment',\n",
       " 'fold_column',\n",
       " 'full_parameters',\n",
       " 'getGLMRegularizationPath',\n",
       " 'get_params',\n",
       " 'get_xval_models',\n",
       " 'gini',\n",
       " 'gradient_epsilon',\n",
       " 'have_mojo',\n",
       " 'have_pojo',\n",
       " 'ignore_const_cols',\n",
       " 'ignored_columns',\n",
       " 'interaction_pairs',\n",
       " 'interactions',\n",
       " 'intercept',\n",
       " 'is_cross_validated',\n",
       " 'join',\n",
       " 'keep_cross_validation_fold_assignment',\n",
       " 'keep_cross_validation_models',\n",
       " 'keep_cross_validation_predictions',\n",
       " 'lambda_',\n",
       " 'lambda_min_ratio',\n",
       " 'lambda_search',\n",
       " 'link',\n",
       " 'logloss',\n",
       " 'mae',\n",
       " 'makeGLMModel',\n",
       " 'max_active_predictors',\n",
       " 'max_after_balance_size',\n",
       " 'max_confusion_matrix_size',\n",
       " 'max_hit_ratio_k',\n",
       " 'max_iterations',\n",
       " 'max_runtime_secs',\n",
       " 'mean_residual_deviance',\n",
       " 'missing_values_handling',\n",
       " 'mixin',\n",
       " 'model_id',\n",
       " 'model_performance',\n",
       " 'mse',\n",
       " 'nfolds',\n",
       " 'nlambdas',\n",
       " 'non_negative',\n",
       " 'normmul',\n",
       " 'normsub',\n",
       " 'null_degrees_of_freedom',\n",
       " 'null_deviance',\n",
       " 'obj_reg',\n",
       " 'objective_epsilon',\n",
       " 'offset_column',\n",
       " 'params',\n",
       " 'parms',\n",
       " 'partial_plot',\n",
       " 'plot',\n",
       " 'pprint_coef',\n",
       " 'predict',\n",
       " 'predict_leaf_node_assignment',\n",
       " 'prior',\n",
       " 'r2',\n",
       " 'remove_collinear_columns',\n",
       " 'residual_degrees_of_freedom',\n",
       " 'residual_deviance',\n",
       " 'respmul',\n",
       " 'response_column',\n",
       " 'respsub',\n",
       " 'rmse',\n",
       " 'rmsle',\n",
       " 'rotation',\n",
       " 'run_time',\n",
       " 'save_model_details',\n",
       " 'save_mojo',\n",
       " 'score_each_iteration',\n",
       " 'score_history',\n",
       " 'scoring_history',\n",
       " 'seed',\n",
       " 'set_params',\n",
       " 'show',\n",
       " 'solver',\n",
       " 'staged_predict_proba',\n",
       " 'standardize',\n",
       " 'start',\n",
       " 'start_time',\n",
       " 'std_coef_plot',\n",
       " 'summary',\n",
       " 'train',\n",
       " 'training_frame',\n",
       " 'tweedie_link_power',\n",
       " 'tweedie_variance_power',\n",
       " 'type',\n",
       " 'validation_frame',\n",
       " 'varimp',\n",
       " 'varimp_plot',\n",
       " 'weights',\n",
       " 'weights_column',\n",
       " 'xval_keys',\n",
       " 'xvals']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(glm_gaussian_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11076795336177944\n",
      "-0.11076795336177944\n"
     ]
    }
   ],
   "source": [
    "rmse=glm_gaussian_v1.rmse()\n",
    "print(rmse)\n",
    "rmse_neg=(-1*rmse)\n",
    "print(rmse_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAFyCAYAAADYhIJtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzsvXt4HOV99/3d08xK2pW0OuGDbIwl\naxwwMjLGgAkYOwaSPCWlheDGL04JSdrmafIk7ZNeKUmaBEKeNE9oEt60adq0NEcTcmh5k6tJCMbg\nAMZgbNnCgEeWHfDZklarw0ra2d3Zff9YzXp2d847s8ff57raYO3uzOy9933/zr/blU6nQRAEQRBE\n5eMu9wMQBEEQBGEMEtoEQRAEUSWQ0CYIgiCIKoGENkEQBEFUCSS0CYIgCKJKIKFNEARBEFWCt9wP\noMfY2IytNWmhUCMikTk7L0koQONcGmicSwONc2mgcc7Q2Rl0qb1Wd5a21+sp9yPUBTTOpYHGuTTQ\nOJcGGmd96k5oEwRBEES1QkKbIAiCIKoEEtoEQRAEUSWQ0CYIgiCIKoGENkEQBEFUCSS0CYIgCKJK\nIKFNEARBEFUCCW2CIAiCqBJIaBMEQRBElUBCmyAI0wgJEaOROQgJsdyPQhB1RcX3HicIonIQUyk8\nvnsEg8NjmJgW0NbMYqCvE9u29MLjJhuAIJyGhDZBEIZ5fPcIdr1yOvvv8LSQ/ff2rX3leiyCqBtI\nNSYIwhBCQsTg8Jjia4PD4+QqJ4gSQEKbIAhDTEUFTEwLiq9FZmKYiiq/RhCEfZDQJgjCEC0BFm3N\nrOJroaAfLQHl1wiCsA8S2gRBGIL1eTDQ16n42kBfB1gfnYVMEE5DiWgEQRhm25ZeAJkY9sR0DC0B\nBgOrOrJ/JwjCWcjSJgjCMB63G9u29KK/tx2tARZT0TiGjofx+O4RiKlUuR+PIGoesrQJog4QEiKm\nogJaAmzRbuzHd4/gmYNnsv+msi+CKB0ktAmihrG7GYpe2dedm3ootk0QDkLucYKoYaRmKOFpAWlc\ntIof3z1i6XpU9kUQ5YWENkHUKE40Q6GyL4IoLyS0CaJGccIqprIvgigvFNMmiBpFsorDCoK7GKtY\nXvYVmYkhFPRjoI/KvgiiFJDQJogaRbKK5Qd8SBRjFXvcbmzf2oc7N/XYlpFOEIQxSGgTRA3jpFXM\n+jzoCjUWfR2CIIxDQpsgahiyigmitiChTRB1AFnFBFEbUPY4QRAEQVQJJLQJgiAIokogoU0QBEEQ\nVQIJbYIgCIKoEkhoEwRBEESVUJbscY7jugAcAHALz/NHy/EMRH1j51GVBEEQpaLkQpvjOB+AfwEw\nX+p7E4TdR1USBEGUknLsUg8D+DaAs2W4N1Hn2H1UJUEQRCkpqaXNcdy9AMZ4nn+S47j7jXwmFGqE\n12uv+7KzM2jr9QhlKm2cY/Ekho6HFV8bOh7Gn9/ZAD9Tff2GKm2caxUa59JA46xNqXeo+wCkOY7b\nCuAqAN/nOO49PM+fV/tAJDJn6wN0dgYxNjZj6zWJQipxnEcjcxiLKEdlxifncfzNcNV1DavEca5F\naJxLA41zBi3FpaRCm+f5m6T/5jjuWQB/oSWwCcJOnDqqkiAIolRQ5g1RN0hHVSpRzFGVBEEQpaJs\nATye528u172J+sXJoyoJgiCcpvqybgiiCOioSoIgqhkS2kRdQkdVEgRRjVBMmyAIgiCqBBLaBEEQ\nBFElkNAmCIIgiCqBhDZBEARBVAkktAmCIAiiSiChTRAEQRBVAgltgiAIgqgSSGgTBFGTCAkRo5E5\nCAmx3I9Sk9D4lgdqrkLUBUJCpA5odYKYSuHx3SMYHB7DxLSAtmYWA32d2LalFx432SnFQuNbXkho\nEzUNbTD1x+O7R7DrldPZf4enhey/t2/tK9dj1Qw0vuWFdi2ippE2mPC0gDQubjCP7x4p96MRDiAk\nRAwOjym+Njg8Tq7cIqHxLT8ktImahTaY+mMqKmBC4bx0AIjMxDAVVX6NMAaNb/khoU3ULKXcYCgp\npzJoCbBoa2YVXwsF/WgJKL9GGIPGt/xQTJuoWaQNJqwguO3aYChmXlmwPg8G+jpzYq4SA30dlIRY\nJE6PbyyexGhkjhJGNSChTdQspdjAKSmn8ti2pRdAJgQSmYkhFPRjoK8j+3eiOJwYX0n5HToexlhk\nnpRfDUhoEzWNkxu4Xsz8zk09ZC3YiNGyPY/bje1b+3Dnph4q83MAJ8aXlF/jkNAmahonN3AjMfOu\nUKMt96pnrIYgWJ+nJONfrz0A7BpfUn7NQUKbqAuc2MBLETMnKtcKq+V8hlIqIqT8moOENkFYhJKe\nnKeSrbBKVSaKwagiYqdQJ+XXHCS0CaIIKOnJWSrVCqtkZaIY9BQRJ7wLpPyag4Q2QRQBJT05S6Va\nYZWgTNjtwjaiiPx8z3FHvAuSkjt0PIzxyXlSfjUgoU0QNlCqpKd6o1KtsHIqE07F0vUUkbHInGPe\nBUn5/fM7G3D8zXCOIlKviX5qkNAmqhZazPVBJYYgyqlMOBVL11NE4HI57l3wM97sNWo50a8YSGgT\nVQct5vqiVCEIs0pgOZQJJ2PpeopIZ2tDSb0LtZjoZwcktImqgxZzfeJUCMKqEliOfAanY+laiojH\n7S6Zd6FWE/3sgIQ2UVXQYibsplglsJT5DE7H0vUUkVJ5Fyoh0a9SIaFNVBW0mAk7qTYlsFSxdDVF\npFTehUqtGqgEKABIVBV0NCBhJ9V4PvS2Lb3Yur4b7c1+uF1Ae7MfW9d3K1q7Th0ZKwl1pxQaSTlR\not5rt+vO0qaj36qbSi0BIqqTarDo8hPkjFi7tZCsWYlVA5VA3QhtvaPfqHyoeqDFTNhFJSuBeoJX\nK5ZeC8ma1LhImboR2mqTOJVOw+1yVbVGWm/QYibspFKVQKuCt9ri9HpQ46Jc6kJoa03iva+eRyx+\nMd5TjRppvUKLmbCDSlQCixG8lKxZ29SFKak1ieUCW87g8LjtyRsEQVQuTidXmaGYBDlK1qxt6kJo\na01iNSo1c5QgiNIhZV/H4smS3rcYwUuZ17VNXbjHtZJN/IxH0domjZQg6pf8JLDOUAP6e9pLlutS\nbIJcpcbpieKpC6ENqB/9lk6n8fSBMwXvd0ojpSx1gshQyWshPwlsNDJf8lyXYgRvJcbprVJp86Tc\nz1M3Qlvt6DcxlYLL5XJcI62FukmCsINKXwuVkn1th+Ct5mTNSpsnlfI8dSO0JeRHvwGl00hroW7S\nbsqtsRLOofXbVvpaqLTs62oWvMVQafOkUp6n7oS2Gk4ujGI191oTbpWisRL2o/fbVooVq0U1dEmr\ndeyaJ3btnZU0b0lol4CxyXlLmrvSBtjf046t65ehrdlf9s3NKpWisVYz0mYUbGko96PkoPfbVpoV\nq0Qld0mrF7TmSXg6honpGBa3N6l+3m7DoJLmLQltB5EmzkF+FGmV92hp7kob4DODZ/HM4Fm0V5F1\nKtd2AVSMxlqNlDurWQs9a+T2jSsQT4hVYcXmJ4F1tF4cZ8I+1CxhLW8HAOw6cBo7buVUr2u3YVBJ\n3hcS2irY4VbJnzhKqGnuWhsgUB3WqZK2yy0PVYzGWo1UQlazGnrW0Rce3Y/JqACWUVYuuOWtTj6e\nKfJzXXpWtGNmar7cj1UzqFnCH717AEDG29Hf045nBs8qfn5oJAxhs2h677RqGFSS94WEdh52uVX0\nhK7cUlZCawOUU8nWqZK2u/fIeaqNt0glxNW0lFk96yiy0KwoFk8ByPRIEOIiWCZznRePnAd/MlJR\nHiQp18XPeDFj0zVrLUfFCmqWcGMDgztuWAEA2Lp+marQ1lLwnXJlV0rtOwntPOxyq2hNHBeAj9/V\nj+6uoOrn9TZAiUq1TmPxpKbSokSlxwvLvdmWM65mRJnVskaUaGS9uKq3A/tev5D9m9J6K/e424UT\nCZjVODZayue+I+fwrg3LwPo8aGv2o92CS9opV3al1L6T0JZhpyWjNXHamv3o1NlcjW6AlWqdRqa1\n+71vXLMI/MnJqujWVCnZ7uWMqxlVZvOtkeYmBpPRuOI1IzMC+JOTiq8NDo/jjhtX4onnTpR93O3C\nzjhrpcxJK2gpn+OT81nlk/V5sHZVB3YrNL9au6pddS922pVd7hI8EtoytCbTxLQ5S8aOiSPfAMPT\nMcX3VKp1GmrW9hSwjAcPffjaqrASKiXbvVxxNTPKbL410sB68eB39yvOg5YAg0mV/v6RmRgee2oY\nLxw5n/1bNeRxqGF3aKNS5qQVtJTPjtaGHOXTpXINtb9LVIor2wkqWyUrMVpN+l0u4Mn9pyCmUoau\nJSREbB5Yis3rlqK92Q+3C2hv9mPr+m7DE0faAB/68LX40oevLepapcbPeNHf0676+tBIGAAq5lQl\nNfQ221KfBLdtSy+2ru/OzoOuUIPj88DKiVOSNRJsZNQPr1jVobreWgMsjp6MKL5WjSfwFXNqVz6V\nNifNonWgyXVrFmf3AyEh4tCxccX3HToW1vye8r3z//zZdXjow9di+9Y+Q14I6ZCYSh1HsrRlaFky\nqTTwzMEz8LhdmpqsE7XVrM+Dxe1N2HErB2Fz9cSwrCaSVBKVVJ8JlCeruVi3vJbV4/EoV1isvjSE\nF2VWtpxSjXt+qWIx2BnaqLQ5aQW1OXHf7VdgYmIWgD3f04wru1pCDiS089i2pReimMKeQ2eRUiiu\n1nNlqdVWezxuW9xW5Y6nmMFqIkklUUn1mXKcyGrWulcxbnmtBB61zfuOGy8DfzJSlnFX2rxvWLsU\nt1+/3PLmbWdoo1LnpBnU5oTHc3F8S/09qyXkUDnqQ4Xgcbtx24bligIbyNSbjkXmFF+rdreV3dTC\nub618B3sIN8tbyU8Iyka8jFTc2M2sr6yjbu0eYenBaSR2bx/8dwJPL57pKjr2jGGQG3NSaU5IX+t\nVN+zmvZusrQVaAmw8DPubD1pPo/8bEjRbVJpbqtKKAephYSQWvgOxeJ0uYuSB6kc424lYczoOrNz\nDOtlTpbqe1ba3q0FCW1V1PMT1dwmWu6c1gCLeDIFIaHcxcdOKik2Uym1jcVg5DtUgoJUCkoZninH\n3DGzeVtdZ2pjaGYOVdu6sro+SvU9qynkQEJbgamoAEGhY1c+g8NjOZo36/Ogv7cDzxwsrCucE5L4\n/L+/rHjqkd2T0UxsplTCplJi8Va+r/wz+d+hkhSkclCK+VPKuWNm87YrBlrMHDI7NqVWLu1aH07P\ngUpqU6oHCW0FjHYjC08LWc1bmpyHj2Vca25XJuNccrNLbTulhZ1Kp+F2uWzf7I269+pN2Fj5vkY+\nUy3JK3ZTq/PH6OZtZ911KeZQuX6valof1RJyKKnQ5jjOB+BRACsAsAAe4nn+F6V8BiMY7UbmdgEN\nbGYI8yenlMiWVklo2/vq+Zz+23ZNZqPuvWpaTHZg5fvqfaYSeoGXi3LOn2KtRb3PK23eN6xdgtuv\nX559j10x0FLNoXL8XtW2Pqol5FBqS/seAGGe53dwHNcOYBBAxQlt4OLCfeXoqGobxlQamBeSYHwe\n1ckpJJST2ZQOzACKn8xG3HvFLqZqi99aTS7S+0w1Ja/YSbk2Y7m1GJ4W0BpgMLCqA9tvMdY0w2gP\nBaXNu3tJK8bGLhbXmY2Bqq2ZUswhJ38vrb2gWtdHpYTy1Ci10P4pgJ/J/p3U+0Ao1Aiv194NoLNT\n/aAOOR9/39WYigr4Xw8/g4mZwsnX2epHz4r2TJ9thdetEJmJwcP40NmhfsC7HjesXYpfPHdC4e9L\n0L2kFefGZ1WfV+v+opjCo798DfuOnMPY5Dw6Wxtw3ZrFuO/2K3LqKyWMjrPTWPm+Rj7TsyKAzlAD\nRiOFzU06WhvQs6Idfsb5JVbqcbY6f4rlO0+8mmMtTkbjeGbwLN66EMXXPrFJcQ5qfV5+Pn1XSHku\nd8s+nz/OeusM0F8zwZYGx+eQE7+Xkb3A6nerlH2jUimp0OZ5PgoAHMcFkRHen9X7TESlJtoqnZ3B\nHI3ZCOs4ZVf52t4OzEzNQ0yIaAsqa91qx1BqHU8pxhMFz2jGur39+uWYm48XxGZuv345xsZmNJ9X\n7f4AsHPXcMFZzr947gTm5uMFLjYr4+yUBW/l+xr5zMxUCv097Ypzo78n06nM6cYnVsa5WKzOn2KY\nExL47UtvKb524uw0HvnxQey4lVP9vJAQ8cLhwgRRCa25DCiPs946A4ytGafnkBO/l9G9wOx3K8d8\nrkS0FJeSJ6JxHLcMwH8B+BbP8ztLfX8r6CUoaMXAO1r8GJ+azzlD+IYrFwEAnlY4vSY/U9FKAole\nbEbreft72hQ/46SLzekkGSuZoUY/Uy3JK3ZSjkzbnU8dUw0pAcCh4XHcvblX9d5OnE+vt86MrBkA\n2DywFGIqjaGRsCNzyO7fy8xeYHR9SAp7sKXB1LPUI6VORLsEwG8BfJTn+adLee9iMJKgoDQ5G/1e\nnBqN5rwvFhfhcrmwbUsvXC6X7mQuJoFEKzaT/7ytARZNDT4MHQ/j2cGzBYLTyfhUKZJkrAhXI5+p\nluQVK2h5PkqprAgJEUffmtB8z+SsoDkHnTyfXm2d6Z0a+MMneRw9GbHtjAIt7Py9zOwFeusjX2Hv\nDDWgv6e96qsQnKTUlvanAYQA/B3HcX+38Ld38Tzv7IkHNqElBNWOJFRC0kb1tPSxyJxj1m3+8z65\n/1ROfXm+4HSq+UCpkpqsCFczn9FLXqmm5D0jno9SKitTUQGRGeVkUIk2nTlotCLEzkYaWmuGZTwF\nx47aeUZBPnb+Xlb2ArX1ka+wj0bma7qKxQ5KHdP+OICPl/KepUaanKOROUPaaP5kzs+QVcOu7EvW\n50FLgMXQiPIReHLBqbbpcctbLd+/1BmmVjJDi8kmVROAd9x4GaJziYoU4mY8H6XItDViJRtx8xo5\nn14tPGQFo4qCHKdLoez4vexyt1dbSVilQM1VHMKqZZq/Yaphp0VgVHDKN72J6RhYJrOgXjxyHvzJ\niKU4dDW1D7SCmgB8fugchLhYcU1JjG6kpfQcaAkJP+PB2/sXG3Lzyq3NiekYdh04nY0j64WHrKLk\nluaWt5b92NFiscPdXq0lYeWGhLZDWNFGtTZMo9ewglHBKd/0fvgkX+Des+LWqqb2gWbR+j3zO+QB\nuePmpFAsprZ2YjqGZwbPWEoaLOY7KeVgrL40hO23rEIj6zN1LdZXeD69XnjIKkpuaQBlO3bULuxw\nt9e6wu4UJLQdQNqc7rhxJQDj2qhehqvLlYnd2Z3wY0VwHj0ZUbyWPCvWKLWagW00Yxm4OG5ej8ux\nTHojsWq9jXTXgdOmhZsd1QFOxdCNhofsuI/caqwVRbUYd3stK+xOQkLbRtQ2pwc+eI2h+KXWhtkW\nZPGJu9eis7XBkclsRnDqWWNjkTl4GB9Egyea1WoGttGMZeCiO3DXgdOOZdIbiVXrlQMayX2wcl+j\nOBFDN+Km7VZ81Tq1qqiaJX8cOlovZo8TypDQtpFiNyetDXMd14nuzoCp53HqqD8tYeTzuvG1nxzG\n1GwcbUFzFlWltw80i5lEpFDQjwbWa6iu14piY0dt7eaBpXh28KziNdRikJWWbKS0Jsrhpq1VRdUs\n+ePQsyLTdIVQh4S2Tdi1OdmhgTt91J+WMBISKQiJTHlOrR9CYoT835PxKXfCG+jrwLyQNFXXO9DX\niY/ePWDoOeyorRUSomnhVinJRlpropxuWqOKqlzZAKwpbpWMNA5+xut4F8Fqp66Ftp3JPnZtTnZo\n4OVoVuLzulUPR6nn8o383zPQyOCJ504oKmVJMW2qrnfXK6fR2MDgjhtW6D6HUWsyf03I56yecAOA\n0chc2a1YJfTWRKW6q/NLQP2MG4CrIisPqp1q6aVQl0LbibaZdm9OVl3FVi1+sxNWLozGInP42k8O\nZy3sfCaofCPn91RTyjxu9QQlNfYdOYd3bVim+5vpCVyvx4Wdu4Z114SScFu7qh3pdBqf/c6+irJi\nJYyuCaPKcik393xlQ2qHDJAnyy7mhCQee2q4wItVqcpQXQptJyzRUmxORjYLMxa/kBBl9arjliYs\n6/OA8XlUjy8FgNYmtu7LN/J/OzWlzGxd7/jkvGGFSMuaNLomlDxBP99zvKKtWDNrQktZdrpHfj5G\nS0Dr2ZNVDNLv+fzQ2aKUoVJb6HUntGPxpGNnSRezOWld18xmYcTi1+q6pjRh9SZlS4BFu0aW9FV1\nUL6hNkZmN3qzdb0drQ2GFSKtWLXZNSEJN7utWCewywtWirCTHKMlg1LFBrNQvlbra80u9BpZ6cmD\nUitxEnUntCPT1mLPVvsxA0B4KpazmOQbvJG6XLNtJfUs/vxj9ZQYHB7HHTeuXIi/Kj+b/Huo3XNZ\nVwDbt67SvFelky+Qzfx+Vjd6o3W9161ZbHqTzr92MfkYdlmxTmKHF6xYZd8M0vxqYL2GSgYZnxuP\n/GyoKly7TmHW2jXixdCb+6VW4iTqTmiHmu1pL6onONtb/AWb+dpVHXABOHTsoiu60e/LOQks/7pW\nrCAti9+oyy0yE8NjTw0rJj+l0mm4Xa6C7/aOq5fi0LEwJqZjCDWzWNvTju239FXt5pGvqIWCDJoa\nGMzFEoZ+vzs39di20av9pvfdfgUmJmaL+p7FWKKVkmimR7EueqvKfj5mPWqNfp+u0I7FU4jFM+9x\nWnCUyhVs9D5WrV0jXgyt+VvOUsa6E9p+xmtre1G1+lklIb877/zs8LSguiCl645NzutuFi0BNufe\nWhno4Sn1g0zktAZY1a5ne189n1O2JH23reu78dCHr83WW46PRwu8DNVE/m84MRPHhOy0Kb3f76a1\nS2wrd1L7TT2e8pw3bsdnrWBkM1d6T7FVGVaVfQkjwuXHTx/D0wdyu82FpwUs6WzE+fE5pNKGH7fo\nun4rz6+EUeErva8xwBpKiJSwau0WewhNOUsZ605oA+a1bq0fKKxyLu7Q8XBRzxiejuF7v34Dw6cm\nobZWWwMsntx/SjWJTMkdabRLF+vz4PzEnOJrSnXGwMWNor3Fjx/86g28cPhM1brszPSBVyIyEwPS\n6iVcVq1Qu1zM+ZtpMZZoKRLNjAgNI++xOn5GlH0tAaUnXISEiBdeVU42nJgScOPaJdhzSLmxjeJn\nVOr6ra5Bs8LRqJDPf5+f9WBeEA3dpxhrV0vZNHIITTk9THUptM1q3XqCTulcXDvY9/qo5utNDT5T\nfaDFVAo/33Mcs7GE7r3PTczBzyg3AlGjFK04S4WZvuFKhIJ+dIYay17ulI/WZqq3Jmbm4jg9GkV3\nVwDBRib791J091ITGqKYwo7bVmu+B7Bn3qkpJ3fdvFLTOjQiXMYm51XXWiwu4uaBpXC7XTg0PI7J\nWQFtQX+mrezxsKm6fsD8WFgRjkZ/i/z3yQW23n2MeCG1FLRiDqEpZyljXQpts5hpRynhdsGUO8sM\n7TrWvNGFZBU1YW60FaedE9pKAoodipoe0sItd7lTPnqbqZIlGk8m8aXvH8SZsShS6czcXtoZwGfe\nvw6M9+IW4lSimZbQ2HPoLOBy4c5NKy0fKWp0TqgpJ/mJnfljasSVGk9qK8dPvvQWjp2eQiQqoDXA\noL+nLZMv4jG3pg/yY6bXoJGzBuSZ63pC/vaNKzAvJDX3inzC05kT5ha3N2UVz4P8qKoX0oi1W6yy\nWa61XZdC20p85q6bV+KNtyI4M2Ys6ccpge0C8PG7+sH4PKb6QFtx98biItb1deCt8zOIzAjZSZlO\np3NibxJ6rTjtjPWY/Q3Nvt+MorakoxFCPKW4cCupx7RVd+KXvn8wJ9kulQZOjUbxpe8fxAP3bXDs\neSW0hEYqDTxz8AyEuGj6SFGlxFAjLmS5cmJkTLUUQCnEdfiY+tr0uHO9bpPROJ4ZPAuPx226rn9i\nRsAPn+Rx77tXG3aTaz0/4/MUZK5vHliqGU78wqP7MRkV0BJgNPs75LPrwGnsuJUzZHyodehTwqqy\nWa61XZdC24ob7WfPnjAssAGA9bmx8crFGBoJ53SOymwSYc0+1Fq0NWfcrkDGBab0eUnrlTMxHTNt\nNbpdmc0gFGRw3RWLsm4jMZWCy+Uy3YrTzliP2d9Q7f1zsSR23MYpLjZpQ3zl6Kjm5jIXS2JgVQe2\nrl+Gtma/4rXMbgxOZOlaSZ6ZmYvjzFhU8TNnxqKYmYvnuMqdwIjX4+hbEdNHiiolhpp1IRsdUzUF\nMD/EpYTX64YYL2wRLCkFZur6gUw4r8HvNfwdtRTYWFwsOBteFFOav1ckmvm7GYENAEMjYcy8Pa5p\nfLQvKGNqHfrMxPONrsFSlzLWndC2Um9pxUp1uVy4e3Mv7t7cW/DD33WzWNCHemI6BpcBl7rkdp0T\nEkiouNQSSRFi3oV2vXJK9Zpq7m7pEhMzcew9ch6NCwtdS8PUasXJLW/V/nIGMWsxar1/75Hz4E9G\nFBe19D1v37gCX3h0f3azyUdu+Sgly5gRvvmNb1oDDAZWddhSOmcleeb0aFR1TqbSmdfftqKt4DU7\nlQ4jXo/JqIDrr1iUE8eVuOKyVtUjRZUwE8Yx2swolU7Dz7iznbf8jAfXXdGFV49PqF67Lchg9aVt\nqlazXCkwWtdv5TsCFxXYg/wYIjMCWpp8mJlPFuwzADB0fAL9vR26yohZJqZjOD0aVVWSJC/k74bO\nFZXbUK6mKUapO6F9PjxrytoQUyn84EnetJUqxMXstfK1MLU+1E/uP6U60dubc92uO586BlH5fA6I\nKeCxp4bxwT+4PPMsCVEzm/3aKy7BiTPTOS5QJfIXulYrzsYGBi8cPouJ6RhYJvP+FzUEpBnMJqDo\nJZXpLepgI4OrV+u7yuXjY3Xh53sEJIVg5Mw0Pnfv+qI2DSvJM91dAdX8DLcr87ocpza8bVt6IYop\n7Dl0VvFZQkE/3ndLHxr8XhzkxzAxI2Sfe+j4hCmrbmLaeBjHaDOjfKs+FhcRT6Q1BdAn3rsWnaFG\nHH1rIqfUUMLndSOg4uXYtqUXc7Ek9hoQ+GZwuTL/OxdXFtjStW/qX4x4XMQbJyMLQt6cK1zt3i8d\nvaCqJLU1Z5SkYnNqytU0xSjlVxtKhJhKYeeuYTz4b/s0S6jyrY3Hd4+oTnwtpAmkh9wi2balB8sW\nNklASvhpwhc/dA0e+vC1WSsOLnrbAAAgAElEQVRXSIg4+pa6hg4AR09GICQy1rOe0Lr5qiWYM5BR\nLi10PTxuNz58x5V46MPXYuOaRVkXWhoXF8Dju0d0r5OP9Bt+4yeHTCWgSNaQHoPD49kxy2fbll5s\nXd+N1oC6K1g+PtLCD08Lhr+3lkfg1GgU//7fb6g+n1Gk79He7IfblVEGt67vzkmeERIiRiNzEBIi\ngo0Mlqqc476082IWufSZnbuOmf7eRvC43dhx22psGliq+PpAXwca2Ywn6MqedgAXFQ2zwsLlAp7c\nfwpiSkUrzkNrTIWEiIO8chXIG29OqM5LKQzG+jxoalCec0IihSeeO6HwdxHhqRi2belFu8r1zYaq\n8udzPKHuEmR8bnzz50N44ch5RKYFpNOAC2n4meI8Lqk08LtD59DoV87sNppTo4WeF6/Y9WcHdWNp\nG0lemI0l8PM9xw2VakgsamvA+YnCQ9v10v7Vuh/lJ/ycGZvFnkPnsrWcmUzTFCIKmrec8LSQzbbU\ncuG1N/vhcbsNN1wxG5NWa9BiVOuVKzX5B1MooTTuRpPKlA5Ukbt49Vzl0kZoxX0v/a5aHp19r13A\nsVOT6O9pz8bPzaIV2lCzkv/2ngH8/Q8HFbPHM4rUMRwaHkckKmQtsXyUspatuNC3b10Fj1s5n0JM\npbDzqWE8d7i4kkspuc3jdhmyrHSbGams1Ug0jhvWKLv0pVCSkBAxO6++1geHxzS9Ow2sF0DhnDJT\nlmQ2PJjp0JZ5Zkm0R6LqRsGyroBmyVs+s/MJbF63NCdfyEpOjdL8q5Tz37WoC6FtdNIJiVS2Tec9\nt3C6FirrdeP8xHz2jNt4QjSc9q/kglHbsA/yYxBT6WwTlVCQUU1CkyNlW7I+j2qMaaCvA52tDQgF\nGdXNRWJOSOYoNXoYWQD53dwklFqIzqnUcAKZBBTJDavEXTevBH9yEqdHo7pWupaLV8tVLm2EoxH1\nrnPyha/0HRmfG3GVc8mBi30Anhk8i/ZmFjesXYrbr19e8HvI+1fPC8mC8VUKbWi5BR+4bwPCU/Pg\nT06CW96K9pYGiKkUHvzuKzmKZlplcOVZy9K9rLjQ84+EhcuFztYGeNxu7Nw1rNsjIRRgMTWbqYRY\nszKEWDyF/W9cUHS5m437Ko1pA+vVDC/ceXMPGvzebF5LfiiJWx7SXJcTM0J2Pin9fpLAlp5BbZ1o\nKVDF9iyQ42c8aPJ7c6pR7rjxMnz+3182LLQnowJuu2aZYr6QVk5No98Lr8elub6roS1vXQhts5Nu\n76vn8d6btX9AABCSmc1VSi7ZuGaRaiZyzudMaq4TM0KOwNUTrhJDI2FMbhTwk93H8MZbGYtXafF6\n3G6svrRNNwwQi4umYjtGSl3UurkptRBVQ0pA6e4KKr4uJET88Mlh3Zi9ZN3oxbT06jONLnwz31GJ\n8LSAXzx3AnPz8ezvIa9hnZiJw+XKCNL83zufOSGB54fOKd7nID+GRDKVmU/Ri79VUhR1x1SOlLUM\noOhEoZ/vOZ6z6RrpQtje7Mfn7l2P6HwiexytlmfDDstqXkhqJvLFEyLu3NSDm/oX41f7TmLf6xey\nr4enBew9ch6szw1BRZFrC7K63h3pXgDQ39OeM8ZqAuyOGy9DdC6BBtaLeEIsqmeBnHhCxKfvWZdT\n162l5CohrSGtnBr+5GTB3Dw1Gs2GadTm352berB6eUjR+1Guhkj51IXQNtsoIxYXMRaZQ3dX0FRT\nlVf4Udx1s7ZmbjWxTQm3G9AKu4WnY/jkP72Qs2moLd7tt6zCweExQ9quUQtEyy2t1c1N66ANJeRl\ncHLyBZgakqfkxSPncfStCVWL3uhRk0aSk7Q2WbONeeTPla8ISJavnmDc+dQx1d9+YkbIaaEpXYvx\nqvjCNTjIj6m60LVCB/IxVlKqjHQhHOjrQLCRwS/3vmkos1muYMXiSUM1v/nP3cB6VY+tZX0u/Obl\nk3h1oauZW2VcXGoDhoxVaUbwDR0PQ0iImmO565XTeO7wWQiJVHYuZtZIIX7Gk/UwanVok5A6BcrH\n0Oz+rCc8k2JaNUdHa/49P3QuW7VhxXtaKupCaFvpaCb9svlWVbNGFmQ8kcL9334RN161RLPBvZXE\nNiWM5Mmobf5DxydyFm8j68MNVy5SbJqSz8R0DCfOTGHl0hbFxSMkRJwbn4WYELFtSy9S6XTOISOs\nz42xSeW+5noHbSihtoiNdoCLyWpgtYS7maMm9azxqah6OMRsY56J6UxXqs5Qo66yoyYY9RIblYgn\nzXcQiswYs2y1LECzyo7bBWwaWGrqlDsgM6+8Hhd27hrG0PEwxiLzlpr4qJ3UJSTSOQ2S1H53IS5i\n4xWX4OCx8ewa8jMe3HDlIkPeHTnh6YvudK2xkCx76ZnkpWpyQSZZ5JJyE0+kFK1Uif6eNss5J/Lf\nUQvtsJz6+MhrzqXvu25VB/70Xasd70VghroQ2oC6y0QJP+NBZ2sDgIzWtvXq7pzWew/8x8uqm7uQ\nTGk2uFfLJJXwuF2qpRR2ElYoazF6V5cLePjHhwo2sJwNa0ZAWzDzeiqVzrHi1Fx9gP5BGx53xrUu\nj4kpLWKjm7MZq7a5iVlI7NFHKfba0sRkTz3TinWaJQ3gkZ8NgVse0t20J1RK4vQSG+0iFGThcsF0\n6EDeDEerO5oSm65agh23cpnr6Jxy53IBbbJ5ZbT8R7Ks88s2pVyVYtZ1W7MfO965GjveiZw4vlz4\neT0uQ8d4ul2ZOLuQEHHizJTpWHUj68Wnd1ydc/9MWdvFZEQ/40Y6nVnnLmTm58XyuzB27houUHok\n5X7P4BnVUlb576hFoNGnmvPT3OSD2+3WFN5yDh4bx1sX9lOddjnQcpnkc8OVi7IatlKygpH4r5JF\nMxUVNC25das6MHjMeBOIYnAvlLVksnGlTHlj906puFy1Njij6B20IaaAKy4L4d3XrcjpdRyemsv5\nt9ENycw+OhmN48Hvqi/gfFeuFHuV3PPyfAJuecjWVrdS/FNPEWhtKqwAKLbPuhnWcZ0AYDl0oNX5\nrC2Y6YallFUs0cB6M0qfQvZ/W5DFJ+5emxVIM3NxHDiqXQXg9bhymuGoubiLUcTlnqT8vA3pEJeX\nj44aMkhS6YxCJHVMc7vUkweViMwIQDqdU3GQn4wot1Ib/V48/+r5gj1DFFPYPLA0q8zOC0mkUmlF\nge12ZXpJ3H7DCkMhiiee+71qqGdqNqHq6lej0uq060Zoa7kjJaRSlvdu7tE8VeiWa7pxgB/VtBjz\nXchCQkR0Pq66qboAvG/rKrx1YcbQc+rtAZKGq4a8rOWum1fiwf94RVf7VLuvdAhAMUdZSqxZcJ/d\nceNleH7onOLie37oPDweD7Zt6clRrEJBBk0NDOZiCd0NSTp05fDIuKIixXrdaPB7C0IhSkcqTkzH\nsolNcgUvv0e7fOPae+Q8GK/LkotZC715cWVvW0GM2FL4SAF50tvFvt6FAlRMpcCfnMyWkLkALGpr\nxDs3dGM0Mod4MqWqcGl1Plvb247brlmGO95+WaaETkyB8XqQFNMALnqB1DrbreM60d0ZyPYD0Gpf\nq3ainZ2KmLQf3XXzypy/CwkRY5Pz+NdfvIaz47Om7yk3OMx+VvLqSIrrzqfUEzwPHhtXVWKkCggj\npNLAi0cu4KXXLigm0cox4mGTu/rNtJF24sAjK9SN0DbijpQOQnj86RHVTNRnFyabXhKO5ELOFyRq\npJHRxo1snkYWmtG1ODg8jqNvRXBO5exsiUCDD9F5ZU9FZEa7vaAZEguKUHQuobqgJIVj5PRUzoYx\nMRPPEcBq4yTP8j925iVFoZ1MpTAZvZh9nY+8DC//d5UEu14ziUyCkfOhEIlAgxevnZjAc4fOZTOu\nt65fhpYAi4SYAut1ZysizCBZqJLFJPd4bFq7pMCd+/jukdwyMWSOgv3kt/YBAEIBH9wq7uSWJibb\n+Ux+pGJTgw+HR8aza1OUWW2sz42OVj/OjGnP8aMnI4gnk/jZsyd01yDj85g6pcoK0n70vd/weP9t\nqwus+nIhN2D0PIN2KjFqHj45ZiqF/D43GK8b03PGPLBKJ5qVg7oQ2lKs1egE+t2hsxBV3iv9Wc9C\nkvftNlLK097MLnRFy7RrfHbwbNHb+dKORoSnYzmJVvmEp2Oq2rAcNYENZFza3V0BW1ys/EInt5YA\nC48bqvEtINP32gzyw+2lkMD4ZGFjHODifbXqjvUykPW0eCGRwuK2Rl2FyQ4WtTXi/MQcovNJALn1\n3nrjrMfqS0NZoRxsZLKWqlJoKSmmdfM6tBpxRKJxfOVHB/E371uLm/oXAy4Xnj54GntkVlv+2hQS\nKV2BDQCnR2fxxe8dQExI6r43nU7j9+embatf1uLFIxcwfHKyoPlSKdBSKwePjRfdmrQYlCzfBtZr\n+PSwyVljwlpC6USzcsS560Jom83YVhPYTtKwUPgvJFKYisZtsb/OjM9h88AS3HDlIvzjfx5RnMha\nFrRR+nvaMC8kccXKNvzukHKtr1EiC80iGlivriAxO0asz4PbN67ILrJMFyZr0squJLKeZc2YmIlp\nhlqKJRRgNNsvmhXYLQEGU9F4TqmcvKe8Vmjptg3LTdej53NqNIq/+uZepFJphIIMJmftExxGT/IT\nEil846dDpmPCVtFqvuQUPq8LoSCL0UhM8fWpaBwhlfyAUqBWbeCUIqF0ohlQ+jh3zQttKyd0lYPT\no7N48LuvYGxyzrIgUeLwSBh3b1mF9au7FF1+wSbzQpvxupEUU2gNsGj0ezF4bBzPDp5FsLH46SRl\nEJ84M1X0tfKZmo3jC4/ux1WrMm7huMopaUawy+135PiEZgc0NVqbfAg0MjgbnkUqlVEiGv3erCUt\nx894bbXmPQueGfk8lQtmrdDSGyptbc0iuc6LVQCKpQSFHiUjPzySSKZVBTaQUd6uuCyE51+1p4TV\nLFrVBqWiHHHumhfadrbgcxonXF9Sm8P8uuHWAIuVS1tw7PSk6WvGkylce3kXTo/N4rTMMpme03cr\n6iF1JesKNRR9LSUi0Ytu4bYgY8k1zHhdcLtdtihXkxatlZn5RI57L5UGovNJLOsKYC6WzCZ/Nfq9\nts8rLUE5eGwcUyqWThrA+bByOIIoL67s/zPOZDSOF8oksAGgvzdzMMzp0ZmyGWbl6Ede80K7lOUs\nlYgLmdpvqW743dddih88eRSvvxnBK0e1Y4tavPS69c+q4XHn9lx2GqtWWiZmao+JFQr4cFWf+bOH\n1RSNuVgSn7t3fbanwIPf3W/DUxpnKhpXLakiKpc0tPsnaH2uXAhxEZ/9zr6y7u3l6Ede80LbrnKW\naiUN4IvfO4CBvnZ4PW688Oo5W93vdiIJIidLopTwMx40sh5EZuJoDTBoavBhdj6JyVkhe8BEMYla\nWkSiCfAnI6pZ6mYJT8fw6vEwruxp1zym0ClCC7XSZpUQgjCDn/HY1lmyGJQ6vDmNK12KLIoiGBub\nKfoBlRoAEISEC8AD910Dj8edc5BEa4DBqu4W7FdpsFHJuAA0+j2YF8SSxl39jAcbr1yEoycjOGsg\nY5sgrFDaYsmLNPndmI1d7PSmdxCPVTo7g6rBCkOWNsdx63ieP2jbE5UYM93QiPojDWD3YMYylPeC\nnozGq1JgA5nvNBuznmhnlVhcxO4DZ1AB3R7LtrETzlOu33U2lsq5fzmyyI0urR85+hQOU03JaER5\neHbwbI7ArhbMtmQsFUYOs3GSgN9HApsoGYPD45pllXZiNKY9xHHcdgDPA8j6mHmeN380UBmo92Q0\nonYJNjGIxdXLcuqVKHnWiBISno5hYjqGxe1Njt/LqJr+hwB+COBNAOML/1c1fkOvx4UGf83n3JUM\n86coE04xplFHSxBE6dh1oDTJzoYkGc/zfqcfxEke3z2C06PGOh0R+pDbkSAIIpehkXEIm3sdzyY3\nmojmBvBJAO8C4APwWwD/h+f54rtpOIyRM6wJgiAIohjC0wLOT8zh0kuC+m8uAqPu8S8D2ALgEQBf\nA7ARwMNOPZSd6J1hTRAEQRB28PBjg47fw2ig950A1vM8nwAAjuP+G8Bhx57KRloCLFqbvJicrXin\nAEEQBFHFzMaSCE/No73FmTbMgHFL2y0JbADgeV4AUBXpmazPg5VLWsv9GARBEEQd8NrvlQ/MsQuj\nlvYhjuO+DuAfkclD+iiAIceeyma239KLgzqHtRMEQRBEsVzicNmXUUv7LwGEAOwFsA9AJ4CPOfVQ\ndvObl+uz7zhBEARRWlYsanb0+kYt7ft5nr/XyQdxCiEh4sDRC+V+DIIgCKLGYX0ux0u+jFraf+Do\nUzjIVFRAJFoV4XeCIAiiihESaUw6fCytUUv7BMdxv0VhG9OvOfJUNtLAUic0giAIojT84Dc8PnZX\nv2PXNyrRpB7jl8n+VhWNseYFKvUiCIIgSsOhkczhIU65yY0K7fM8z9/vyBM4jCiW+bghgiAIom5I\nAzg7FsVlS1ocub5Rof0HAKpSaB87PZXz73/7tw8rvu8/1/8RfnXVuwEAf/3rr+PyM68XvIdfzOGr\n/+OTAIBbh36Lu1/+qeK1/uID30LS40P3xGl84T8fUHzPN2/5KA5fuhYA8A8/+hu0zE8WvGf35Vuw\nc+P7AAD37fkPbDy2t+A9F5ovwWfufggAcO3IS/jws/+meL9PbfsywsEONMWieOSHf6X4nh/ccA/2\nvG0TAOBz//UQloffKnjPwRXr8K2tHwEA/PH+/8S7D/+64D2Cz4+//NNvAgC4szz+5lfKzfO+fPun\ncPySXgDAvzz6EXhShV6RX6y7Hb9Y9x4AwMee/CbWniqsNDze1YMvv+dvAQDveO1pvO/FHyve76Pv\n/38RYxpwyeR5fOlnf6f4nn9+x1/gwGVXAwC+8uP70R4tLBX8HXcTvn/jDgDA+5/7AW7if1fwnnCg\nA5/6ky8DAK7+/QF85OlvK97vM3d9ERdaF8Efn8c/fv9/Kb7nsev/BE9f8Q4AwP2/+Hv0jB4veM/h\nZf345m2Zgo73HPwF3nPwlwXvEd1e/Pl9/wwA6Lkwgvt/+RXF+3313Z8Ev4QDAPzT9z4GNlF4KMmv\n1r4L/3nNHwMA/ueuf8a6Nw8WvOdk+6V48I8+CwDY9MYe7Hjhh4r3+/g9X8esP4D2mXF85XHlbeY7\nN38IL/VeCwD40k8+i0umC5NL967aiEc3fQAAsH3vY9jy+u6C90w1tOJ//z9fBQCsfeswPvbUPyre\n7wt//HmcbuuGV0zg2//xPxXf85MN78Vv+28FAPzNfz8M7hxf8J7Xl16Or70rs97efehX+ONX/kvx\nWh/60HcAAJeOvYm/+/++pPier7/zE3it+woAwCM/+ASahMLzFH675hb85Lq7AQB/tvs72HDi5YL3\nnAktxefv/AIAYOPwXtz3u/9QvN8n3/dVTDa1onV2Eg8/9jeK73n0pg9gb99GAMADP/8ClkbOFLzn\n5ZUb8K9bMvvu3ft+gluPPFXwnlm2CR/f8Q0AwBWnX8Nf/eYbivf74h9+Bm91rgBQ3r1c+r3UuBCZ\nL7vQrtqY9vjkXLkfgSAIgqgjxqbmHbu2K53WD01zHKekiqV5nr/P7A0XDh/5FoC1AAQAH+J5fkTt\n/WNjM0XFzv/qkWcxNU8ucoIgCKI0cMtb8KntV1v+fGdnUPUEZKNHc34AADiOa+V5vtCPa447APh5\nnr+e47jrAPwDMud1OwIJbIIgCKKUnB2d0n+TRQzVaXMc18dx3OsAXuM4bgnHcW9wHLfa4j3fDuA3\nAMDz/D4A6y1ehyAIgiAqjpnCNBDbMBrT/kcAHwfwf3meP8tx3DcB/CuAmyzcsxmAXA0ROY7zqp3N\nHQo1wut1tsMMQRAEQdhJZ6cz52obFdrtPM8/xXGZjFKe57/FcdyfWbznNAD5t3GrCWwAiEQokYwg\nCIKoHpa2N2BsbMby57UEvlGhneY4zo+Fhiocxy0CYNX8fQHA7QB+shDTftXidQyx8pImnLhwsTSC\nSr6o5ItKvqjkC6CSL4BKvgBnSr76ezo0Xy8Go73HvwXgSQBdHMd9GZmTvr5l8Z7/BSDGcdxeAF8H\noCxBbGKNg4NHEARBEPksdvB4TkMlXwDAcdxNAP4HMhb2kzzPF6pLDlBsyRd/KoKv/GjQrschCIIg\nCE3++u5+rFlp3WAsuuSL47jlAN4E8E8Lf0pzHNfO83zY8lOViCUOH0hOEARBEHJSDlYaG41pvwBg\nCTJJZGkALQCSHMeNA3gvz/OFwdYKgQ4MIQiCIErJ0k7njEWjMe1dAD7A83yI5/k2AHcD+C4yPcm/\n7tCz2QIdzUkQBEGUEjHl3CGYRoX2Wp7nvy/9g+f5nwO4muf5QQCMI09mE2RpEwRBEKWiLcigJcA6\ndn2jQtvLcdwa6R8L/+1ZKAPzOfJkNtESYBEKVPQjEgRBEDXCOq7LsbO0AeMx7b8F8CzHca8hI+hX\nAdgO4AFkSrgqFtbnwdWrL8GuV06X+1EIgiCIGmZRqAF33bzS0XuYKflqQ6ZtaQLAXp7nIxzHBXme\nt972xQDFlnwBgJhK4bGnj+F3g2eQpPNDCIIgCIfYur4b27f2FXUNrZIvoweGuAF8CMAnANwP4GML\n/cIdFdh24XG78b53rEJHa0O5H4UgCIKoYQaHxyAkRMeubzSm/WUAWwB8A8DXAGwE8FWnHsoJdj41\njPMTzh1MThAEQRATMwKmooJj1zca034ngPU8zycAgOO4/wZwGA63ILULISFi8FhhD2mCIAiCsJNQ\nkK2I7HG3JLABgOd5AZnYdlUwFRUwGY2X+zGICsVtdBVUOF4XEPDTMbYEUU7etjxUEdnjhziO+zoy\n52qnAXwMQOFxSxVKS4BFezOL8LRzLotaYB3XjoOV35nWdnweNwQn+w6WCL/fi+h86foSMD43OppZ\nnA3XZtjpklADLkRq87s5jdsFNJZ4PhaL2wUU2xPFz7jxvluKS0LTw6iN8ZcAQsi0M90HoAPAR516\nKLthfR6sXaXdvN3tArw1YnGZxe0ClnUFEGyoz3p2IZGCr8p/e6/HVXKB/eB9G4oWatde3oW///Nr\nEXLQnWiFRtaDpOiMIqeaFlwjBBt9+OIHNzhqbTrB0s5A0de4uq8LjQ534dS8Osdxr2LhDG1k5trY\nwn9fBWAPgH7nHs1e9BZKa5DFmstCeOOtSYxNFp4fXG2sW9WBgwbj+Kk0cGo0itHInMNPVbkkKszQ\nduHiwjNCUnSubaLi/ZIpnLoQRbFy7aXXRxFsZHD16s6K6qWQFFOYcMgzV9pfyl5cLiCdBhivG3GV\n+tlrL78EYhqOjZ/dtAUZrL60Ddve0YtfvvAmBofHEZmJIRT0o7+nDYdHxjExox9e9TMex61sQN89\nXjXWtBZCQsQhHQE2MS3gd4fPAwA8bhS9GdnJko5GxAQREzP6i6C92Y+Bvg6854ZL8fo/RxCLGy89\nECpNctUxlb6xtzQxCM/Yo9wODo/jgQ9uQEIUsWfwnC3XLJZ4Mo3WAFOSXJi2IIumBh+icwIi0dKk\nCjX5vUgmUxBMNq6Q2nqoCWzW50ZSTOEbPzlkeg673UA6lVEMHGzdncPitkbEkyJePHIe/MkIBvo6\n8cAHr0F0LoGWAAvW54GQeB17j5zXvdbb+xc7bmUDOkKb5/k9jj9BCZiKCqa0Pklg+7xuJMrcjSXQ\n4MUD923Azl3H8MzBM6rvY31u/O09V2NRWyNYnwc7dw2bEtjVhFkrVE53ZxPmhSQiMwKam0qzKVcC\nXo/LVms8Eo3jx0+P2HOtmRiic3H4PPa6U1mv27RQkrN6eQj7Xr9g4xMV0hpg8PkPXINgI4M5IYnH\nnhrGqyfGMT3nbKhjXkgitWAxp9Mp2FVWLCRSeHbwrKXPplJAWzOLRtaL02Oz9jyQCu3NLBr9Ppwa\njWb/Fp4Wst4eeXOU7beswkuvX1A9BKS9mcVAXye2bel19JklqjySZ4xAow8sY35D8LjLH31ifR7M\nxZI4PKLtKUgkU2hgPAuaoYjB4THN91crbUEWSzqsH3t3emwWb7sshP+97Sp8ZsfVaG+unFhqMbMt\n0KCt4RcjsJ22HkJBPxpYr61zNtjowwMf3ICt67vht7D2WcaNe27jsHHNItueSYnp2Xj2UKMnnjuB\nF46cd1xgAxct2XjSPoFtBxPTAk6PzWJpZxOc2n43rlmEz917DeZiyl6NweFxheYoyuuH8bnxuXuv\nwfatffCUqAylLoT2E8/93pLVGYuLuO7yLrQ3++F2wdLiBzJZqIzP2gycmBbwvV8f1fUUhIL+bG3g\nVFRwJFPez3hw3eVdtl/XDE0NPpwZL04Lf/7weTz840P4+x8dRKO/cpLvzIhV1ueGywV0hRqwdX03\nvvKR67GsK+BIktOcwyflDfR1YF5IWoqBsozyFjYzl8BXHxuEKKbQ6DevdEgJRTtu41QVu7Ygi+4i\nz01uDWRqemtZ0bZCTBBx49oltl/3hjWL8IF3r9acb5GZWE5zlJ1PHVMNlyYSqZKfJFnzQrvYxTB8\nahL9PW344oeuxcN/eQM2DywxpQF63C6MRubRxHrBqKSnsz432lQ2BsbnNpRQNtDXkc3WbGC9ms/I\neI19AbcrE19qC7LYuGYRHv7LjfjTd70NrQFrp7Eava8SzY0Mblq7SFU7NksaGXfYqdEolnUFELL4\nncoB63NDSKTQ0sRg/dsuwbYtvWhgfHjgvg24f8e6cj+eKRivC0lRRKDRp7oGtAg2+PCOq5eivdlf\n8Fp4WsAzg2dNKwN+xoPtCwlFWpUnl68MFb1hzwlJ/HzPcUxMxyo2caulyWd5zVslMhPDrdcsw9b1\n3VmjiS2yxCMUYHD3ll6Ep2JoYL2q801SpICM/Dj61oTqNduanW2kokTNC229eLZeqcnETBzPDJ7F\n0wdPo5H14rYNy6F1xoprQS5JAlpMpZEGEIkmVJM3bly7BOv6OhVfM5IctnHNomw8RUiIOD0a1Uzk\naGR9ePCDG7DpKm1NNrQhBvwAACAASURBVJ0GPrntKnzpz67Dh/7gcjSyPrA+DwZ0yufUWL/6Ekuf\nc7mAmbk4ho5POOJBmIsl0d9r7Ttp4XHDsvud1ag/lObEZDSOX+19E4/vvhhbftFAwowSN61dhNYm\n6xuzVVdmPJnGs4Pn8JUfDeLKnnbTnx+fEuByufC5e9errmWzz5afUKT28ZdfO1/0fIzFRex65TS+\n9cSR7N6RT0uTD5+8e21R9ymGa952CdavLs7D5mc8cLsyibLLuvRLq0JBP9qa/di+tQ8PfHADNrzt\nkqLzi6bn4vjUt1/E/f+yDw9+dz8aVMI+kiIlplKYigqIaGSOr3a4kYoSzqe6lZmWAIs2lcYq7c1+\nfO7e9YjOJ7DrwGkcPjaumqG999XzeO/NvZrXA4DWJh/6lrfh2OlJRWXBz3jQ5PciMiMgFMxkessT\nGKRyA8bnMezSl+L1O3cNY3B4THcjmZqLg/G68d7NvRDiIl5+44KikG9r9mPl0paCSbn9lj6MnJnO\nSeLQo72ZzVovRjIx5UhKklNJYxPTMQyNKDeVYbxuNDV4NReuGqkU8PG7+uHxuPHPTxwxnFzT3uzH\nZ95/NX7+7HEcPRnBxLQAZsG6VmJweBx3buoBAAwd126OI1npUiMJKYnmrptX4tipVzA5a22Mi832\nPTUaxYUJa2GPweFx3LR2CSZV+j2bebYbZAowoF15ErfRK3pGY27E4iIe/slh3WswPje6WhtsS+Jq\nCTBYt+ri/pRKp7H31fPZfUkK0cTi6sLU7QI2DSzFnZtWZjOyvR4XHt89gsHhcYSnlSsQ5J7DJ547\nYUtCoJgCxIVnz+yRyvNFUqQA4M5NPar7falKvPKpeaHN+jwY6FOuAR3o60CwkUGwkcGOWznccOUi\nPPS9A4rXicVFjEXm0N0VVL0ekLGoX9KYYPGEiE/fsw6Mz5MtKZDYvrUPd27qwVhkDo/8bMiw0JYE\njlZ2uZxQgMWT+09haGRcUyDIF44cj9uNz927Hjt3HcOh4XFEooJuN6FGvy8bI5QEkRp+xgMhLpas\n9KMlwKhu+EkxhT+9jcMjP3vVdMZ6S4BBS4CFx+PC+JTx8qiBvg60Blh88A8uh5AQ8cMnebygoejI\nY3B6LtYmvw+fef9atDQxmBeS2Tm4c9cwzk2Ut04/nlQfYa0yzPB0DGIqpbq5hgIMuOUhDJ+K6Nbb\nMnl5K07lh5jBiLeN9boRT6QwF0tgWVcAc7FE1jBo9HtNKdhApvxqOhrH0PEwPJ4RbNvSi3tu4fDe\nm3sxFpkDXC50tjbg53uOa9bXb7pqCXbcygHIePgkpL1uYjqGXQdOY2gkjMhMDB2tDejvac/xHB7k\nR009u11IyrDafl+qEq98al5oA8hOAHnRfL6FCwCMV8fNseC/kj53kB9TtczVhFgo6EdnqFHVpcL6\nPGB8HlPxrYnpGA4NGz8QpanBlyPgpU3Bz3gQT4iq4yPH43bj7s292HzVEkzMCHjkp9pdbWfnExAS\nIlifB+tUFoGf8eDt/Ytxx42X4a1zM3j4x4dUrxcKsJiaFdDU4MPMXHFx7oFVHRg6Hlbe8IN+XLak\nRdO7osZkNI4Hv7sfPq9bUwFjfZnSwtYAi9WXhnDHjZflvH70ZETzPvIkRL3njMwIYLzurLIKFJ/3\nUQrE1EUvgRK/O3xOdXOdj4t46fULCAUZXHv5JTh0bEz1Os8cPAOP25Ut+ZHyQ+xSHp3qASGVtk3M\nxDExE8dNaxfh3detKLBsIzMxQ6WOUldfqQxKFFO4bcNytARYdHcFs+/L3wvzPThaewjr82BxexN2\n3MpB2CxiKiqgZ0U7ZqYudtmbigqGGps4gaQMa8kPIZF57nwDzEnqQmh73O6sZudhfBDjiYIBFhIi\nkE6rbgx+xoPOhfO4pevdtHYJPv/vLytaYGqLXM16laPngi98v7qlCADNTRnB1rbQ4UfNhdrIevHp\nHVejs7VB8xnFVGphExjDxLSA1oAPbrdLtY4RyAiLE2emsHJpS8EikITV9ltWZbXxlUvVBaUU1pgX\nkvC4XfjUt1+0tKlKrrvtW1fB4xnR9MZoeVek5hhzsUTB8+r9hm1BBp/902uyrnB5k4dtW3oN9RiQ\nzymt5wQyoZT8xBkzfQw8Or+zs6jfd2gkjAc+uAFAYYhJUpgmZuKaXjAJycJifZ5sPXOxhAIsAo0+\n0xavGi5X5ppzQlJRIXx+6Dw8Hk9mbrvduHNTD27qXwy4XGhpYvDgd/crzk01BWXPobN4dvAs2mTC\n2ON25+ytU1EBDaw3x4NjFNbnQVeoEX7GixnZ3+1WmswgKcP53zFXEcrsgfnj4iR1IbQlWJ8HnR1N\nGBvLTAshIcrcMxddxUrccOWigknY2dqgKljagizWrurIun2MWK/y59TbfOVoWYpyAdcSYDEVFVSb\nH0xGM1aY3mJ7fHeugDPSxcnlAh7+8aGcyS1fBPn3NBrWADI9g61shnLX3bYtvYoxu3Q6DTGVUtS2\n+3vasHX9MjBeN0Yj82gNMPjqjw+Zir33LQvhV/veynF/y5s8aMXU3C7gndevwB+9fUX2b9u29EIU\nU3jGRIMLLSXR7cqIyjaLblY7ERJaSmEME1Pz2Hp1N27fuAJTUUE1xKQnBCQLqyvUmBmbIFOUtecC\n8Mk/WYt/eFzdc2SGtiCLTywkpn3+319WfE8qnfEauFyA2+UqEC5XrerA0wcKw2lq4yL9Xa0BiSR0\nAWTXpR3YpTRZId/Akn/HnbuGc/YmtXFxgroS2hJySzF/o5KsbNaX6a3bFlR382gJlnVcJ7Zv7cu6\nfSQNNCmm4TGgiCkJibWr2uECcOhYoSKgZin297TlCDitDVruZlXDqitVbdFLi0AJo2GNz7x/Hb70\n/YM4PRpVtcX0EgAl7Vi+yQuJFJ4+cAZpAPfcwhVo2y5XGl/6/kGcGdPO1tdiy7ol+JdfvK74ml5M\nbdNVS/CRO9dmlVDpe9y2YTmeHTyrOBbxBXeefNy15vGmq5bgtg3L0cB68eB39ys+p8edKZOJzAho\n1bD+JAUgFGDR6Pdidj6JyVkBrMGkSxcyZwREFEJSjM+DR342lBVM3PKQqvdA77eSrwPW58E6rktV\ngQ40eMEuhLPULptGJlRil5t3HdeJ7s4AhISo65GTK6HAxfW35eql2Lq+u0AJVVP+85F7I5xE64TG\nUMCHq/q6cgyjNT0hvPTaBdXkOC1PkdHwoNYeWIpxqUuhnW8pKtHk9+Ezd6/VdRXLYzoZgcBiHXdR\nyHs9Luw6cNq0G0XJJSM9x103F8ZR1GJLQ8fD2LlrOHs/PQtWb7KZbQmrZtUYmdxaYyCH8WZavc7M\nxfG93xzFQYX4/tv7F2teR0iI2Puqct9rqXKA9XlytO3PP/pyUZann/HAz/pUxzM8HcPEdMyw8iJh\nRTHTuofH7cZoZE71OdPpTJa8lFyplpwkKQAXezpn5nGgkcETz53A4PA4JqZjmsLv8ktDikl5cjd4\neFrA3iPn4WeUlQHJC7b31XOKobC1q9pz5se2Lb1obGDw25feKrhedD6Ja1Z3YfPAUnzjp4cVBXNb\nkEV3V6Boix3IzW434pFTU4YOHwvjoQ9fW7Am8i1INeTeCCfR+o7z8RQ8bldBr3Cfx6P4/qWdTbj/\nnnWYisZzkt+kuX7HjZflXEcNrT2wFONSd0I7Fk8ashSNuoolpBrL/FrLfAXBrBtFLiS0/iYJODGV\nxjMHz2i6s8wKATlm4u3BRh+iKkliZia30vdVotHvRWuQhZ9xZzVtP+PBDVcuygofteuMReZUtXN5\n5YDEzFwcZ8aKcxVvvHKRZogFAHYdOI0dtxZa+Vrz0opipqcgaf3urQE2J7lSTwGQP6f0e8grJ9SE\nX0sTgztv7kGD35uTD6Fm2adVGipIXrB0Oq0YKsovl/a43djx7rfhhcNnFO8zdHwCd29ZpWqRr+M6\nEWxkNC12I4QCDO65jcsZQykcsufQWVPeHvn6k6+Ji8r/qKaCYcQrZxfSMz0/dC5n/OWlWfK9VD7/\nJqZjaAkwGFjVge23ZFqNNrK+nOQ3+VyXZ7irUay3sljqTmhHpo1ZikYHX0so37mpp6RuFCEhYkil\nR7n8fkYtWCXMxNvXc52aWdl2T+7Hd49gd16cLhYX4XK59JND1DpbqLyu18BGzsYrLpEJmlxvjMft\nRn9Pu2oMemhkHMLmQitfD6uKmdo9tH53qRmFXnKSXmiI9XnQ3RVUFW5Ts3F86fuv5JzEFE+mVOO6\nkhWtlNEsJES8qpKQeehYGHfdLOasCa19w0iWMaCeNyF/Vi2uXt1VsE49bjd23LYacLkMl3wCQCio\n3clrNqZdhG7EK6eFmaxrKZFucHhMUWmS723Sde/c1KO7v5lZT/mfK8ZbWSx1J7RDzcYsRSODrx3b\nGMNNa5dYdqNYKSUw67axOmmVsr+l7GkpZnzD2iW4/frlmlnZVie30tgUG2dqa2ZV413yygGJ7q6A\noazWtiCLHe9cDdbnwV039yr+plvXL1MV2uFpwZK7zarg1MKsxWM1NCSfX/nNN/I9R0biutJv1N/T\nnn2+8JS6u19prWjtG1pZxvLf2eN2m6p1NlOCmckSdxku61Lr5KUXOpQqLqyeaJVfeWJ0TujtbRPT\nMTwzeKZk2dzFeCuLpe6Etp/xalqK0nnURgZfq/FCeFoA0mnTbhSrkxoondtGbXOSC9PuJa0YG5ux\ndXJrjY3Wop6Y1nfFP/Hc71UTVJQqB4KNjKGs9XVcZ/azakpSoMGnqgC4XVBtt2gEq4JTCTMWD2A9\nNCTNr9s3rsAXHt2PiEI5o/xeRj0/Q8cnsr0CzK4VrX2j0e+F13PRE6OnDEseBQm1NWI0xgooK2lq\nZV1qnbyMJJmm08Bt1yyzLAitzgm932vXgdM5ngal6+oZQmatf6veymKpO6ENKC8SqXynrdlvaPCF\nhIjofFxzs20JsKbdKMXEwFmfR7WU46q85Bo7yN+ctGLtdkxuvVCE2qJ2uYAn95/K1qzmo7VZ+RkP\n/uimHsXXpKx1efa4x+1CKp1Gm0HlRK9XfCqdKXuxWkZTbE5FPka9OXZk2M4LSdX+A/J75ccw1Zwf\n8s9YcXFu29IL/uRkgaJ2ajSKx3ePWC710VojRmKscuRrUKuTV+Ygo7mcexlJMm1rtq78G5kTamj9\nXv09bZphwTtuXLmQ6KisuBZjKFn1VhZDXQrtYgSJVrmYHGmzNWNp2rHRaWXelpNiJ7eRsVFb1FLN\nqrzTlRytzSqeEBGdiyu2K5RnrZ8ejaK7KwDG5zE0p/LnkQvKv1GbTuxRCydKU4xaqFpjKmXFL27X\nPtbS6L3k61lqAWzEgjbrBUqKac0zmIvNUbFbACh9vzUrQ5iLJfHZ7+wrEFBGkkyLCWsZUfi6TX6f\ngb4ObB5Yqtp7IjITw2NPDav2Qdi+tc92xdZp6lJoS0huMjOC20i5GHBxszWjIBRbSiAkRBxWOdzg\n8LEw3puXXFNNGBkbvUxatY212LBCsJHB21a0Zf9tZOPNn0dqSpXcvW4WJ0pTjFqoegJAyoq3417y\n92udDZD/GbPKe7lLfcwi/35SE6kXj5zLqZLIF1BqYydvMZxvoRul2HWmFZbTqmxQawM8ODyO2zeu\nKGvNtRVq/mhONcRUCjt3DeOz39mH+/9lHz77nX3YuWsYYko9i9NMY5H8zVbSorUmgDSplTAyqY1s\nKtWKkbGRGouoHZ2qNgaScFDCiWxQrXkknWHe3uzH1vXdRSW2FDuf1Ni2pTfnnGOlZ2V9HvRrHLU5\nNBLOtA624V5yhISIzQNLsXndUsOfMbI2Ae3xbG5iiso9cBLW58Ezg2fwzMEzqmWNg8PjEBJiwXi3\nBVlsXLMIX/nI9QAyHdiM7pdKz2HHOsv/vbSuu/pS9SY7kZkYTo9Gq27PrMxZVgKsuET0Yj4uFwzH\nMpUotpSg3PWDTmKHhac1BqXMBtWaR6k08Nfb1mJVd2vRyoJTpSlGLVStrPiJ6VhB7Xsx91KKS/b3\ntJvKU9EiFk9iKiqgv7dDsbRKOhymVP2nzWDE2JB7CpTG2662nXqHb5wbn4WYMO8RVMtTuumqJeBP\nRlT3g+6uQNXtmXUltKVJMT8Xt+QS0RIIUj9gvQ5qehQjPMpdP+g0RsbG6hiUMhtUz3U8eGwcay5T\nt1LN4KQyoheDbWv2q7agTAN45GdDtiX8KCnhzwyehcfjLiouKSkDQ8fDGIvMIxRk0N3ZhPGp+QKr\ntVJjoUYSzPIFlHy87cyN0D18Y0bIaR1tVPlRCgUMjYzj2cGzYBnla+gdBlSpe2ZdCO0cLXxGQItG\nDaNWbEpLIEj9gIulWOFRzvrBYtEruTA6NsUqPqVozajdUCUMYbM9+QflLE3RK8eyS8g52Qs6XxmQ\njr7UotJiocUmmDmVG+HE4RvyUICEvDuiUt17te2ZdSG08xeeVtMBPZdIqX5gM8IjX9iVa5O2itmS\nC72xKaegMoqW69iJpKZylKYAxs6eL1bIOZUgZvVwnEpLStNSnqQEM639y8mwm90Kl9b11I4erob9\nQk7NC22zC2+grwMAVDMkK+kH1hJ25dqkrWAlv8BII4RKHgMt13GlxtKsYOTs+WKFnFNCxezhOHbc\n0ymMnGGvhpNhN7sVLq3r6Z0nUcn7hZyaF9p6Cy8UYDE1K2SPvkyn04o1jPkWXyX8wNVWX6iEWU27\nmEYIlYTWRsgtby3DEzmL1sEoxQo5p4SKmcNx7LqnU1Rq2M1uhauWk3Elal5oa/2I7c1+fO7e9ZgX\nkopHClayECz3ma52YVbTrgVFRSK/kxfLZH6vF4+cB38yYqsyYqWXvZ04nSTphFDRi8nnY8TVXG6s\nGhtOeRjtnhe1nowL1IHQ1vsRg40Mgo2MKSFY7g0QqL5GD2qY0YxrRVGRkG+EP3yS1+zaZJVK8kw4\nmQ/ilFCRnm3oeBjjk/NZj5wLmdPAzLqaqx0nPIx2z4tqSywzS80LbcDYj2hECLa3+CtmA6wVN5AZ\nzbhWFBUltLo22Zn9XE7PRCnyQewWKtIz//mdDTj+Zjjnme+6ufzKey0gnxcexgcxnihqPCsp78gJ\n6kJoG5kURoRgJW2AteQGMqoZ14qikk85sp/t8kxY8TpVQj6IWfyMt+CZq/F7VDKsz4POjiaMjc3Y\ndr1a/H3qQmhLaE0KPSEIQHMDvH3jimxsvFQCs1bcQEY141pSVOQ4mf2slkRl5LhSLSrJ7U4QQGWE\nLUtBXQltPdRa4W0eWIqxyXnNU4u+8Oh+TEZLu3nVmhvIiGZcK4qKHCezn/2MW7HfNMt4ivJMVJLX\niahv6k2BJKEtQ6sVXijIgGU8iMWVDzmILDSWL8fmVatuICVqTVGRsFsZERIixiJzqoenFEOtJQQS\n1U29KZAktBVQaoWn17owH9q8nKXWFBW7lBGj573HF1yJVsawlhMCieqiHhXI2vMd2IDWRPAzHrQ3\ns3C7gNYAo3qNSj3WjahsjB4TqYZkdeg1BCkmVu7UkZ8EYZZaPo5YDbK0FdCaCPGEiE/fsw6Mz4MG\n1osHv7u/5rKZierETMveYmLltZoQSFQftVpRogVZ2gpoWRKMz4O2lgZ0hRqzx7opQZsXUWqMnPfe\n3uzH1vXdRSfubdvSi63ru9He7IfbxusShBkkBVKJWt2DydJWQMuSiMVFPPHciWyCQy1mMxPVSSnO\ne5eo1YRAovqotz2YhLYKd9x4GZ4fOqeYLS5PcKDNi6gUSnHeu9I9KemMKCf1tgeT0FYhOpeAoFbe\npZAhS5sXUQnUm9VBEBL1sgeXVGhzHNcC4IcAmgEwAP6a5/kXS/kMRqnHBAei+qk3q4Mg6o1SJ6L9\nNYCneZ7fBOBeAP9U4vsbph4THIjaodjSMYIgKpNSu8e/DkAyXb0AYiW+vynI1UgQBFF+6qWvuBFc\naSf6HALgOO6DAP4q788f4Hl+P8dxiwD8GsAneJ7fo3WdZFJMe73l/ZFi8SQi0wJCzSz8DKUBEARB\nlAJRTOHRX76GfUfOYWxyHp2tDbhuzWLcd/sV8HhqumLZpfqCU0JbDY7jrgTwYwCf5Hn+13rvHxub\nsfUBOzuDth39RqhD41waaJxLA41zacgf5527hhWrIbau767JvuISnZ1BVaFdUlWF47jLAfwUwHYj\nApsoHiEhYjQyByGhnAlPEARRiej1Fa/XPa3Uvt4vA/ADeITjOACY4nn+D0v8DHVBvR1XRxBEbWHn\nwTR2xcQrIbZeUqFNArp01NtxdURpsWPzMnoNrfdVwiZqhmp73nJiR9mtXcZLJRlBlFVVg5TyuDpp\nE2pgvZgXkhW5GeltlHZupJW4Kdv5THZsXvnXCAUZrL60DdtvWYVG1mfoXgAqZhM1QrHjVgnzamYu\njtOjUXR3BRBsVD/h0K5nteNgGruMl0oygkho1yClOO9Y2oQO8qOYmInD7QJS6f+/vbuPkbO4Dzj+\nPZ99Z5s7G7+cE3CIaDEeKpI4BgtoSCChJiSN0kQhCalVKIVSRU2qJKrUNC1JWwStWjWtFKUvCMUi\ntIGiIqEokRLAmDQJrsuLTYybZozVFoOB+Ow723c93/pud/vHc2evz7t7u3fP7e5z+/38A7vPc/PM\njp9nfjPPM88MrJqojD77yY2zSj8N1SrK8XyRgeOjbHv+VfbsPzzrir+VWuK15mkmlWulymtkdJyb\nbwgzqkgHhk6yY+8b7NrXz7vfcd6p/FWrKIEZVaLlfnM9Pf7+wRHo6Kh7DveZVvozOa/SvgvS0VHk\nngd2cbB/mEIRFnTA2r4e/viWy+haeDqE1NoYqyd/s3ntNq3OS6ut2W3QbrK5eNbSiNncplZChYkx\n/pOV0dIlXXz06gtnfZzZqFRRxgNHGRkdO6t8ZtN6bqWW+HR5KhSLLOjoqLuBUa3y2rH3DeKBwZoC\nSqU0Rk/mT+X3xmsvqrjfrthPR4WxteUq0dxYvmwDbcPFq+kAXnipeqMtXyhw76N72PbMy4yeLACw\nuKuTq9/+Zj71KxeXbQCVfk7yfKjib6lW6ddzXs3FXZCVy7rJjeUZPjF+ap9CEV45NMw9D+ziz267\nomJepzbGPv7eX+SRH/x3XfmbzQx/aXVeGtEJqodBu0nyhQIPbnuJF/Yd5uhw+s9a3nnxap58/uBZ\n+5e7rVRvw6GWdZt37n2dD15xQdNu5VXL4yuHhqv+bb2t51ZriU+Xpx0vvnHGQji1NjCmW/qzlnSm\nSwOSMrtmw/lVKsrKf19aiZZeG+UaaNunXB+V8j81GEHSwHjy+YMU4YwG0IreLs5Z0sXI6Nip6zG8\ndQUDQyfL5ndgKFex0q/3vEqj4VgujUoO9g8zNHKS3qVdNTXG4oGjZ1x79eRvJvOKp9V5abUprVvv\n4U8byBcK3HX/czy16yCDwzmKnD6BH96+v660Ji+yI8fPTKcI0653nDQc9nHnfTv50r07ufO+nTy4\nbR/5QqHqMWupeA8fPcGx4er7zKVa8ljJZMWfxrHqTSst1fJUbuU6mP41mmrrzNeSTr5Q4LFnDlTs\nJU8aHBqFYrHisVb0Vs5HaSVaem3UozT/ubF8xV4ywNN7Xj/j+hsYOskrh4bPuB537H2j4t8v6IAl\n3eX7TvWcV2m8HlVLY7xUoQivTgThWq63g/3lG8tz9fpWWlNRt9qU1gbtJnjwiX0Ve3v1nMDVLrKf\nvHSEG6+9iLvvuJI//52ruPuOK9myef0ZvfhKAX+6hkMtlffqc5c0dVGVWgNMOfW2nqsdq1mLy8zk\n90/XwKhWedWSzsPb9/PU7tdOPUqpZEXvYvpWLK14rMtC37SVaL0BqFRp/o8N5yr2kgFyY9UbuNMp\nFOFEbrzstnrOqzQajvU2dBd0wFvW9Eyb10mV/t3nsmF703Xrpu28NDKdNHh7vMFyY3l2v3S44vaB\nOXjWksatt1LVRnVOuupt5zV19HQteayk3tZzGqNc01YtT4u7Osv2tmtpYExWUrtiPwMVblOXS6ee\nIDpZZrUMQqq0bTZ3Wkrzv7ynm5W9XVUD92ysWtZdsczrOa/SuIVbLY1y1vadHkVey/U2OVh1pvmb\nibRWvWul1fMM2g12bDjH0eHKFcC551S+iKeazYU628EVUyvvqaPHb/vwpQwM/F9Nv2OulKv0ly5e\nWPEux6plM18QphUXl6mUp2KxWPN4h6lKK69/fizydJlbv+XSmS6IdnTAyillNl1FWW1bvQGoUv67\nF3VyWVgzo8ZfqUoNpY3r+6qWea3nVRoNx2pp9CxZyMjo+Fmjx8vl9cd7Xi/7W9f29ZS99hrRsE1r\nre1WWLO74XOP12u+zT2eG8tz5307K1Ym77tsLTe/P9Sc3kzn5q2Wj1XLFnP3HVfWPCit3HvazS7n\nUqUD7RZ2dkwMTjpdCb7jopVs3nQBK5ctztx72rWU89Q8nR6gdXYgmNkgyOnTqXa+rezt5vOf3FD3\na1TTqXRtQHKOb7h41cTo8SNV858vFPj2jpfZ9syBiuMBpnPd5WsnBqzNrMxrOa/S+HetlsbI6HhN\n72mP5MZ56Il9/OzAIINDuVNpnB49Xjl/rVRvNFO1uccN2k1QqTK5YE0PX7l105xVnLXmI43J+Fuh\nnKtphckq0jCbcm701I6NXvyh3LVRroFWS/77+np59bWj9A+OcDJf4B8e3Vu2AdIBrO07hxO58TMC\n1mzeja9XI2erm0ka1dJu9XqjUQzaJVrhpCitTAaOj7K8p4uNF69my/XrZzwRx0wusrR6XOW0Qjm3\ngyyV81yeb9WkEYBqXX3qfRvP5+YbLpk3jcJGy9L5PJcM2iVa6aRolQt7LvLRSuU8n2WxnFvlvK/H\n1HJuVgNkvsvi+TwXqgVtB6I1USsMamilfKg9zIfzrZVGE6u92CSUpBmabIAYsM+WG8tzaHCkbde9\nniv2tCVJqWnFxXPmE4O2JCk1rbh4znxis0eSlIo05kBXdQZtSVIqWnHxnPnGoC1JSkUrLp4z3xi0\nJUmpaLVlLOcjSeV8XQAACLdJREFUB6JJklLTiovnzCcGbUlSapx4Zm55e1zSvOBkHq3FiWfmhj1t\nSZnmZB5qJwZtSZnmZB5qJzZDJWWWk3mo3Ri0JWWWk3mo3Ri0JWWWk3mo3Ri0JWWWk3mo3TgQTVKm\nOZlH+8mN5dv2HXCDtqRMczKP9uHrfQZtSfPE5GQemr98vc9n2pKkDPD1voRBW5LU8ny9L2HQliS1\nPF/vSxi0JUktz9f7Eg5EkyRlgq/3GbQlSRnh630GbUlSxrTz630+05YkKSMM2pIkZYRBW5KkjDBo\nS5KUEQZtSZIywqAtSVJGGLQlScoIg7YkSRlh0JYkKSMM2pIkZYRBW5KkjDBoS5KUEQZtSZIywqAt\nSVJGGLQlScoIg7YkSRlh0JYkKSMWNuOgIYRLgP8A3hRjHG1GHiRJypqG97RDCMuArwK5Rh9bkqQs\n6ygWiw07WAihA3gI+Avg28Al0/W0x8fzxYULOxuRPUmSWkFHpQ1zdns8hHA78IUpX78M/EuM8Sch\nhJrSGRwcSTVffX299PcPpZqmzmY5N4bl3BiWc2NYzom+vt6K2+YsaMcYvwF8o/S7EMJ+4PaJgP5m\n4HHgmrnKgyRJ80lDB6LFGNdN/n8I4X+B9zfy+JIkZZmvfEmSlBFNeeULIMZ4YbOOLUlSFtnTliQp\nIwzakiRlhEFbkqSMMGhLkpQRBm1JkjLCoC1JUkYYtCVJygiDtjQP5MbyHBocITeWb3ZWJM2hpk2u\nImn28vkCD27bx+59/Qwcz7FyWTcb1/dx03Xr6Fxgm1yabwzaUoZt/c5/su25V099PnI8d+rzls3r\nm5UtSXPEprjUAmZyezs3lmfn3tfLbtu97zBDIye9ZS7NM/a0pSbKFwo8vH3/jG5vHxvO0X/0RNlt\nR46P8qdbn+XosLfMpfnEK1hqooe372fbc69y5HiOIqdvbz+8ff+0f7u8p5u+c5dU3D44XH+aklqb\nQVtqktxYnt37+stu273v8LS3tbsXdXLV286r+Xi1pCmptRm0pSY5Npxj4Hiu7LbBoVGODZffVuq2\nD1/K5k1vYdWyxSzogHN7uiruW2uaklqXz7SlJlne083KZd0cKRO4V/QuZnlP97RpdHYuYMvm9dx4\n7UUcG86xpHshd93/7KzSlNS67GlLTdK9qJON6/vKbtu4fjXdizrrSmvNiqX0Lu1KLU1JrceettRE\nN123DkieNw8OjbKidzEb168+9X2rpCmpNXQUi8Vm56Gq/v6hVDPY19dLf/9QmkmqDMu5PrmxPMeG\ncyzv6a6rN1ytnGeaps7m+dwYlnOir6+3o9I2e9pSC5i8vd3qaUpqLp9pS5KUEQZtSZIywqAtSVJG\nGLQlScoIg7YkSRlh0JYkKSMM2pIkZYRBW5KkjDBoS5KUES0/jakkSUrY05YkKSMM2pIkZYRBW5Kk\njDBoS5KUEQZtSZIywqAtSVJGLGx2BholhLAA+HtgA5ADfjvGuL+5uWp9IYRFwFbgQqAbuBv4KXA/\nUAT2Ap+JMRZCCH8CfAgYBz4fY3wmhLCu1n0b+btaVQhhDfA8cD1J2dyP5ZyqEMKXgF8DukjqhH/D\nck7VRL3xTZJ6Iw/cgedzKtqpp/1RYHGM8ZeBPwS+2uT8ZMVvAEdijO8BPgh8Hfgb4M6J7zqAj4QQ\nLgOuBa4EPgX83cTf17NvW5uo6O4FTkx8ZTmnLITwXuBdwNUkZXMBlvNc+FVgYYzxXcBdwD1Yzqlo\np6D9buD7ADHGncCm5mYnM/4V+HLJ53HgcpLeCcD3gM0k5ft4jLEYYzwALAwh9NW5b7v7a+Afgdcm\nPlvO6bsBeBF4FPgO8F0s57mwj6QcFgDLgDEs51S0U9BeBhwr+ZwPIbTN44GZijEOxxiHQgi9wCPA\nnUBHjHFyKr0hYDlnl+/k9/Xs27ZCCLcC/THGx0q+tpzTt5qkwf4J4NPAt4AFlnPqhklujf8MuA/4\nGp7PqWinoH0c6C35vCDGON6szGRJCOEC4Cngn2KMDwKFks29wFHOLt/J7+vZt53dBlwfQvgB8E7g\nAWBNyXbLOR1HgMdijCdjjBEY5cyK33JOxxdIynk9yTiib5KMIZhkOc9QOwXtp0mesxBCuIrkFpmm\nEUJ4E/A48MUY49aJr3dPPBuE5Dn3j0jK94YQwoIQwltJGkWH69y3bcUYr4kxXhtjfC/wAnAL8D3L\nOXU/Bj4QQugIIZwPnAM8aTmnbpDTveIBYBHWG6lop9vDj5L0ZHaQDGz4rSbnJyv+CFgBfDmEMPls\n+3PA10IIXcB/AY/EGPMhhB8B/07SGPzMxL6/D9xX4746Uz1lZznXIMb43RDCNcAznC6T/8FyTtvf\nAlsnyqWLpB55Dst51lzlS5KkjGin2+OSJGWaQVuSpIwwaEuSlBEGbUmSMsKgLUlSRhi0pXkuhLAp\nhPBIHfuvDiH4WonUgtrpPW2pLcUYnwM+3ux8SJo9g7Y0z03MLPV1ksktjgNvJ1ndag9wS4xxOITw\nMZKVmEaAZ6f8/e3A75LcmTsCfJZkQYgngOdjjH8QQthMspTi5THGnzfgZ0ltydvjUnu5HPgA8Esk\nCzp8YmKq2q3AjTHGy4GXJ3cOIVwL/CbwnhjjRuCvgEdjjAWSZVtvCSF8hCRgbzFgS3PLoC21l+/H\nGHMxxjGS+fdXkix5+GKM8acT+9xbsv+HgHXAjhDCCyRBe0UIYWWM8XXgDpIpgu+NMf6wYb9CalPe\nHpfay4mS/y+SzMNPyX8hWTN9UifJ6m5fBJhYH/l8kgUhAC4Ffg5cOSe5lXQGe9qSfghcGkLYMPH5\n1pJtjwG/HkI4b+Lzp4EnAUIIV5AsHrMJWB5C+Fxjsiu1L4O21OZijP3AFuBbIYRdwC+UbHsc+Evg\niRDCnon9Pgb0AA8BvxdjPEgS6L8SQtjY4OxLbcVVviRJygh72pIkZYRBW5KkjDBoS5KUEQZtSZIy\nwqAtSVJGGLQlScoIg7YkSRlh0JYkKSP+H9aTjfPkfpiCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show the graph of logerror with lines\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(range(logerror_array.shape[0]), np.sort(logerror_array))\n",
    "plt.xlabel('index', fontsize=12)\n",
    "plt.ylabel('logerror', fontsize=12)\n",
    "#####horizontal line\n",
    "cols=hf.shape[0]\n",
    "steps=100\n",
    "horiz_line_data = np.array([rmse for i in list(range(steps))])\n",
    "xs = np.arange(0,cols,(cols/steps))\n",
    "plt.plot(xs, horiz_line_data, 'r--') \n",
    "horiz_line_data = np.array([rmse_neg for i in list(range(steps))])\n",
    "plt.plot(xs, horiz_line_data, 'r--') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binomial Model\n",
    "\n",
    "Let's perform classification on 'logerror_highlow'\n",
    "\n",
    "We will use the Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) algorithm to ensure that this demo can be run in almost all environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glm_bi_v1 = H2OGeneralizedLinearEstimator(\n",
    "                    model_id='glm__bi_v1',            \n",
    "                    family='binomial',\n",
    "                    solver='L_BFGS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_binary='logerror_highlow'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glm Model Build progress: |███████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "glm_bi_v1.train(X, y_binary, training_frame=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details\n",
      "=============\n",
      "H2OGeneralizedLinearEstimator :  Generalized Linear Modeling\n",
      "Model Key:  glm__bi_v1\n",
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: glm\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.2234589504384685\n",
      "RMSE: 0.47271444915346994\n",
      "LogLoss: 0.6332640128269416\n",
      "Null degrees of freedom: 81198\n",
      "Residual degrees of freedom: 75207\n",
      "Null deviance: 111583.6209405093\n",
      "Residual deviance: 102840.80915506966\n",
      "AIC: 114824.80915506966\n",
      "AUC: 0.6427532928875853\n",
      "pr_auc: 0.6248905795125741\n",
      "Gini: 0.28550658577517063\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.3101948010211503: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>3270.0</td>\n",
       "<td>41790.0</td>\n",
       "<td>0.9274</td>\n",
       "<td> (41790.0/45060.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>17.0</td>\n",
       "<td>36122.0</td>\n",
       "<td>0.0005</td>\n",
       "<td> (17.0/36139.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>3287.0</td>\n",
       "<td>77912.0</td>\n",
       "<td>0.5149</td>\n",
       "<td> (41807.0/81199.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0     1      Error    Rate\n",
       "-----  ----  -----  -------  -----------------\n",
       "0      3270  41790  0.9274   (41790.0/45060.0)\n",
       "1      17    36122  0.0005   (17.0/36139.0)\n",
       "Total  3287  77912  0.5149   (41807.0/81199.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.3101948</td>\n",
       "<td>0.6334359</td>\n",
       "<td>330.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.2999656</td>\n",
       "<td>0.8118828</td>\n",
       "<td>332.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.4369864</td>\n",
       "<td>0.5487036</td>\n",
       "<td>221.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4754347</td>\n",
       "<td>0.6130002</td>\n",
       "<td>172.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9880326</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.1748224</td>\n",
       "<td>1.0</td>\n",
       "<td>364.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9880326</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.7985921</td>\n",
       "<td>0.2488978</td>\n",
       "<td>60.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.4309438</td>\n",
       "<td>0.5884154</td>\n",
       "<td>230.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.4441490</td>\n",
       "<td>0.5941733</td>\n",
       "<td>210.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.310195     0.633436  330\n",
       "max f2                       0.299966     0.811883  332\n",
       "max f0point5                 0.436986     0.548704  221\n",
       "max accuracy                 0.475435     0.613     172\n",
       "max precision                0.988033     1         0\n",
       "max recall                   0.174822     1         364\n",
       "max specificity              0.988033     1         0\n",
       "max absolute_mcc             0.798592     0.248898  60\n",
       "max min_per_class_accuracy   0.430944     0.588415  230\n",
       "max mean_per_class_accuracy  0.444149     0.594173  210"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 44.51 %, avg score: 44.60 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100001</td>\n",
       "<td>0.8788141</td>\n",
       "<td>2.2440854</td>\n",
       "<td>2.2440854</td>\n",
       "<td>0.9987685</td>\n",
       "<td>0.8992502</td>\n",
       "<td>0.9987685</td>\n",
       "<td>0.8992502</td>\n",
       "<td>0.0224411</td>\n",
       "<td>0.0224411</td>\n",
       "<td>124.4085371</td>\n",
       "<td>124.4085371</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200002</td>\n",
       "<td>0.8626453</td>\n",
       "<td>2.2468524</td>\n",
       "<td>2.2454689</td>\n",
       "<td>1.0</td>\n",
       "<td>0.8700046</td>\n",
       "<td>0.9993842</td>\n",
       "<td>0.8846274</td>\n",
       "<td>0.0224688</td>\n",
       "<td>0.0449099</td>\n",
       "<td>124.6852431</td>\n",
       "<td>124.5468901</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300004</td>\n",
       "<td>0.8517865</td>\n",
       "<td>2.2440854</td>\n",
       "<td>2.2450077</td>\n",
       "<td>0.9987685</td>\n",
       "<td>0.8571908</td>\n",
       "<td>0.9991790</td>\n",
       "<td>0.8754818</td>\n",
       "<td>0.0224411</td>\n",
       "<td>0.0673511</td>\n",
       "<td>124.4085371</td>\n",
       "<td>124.5007724</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400005</td>\n",
       "<td>0.8394413</td>\n",
       "<td>2.2440854</td>\n",
       "<td>2.2447771</td>\n",
       "<td>0.9987685</td>\n",
       "<td>0.8459307</td>\n",
       "<td>0.9990764</td>\n",
       "<td>0.8680941</td>\n",
       "<td>0.0224411</td>\n",
       "<td>0.0897922</td>\n",
       "<td>124.4085371</td>\n",
       "<td>124.4777136</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500006</td>\n",
       "<td>0.6657080</td>\n",
       "<td>2.0614594</td>\n",
       "<td>2.2081136</td>\n",
       "<td>0.9174877</td>\n",
       "<td>0.8008453</td>\n",
       "<td>0.9827586</td>\n",
       "<td>0.8546443</td>\n",
       "<td>0.0206148</td>\n",
       "<td>0.1104070</td>\n",
       "<td>106.1459435</td>\n",
       "<td>120.8113596</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000012</td>\n",
       "<td>0.5135524</td>\n",
       "<td>1.2833623</td>\n",
       "<td>1.7457379</td>\n",
       "<td>0.5711823</td>\n",
       "<td>0.5525037</td>\n",
       "<td>0.7769704</td>\n",
       "<td>0.7035740</td>\n",
       "<td>0.0641689</td>\n",
       "<td>0.1745759</td>\n",
       "<td>28.3362263</td>\n",
       "<td>74.5737929</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500018</td>\n",
       "<td>0.4866759</td>\n",
       "<td>1.1765538</td>\n",
       "<td>1.5560099</td>\n",
       "<td>0.5236453</td>\n",
       "<td>0.4984883</td>\n",
       "<td>0.6925287</td>\n",
       "<td>0.6352121</td>\n",
       "<td>0.0588284</td>\n",
       "<td>0.2334044</td>\n",
       "<td>17.6553761</td>\n",
       "<td>55.6009873</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000025</td>\n",
       "<td>0.4721866</td>\n",
       "<td>1.1118046</td>\n",
       "<td>1.4449585</td>\n",
       "<td>0.4948276</td>\n",
       "<td>0.4789059</td>\n",
       "<td>0.6431034</td>\n",
       "<td>0.5961356</td>\n",
       "<td>0.0555909</td>\n",
       "<td>0.2889953</td>\n",
       "<td>11.1804565</td>\n",
       "<td>44.4958546</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000037</td>\n",
       "<td>0.4533620</td>\n",
       "<td>1.1118046</td>\n",
       "<td>1.3339072</td>\n",
       "<td>0.4948276</td>\n",
       "<td>0.4620539</td>\n",
       "<td>0.5936782</td>\n",
       "<td>0.5514417</td>\n",
       "<td>0.1111818</td>\n",
       "<td>0.4001771</td>\n",
       "<td>11.1804565</td>\n",
       "<td>33.3907219</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000049</td>\n",
       "<td>0.4405716</td>\n",
       "<td>1.0357104</td>\n",
       "<td>1.2593580</td>\n",
       "<td>0.4609606</td>\n",
       "<td>0.4467127</td>\n",
       "<td>0.5604988</td>\n",
       "<td>0.5252594</td>\n",
       "<td>0.1035723</td>\n",
       "<td>0.5037494</td>\n",
       "<td>3.5710425</td>\n",
       "<td>25.9358020</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000062</td>\n",
       "<td>0.4297530</td>\n",
       "<td>0.9607231</td>\n",
       "<td>1.1996310</td>\n",
       "<td>0.4275862</td>\n",
       "<td>0.4350485</td>\n",
       "<td>0.5339163</td>\n",
       "<td>0.5072173</td>\n",
       "<td>0.0960735</td>\n",
       "<td>0.5998229</td>\n",
       "<td>-3.9276892</td>\n",
       "<td>19.9631038</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999951</td>\n",
       "<td>0.4198185</td>\n",
       "<td>0.9489416</td>\n",
       "<td>1.1578538</td>\n",
       "<td>0.4223427</td>\n",
       "<td>0.4247440</td>\n",
       "<td>0.5153226</td>\n",
       "<td>0.4934731</td>\n",
       "<td>0.0948836</td>\n",
       "<td>0.6947065</td>\n",
       "<td>-5.1058383</td>\n",
       "<td>15.7853756</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999963</td>\n",
       "<td>0.4100937</td>\n",
       "<td>0.8804784</td>\n",
       "<td>1.1182280</td>\n",
       "<td>0.3918719</td>\n",
       "<td>0.4150402</td>\n",
       "<td>0.4976864</td>\n",
       "<td>0.4822682</td>\n",
       "<td>0.0880489</td>\n",
       "<td>0.7827555</td>\n",
       "<td>-11.9521621</td>\n",
       "<td>11.8228005</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999975</td>\n",
       "<td>0.3988666</td>\n",
       "<td>0.8674732</td>\n",
       "<td>1.0868832</td>\n",
       "<td>0.3860837</td>\n",
       "<td>0.4046417</td>\n",
       "<td>0.4837359</td>\n",
       "<td>0.4725648</td>\n",
       "<td>0.0867484</td>\n",
       "<td>0.8695039</td>\n",
       "<td>-13.2526802</td>\n",
       "<td>8.6883171</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999988</td>\n",
       "<td>0.3830450</td>\n",
       "<td>0.8259673</td>\n",
       "<td>1.0578921</td>\n",
       "<td>0.3676108</td>\n",
       "<td>0.3917972</td>\n",
       "<td>0.4708329</td>\n",
       "<td>0.4635905</td>\n",
       "<td>0.0825977</td>\n",
       "<td>0.9521016</td>\n",
       "<td>-17.4032696</td>\n",
       "<td>5.7892123</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0031575</td>\n",
       "<td>0.4789780</td>\n",
       "<td>1.0</td>\n",
       "<td>0.2131773</td>\n",
       "<td>0.2877187</td>\n",
       "<td>0.4450671</td>\n",
       "<td>0.4460031</td>\n",
       "<td>0.0478984</td>\n",
       "<td>1.0</td>\n",
       "<td>-52.1021976</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score     cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100001                   0.878814           2.24409   2.24409            0.998768         0.89925   0.998768                    0.89925             0.0224411       0.0224411                  124.409   124.409\n",
       "    2        0.0200002                   0.862645           2.24685   2.24547            1                0.870005  0.999384                    0.884627            0.0224688       0.0449099                  124.685   124.547\n",
       "    3        0.0300004                   0.851787           2.24409   2.24501            0.998768         0.857191  0.999179                    0.875482            0.0224411       0.0673511                  124.409   124.501\n",
       "    4        0.0400005                   0.839441           2.24409   2.24478            0.998768         0.845931  0.999076                    0.868094            0.0224411       0.0897922                  124.409   124.478\n",
       "    5        0.0500006                   0.665708           2.06146   2.20811            0.917488         0.800845  0.982759                    0.854644            0.0206148       0.110407                   106.146   120.811\n",
       "    6        0.100001                    0.513552           1.28336   1.74574            0.571182         0.552504  0.77697                     0.703574            0.0641689       0.174576                   28.3362   74.5738\n",
       "    7        0.150002                    0.486676           1.17655   1.55601            0.523645         0.498488  0.692529                    0.635212            0.0588284       0.233404                   17.6554   55.601\n",
       "    8        0.200002                    0.472187           1.1118    1.44496            0.494828         0.478906  0.643103                    0.596136            0.0555909       0.288995                   11.1805   44.4959\n",
       "    9        0.300004                    0.453362           1.1118    1.33391            0.494828         0.462054  0.593678                    0.551442            0.111182        0.400177                   11.1805   33.3907\n",
       "    10       0.400005                    0.440572           1.03571   1.25936            0.460961         0.446713  0.560499                    0.525259            0.103572        0.503749                   3.57104   25.9358\n",
       "    11       0.500006                    0.429753           0.960723  1.19963            0.427586         0.435049  0.533916                    0.507217            0.0960735       0.599823                   -3.92769  19.9631\n",
       "    12       0.599995                    0.419819           0.948942  1.15785            0.422343         0.424744  0.515323                    0.493473            0.0948836       0.694707                   -5.10584  15.7854\n",
       "    13       0.699996                    0.410094           0.880478  1.11823            0.391872         0.41504   0.497686                    0.482268            0.0880489       0.782755                   -11.9522  11.8228\n",
       "    14       0.799998                    0.398867           0.867473  1.08688            0.386084         0.404642  0.483736                    0.472565            0.0867484       0.869504                   -13.2527  8.68832\n",
       "    15       0.899999                    0.383045           0.825967  1.05789            0.367611         0.391797  0.470833                    0.46359             0.0825977       0.952102                   -17.4033  5.78921\n",
       "    16       1                           0.00315753         0.478978  1                  0.213177         0.287719  0.445067                    0.446003            0.0478984       1                          -52.1022  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scoring History: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>timestamp</b></td>\n",
       "<td><b>duration</b></td>\n",
       "<td><b>iterations</b></td>\n",
       "<td><b>negative_log_likelihood</b></td>\n",
       "<td><b>objective</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:28</td>\n",
       "<td> 0.000 sec</td>\n",
       "<td>0</td>\n",
       "<td>55791.8104703</td>\n",
       "<td>0.6870997</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:28</td>\n",
       "<td> 0.062 sec</td>\n",
       "<td>1</td>\n",
       "<td>55540.7363374</td>\n",
       "<td>0.6840166</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:28</td>\n",
       "<td> 0.120 sec</td>\n",
       "<td>2</td>\n",
       "<td>54203.8316944</td>\n",
       "<td>0.6690222</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:28</td>\n",
       "<td> 0.141 sec</td>\n",
       "<td>3</td>\n",
       "<td>53427.1974709</td>\n",
       "<td>0.6612905</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:28</td>\n",
       "<td> 0.171 sec</td>\n",
       "<td>4</td>\n",
       "<td>53326.6581786</td>\n",
       "<td>0.6602605</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:28</td>\n",
       "<td> 0.187 sec</td>\n",
       "<td>5</td>\n",
       "<td>53041.1392489</td>\n",
       "<td>0.6575935</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:28</td>\n",
       "<td> 0.229 sec</td>\n",
       "<td>6</td>\n",
       "<td>52124.0933546</td>\n",
       "<td>0.6503448</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:28</td>\n",
       "<td> 0.243 sec</td>\n",
       "<td>7</td>\n",
       "<td>51956.5070436</td>\n",
       "<td>0.6484665</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:28</td>\n",
       "<td> 0.262 sec</td>\n",
       "<td>8</td>\n",
       "<td>51696.4244124</td>\n",
       "<td>0.6469265</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:28</td>\n",
       "<td> 0.277 sec</td>\n",
       "<td>9</td>\n",
       "<td>51602.8539268</td>\n",
       "<td>0.6465430</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:28</td>\n",
       "<td> 0.303 sec</td>\n",
       "<td>10</td>\n",
       "<td>51548.7534794</td>\n",
       "<td>0.6462556</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:28</td>\n",
       "<td> 0.336 sec</td>\n",
       "<td>11</td>\n",
       "<td>51500.0491146</td>\n",
       "<td>0.6460627</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:28</td>\n",
       "<td> 0.368 sec</td>\n",
       "<td>12</td>\n",
       "<td>51447.7990792</td>\n",
       "<td>0.6457363</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:28</td>\n",
       "<td> 0.400 sec</td>\n",
       "<td>13</td>\n",
       "<td>51455.0839031</td>\n",
       "<td>0.6454288</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:29</td>\n",
       "<td> 0.432 sec</td>\n",
       "<td>14</td>\n",
       "<td>51427.6655514</td>\n",
       "<td>0.6453508</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:29</td>\n",
       "<td> 0.460 sec</td>\n",
       "<td>15</td>\n",
       "<td>51420.4045775</td>\n",
       "<td>0.6453245</td></tr></table></div>"
      ],
      "text/plain": [
       "    timestamp            duration    iterations    negative_log_likelihood    objective\n",
       "--  -------------------  ----------  ------------  -------------------------  -----------\n",
       "    2019-02-20 01:34:28  0.000 sec   0             55791.8                    0.6871\n",
       "    2019-02-20 01:34:28  0.062 sec   1             55540.7                    0.684017\n",
       "    2019-02-20 01:34:28  0.120 sec   2             54203.8                    0.669022\n",
       "    2019-02-20 01:34:28  0.141 sec   3             53427.2                    0.661291\n",
       "    2019-02-20 01:34:28  0.171 sec   4             53326.7                    0.660261\n",
       "    2019-02-20 01:34:28  0.187 sec   5             53041.1                    0.657593\n",
       "    2019-02-20 01:34:28  0.229 sec   6             52124.1                    0.650345\n",
       "    2019-02-20 01:34:28  0.243 sec   7             51956.5                    0.648467\n",
       "    2019-02-20 01:34:28  0.262 sec   8             51696.4                    0.646927\n",
       "    2019-02-20 01:34:28  0.277 sec   9             51602.9                    0.646543\n",
       "    2019-02-20 01:34:28  0.303 sec   10            51548.8                    0.646256\n",
       "    2019-02-20 01:34:28  0.336 sec   11            51500                      0.646063\n",
       "    2019-02-20 01:34:28  0.368 sec   12            51447.8                    0.645736\n",
       "    2019-02-20 01:34:28  0.400 sec   13            51455.1                    0.645429\n",
       "    2019-02-20 01:34:29  0.432 sec   14            51427.7                    0.645351\n",
       "    2019-02-20 01:34:29  0.460 sec   15            51420.4                    0.645324"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm_bi_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F0point5',\n",
       " 'F1',\n",
       " 'F2',\n",
       " 'Lambda',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_bc',\n",
       " '_bcin',\n",
       " '_check_and_save_parm',\n",
       " '_check_targets',\n",
       " '_compute_algo',\n",
       " '_end_time',\n",
       " '_estimator_type',\n",
       " '_future',\n",
       " '_get_metrics',\n",
       " '_have_mojo',\n",
       " '_have_pojo',\n",
       " '_id',\n",
       " '_is_xvalidated',\n",
       " '_job',\n",
       " '_keyify_if_h2oframe',\n",
       " '_metrics_class',\n",
       " '_model_json',\n",
       " '_parms',\n",
       " '_plot',\n",
       " '_requires_training_frame',\n",
       " '_resolve_model',\n",
       " '_run_time',\n",
       " '_start_time',\n",
       " '_verify_training_frame_params',\n",
       " '_xval_keys',\n",
       " 'accuracy',\n",
       " 'actual_params',\n",
       " 'aic',\n",
       " 'algo',\n",
       " 'alpha',\n",
       " 'auc',\n",
       " 'balance_classes',\n",
       " 'beta_constraints',\n",
       " 'beta_epsilon',\n",
       " 'biases',\n",
       " 'catoffsets',\n",
       " 'class_sampling_factors',\n",
       " 'coef',\n",
       " 'coef_norm',\n",
       " 'compute_p_values',\n",
       " 'confusion_matrix',\n",
       " 'convert_H2OXGBoostParams_2_XGBoostParams',\n",
       " 'cross_validation_fold_assignment',\n",
       " 'cross_validation_holdout_predictions',\n",
       " 'cross_validation_metrics_summary',\n",
       " 'cross_validation_models',\n",
       " 'cross_validation_predictions',\n",
       " 'custom_metric_func',\n",
       " 'deepfeatures',\n",
       " 'default_params',\n",
       " 'download_mojo',\n",
       " 'download_pojo',\n",
       " 'early_stopping',\n",
       " 'end_time',\n",
       " 'error',\n",
       " 'export_checkpoints_dir',\n",
       " 'fallout',\n",
       " 'family',\n",
       " 'find_idx_by_threshold',\n",
       " 'find_threshold_by_max_metric',\n",
       " 'fit',\n",
       " 'fnr',\n",
       " 'fold_assignment',\n",
       " 'fold_column',\n",
       " 'fpr',\n",
       " 'full_parameters',\n",
       " 'gains_lift',\n",
       " 'getGLMRegularizationPath',\n",
       " 'get_params',\n",
       " 'get_xval_models',\n",
       " 'gini',\n",
       " 'gradient_epsilon',\n",
       " 'have_mojo',\n",
       " 'have_pojo',\n",
       " 'ignore_const_cols',\n",
       " 'ignored_columns',\n",
       " 'interaction_pairs',\n",
       " 'interactions',\n",
       " 'intercept',\n",
       " 'is_cross_validated',\n",
       " 'join',\n",
       " 'keep_cross_validation_fold_assignment',\n",
       " 'keep_cross_validation_models',\n",
       " 'keep_cross_validation_predictions',\n",
       " 'lambda_',\n",
       " 'lambda_min_ratio',\n",
       " 'lambda_search',\n",
       " 'link',\n",
       " 'logloss',\n",
       " 'mae',\n",
       " 'makeGLMModel',\n",
       " 'max_active_predictors',\n",
       " 'max_after_balance_size',\n",
       " 'max_confusion_matrix_size',\n",
       " 'max_hit_ratio_k',\n",
       " 'max_iterations',\n",
       " 'max_per_class_error',\n",
       " 'max_runtime_secs',\n",
       " 'mcc',\n",
       " 'mean_per_class_error',\n",
       " 'mean_residual_deviance',\n",
       " 'metric',\n",
       " 'missing_values_handling',\n",
       " 'missrate',\n",
       " 'mixin',\n",
       " 'model_id',\n",
       " 'model_performance',\n",
       " 'mse',\n",
       " 'nfolds',\n",
       " 'nlambdas',\n",
       " 'non_negative',\n",
       " 'normmul',\n",
       " 'normsub',\n",
       " 'null_degrees_of_freedom',\n",
       " 'null_deviance',\n",
       " 'obj_reg',\n",
       " 'objective_epsilon',\n",
       " 'offset_column',\n",
       " 'params',\n",
       " 'parms',\n",
       " 'partial_plot',\n",
       " 'plot',\n",
       " 'pprint_coef',\n",
       " 'precision',\n",
       " 'predict',\n",
       " 'predict_leaf_node_assignment',\n",
       " 'prior',\n",
       " 'r2',\n",
       " 'recall',\n",
       " 'remove_collinear_columns',\n",
       " 'residual_degrees_of_freedom',\n",
       " 'residual_deviance',\n",
       " 'respmul',\n",
       " 'response_column',\n",
       " 'respsub',\n",
       " 'rmse',\n",
       " 'rmsle',\n",
       " 'roc',\n",
       " 'rotation',\n",
       " 'run_time',\n",
       " 'save_model_details',\n",
       " 'save_mojo',\n",
       " 'score_each_iteration',\n",
       " 'score_history',\n",
       " 'scoring_history',\n",
       " 'seed',\n",
       " 'sensitivity',\n",
       " 'set_params',\n",
       " 'show',\n",
       " 'solver',\n",
       " 'specificity',\n",
       " 'staged_predict_proba',\n",
       " 'standardize',\n",
       " 'start',\n",
       " 'start_time',\n",
       " 'std_coef_plot',\n",
       " 'summary',\n",
       " 'tnr',\n",
       " 'tpr',\n",
       " 'train',\n",
       " 'training_frame',\n",
       " 'tweedie_link_power',\n",
       " 'tweedie_variance_power',\n",
       " 'type',\n",
       " 'validation_frame',\n",
       " 'varimp',\n",
       " 'varimp_plot',\n",
       " 'weights',\n",
       " 'weights_column',\n",
       " 'xval_keys',\n",
       " 'xvals']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(glm_bi_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.3101948010211503: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>3270.0</td>\n",
       "<td>41790.0</td>\n",
       "<td>0.9274</td>\n",
       "<td> (41790.0/45060.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>17.0</td>\n",
       "<td>36122.0</td>\n",
       "<td>0.0005</td>\n",
       "<td> (17.0/36139.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>3287.0</td>\n",
       "<td>77912.0</td>\n",
       "<td>0.5149</td>\n",
       "<td> (41807.0/81199.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0     1      Error    Rate\n",
       "-----  ----  -----  -------  -----------------\n",
       "0      3270  41790  0.9274   (41790.0/45060.0)\n",
       "1      17    36122  0.0005   (17.0/36139.0)\n",
       "Total  3287  77912  0.5149   (41807.0/81199.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Act/Pred) for max f2 @ threshold = 0.2999655906020801: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>3254.0</td>\n",
       "<td>41806.0</td>\n",
       "<td>0.9278</td>\n",
       "<td> (41806.0/45060.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>12.0</td>\n",
       "<td>36127.0</td>\n",
       "<td>0.0003</td>\n",
       "<td> (12.0/36139.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>3266.0</td>\n",
       "<td>77933.0</td>\n",
       "<td>0.515</td>\n",
       "<td> (41818.0/81199.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0     1      Error    Rate\n",
       "-----  ----  -----  -------  -----------------\n",
       "0      3254  41806  0.9278   (41806.0/45060.0)\n",
       "1      12    36127  0.0003   (12.0/36139.0)\n",
       "Total  3266  77933  0.515    (41818.0/81199.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Act/Pred) for max f0point5 @ threshold = 0.43698642101660923: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>29292.0</td>\n",
       "<td>15768.0</td>\n",
       "<td>0.3499</td>\n",
       "<td> (15768.0/45060.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>16733.0</td>\n",
       "<td>19406.0</td>\n",
       "<td>0.463</td>\n",
       "<td> (16733.0/36139.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>46025.0</td>\n",
       "<td>35174.0</td>\n",
       "<td>0.4003</td>\n",
       "<td> (32501.0/81199.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  -----------------\n",
       "0      29292  15768  0.3499   (15768.0/45060.0)\n",
       "1      16733  19406  0.463    (16733.0/36139.0)\n",
       "Total  46025  35174  0.4003   (32501.0/81199.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Act/Pred) for max accuracy @ threshold = 0.4754347403777035: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>39777.0</td>\n",
       "<td>5283.0</td>\n",
       "<td>0.1172</td>\n",
       "<td> (5283.0/45060.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>26141.0</td>\n",
       "<td>9998.0</td>\n",
       "<td>0.7233</td>\n",
       "<td> (26141.0/36139.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>65918.0</td>\n",
       "<td>15281.0</td>\n",
       "<td>0.387</td>\n",
       "<td> (31424.0/81199.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  -----------------\n",
       "0      39777  5283   0.1172   (5283.0/45060.0)\n",
       "1      26141  9998   0.7233   (26141.0/36139.0)\n",
       "Total  65918  15281  0.387    (31424.0/81199.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Act/Pred) for max precision @ threshold = 0.9880326363197696: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>45060.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td> (0.0/45060.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>36137.0</td>\n",
       "<td>2.0</td>\n",
       "<td>0.9999</td>\n",
       "<td> (36137.0/36139.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>81197.0</td>\n",
       "<td>2.0</td>\n",
       "<td>0.445</td>\n",
       "<td> (36137.0/81199.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1    Error    Rate\n",
       "-----  -----  ---  -------  -----------------\n",
       "0      45060  0    0        (0.0/45060.0)\n",
       "1      36137  2    0.9999   (36137.0/36139.0)\n",
       "Total  81197  2    0.445    (36137.0/81199.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Act/Pred) for max recall @ threshold = 0.17482238691356144: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>2303.0</td>\n",
       "<td>42757.0</td>\n",
       "<td>0.9489</td>\n",
       "<td> (42757.0/45060.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>0.0</td>\n",
       "<td>36139.0</td>\n",
       "<td>0.0</td>\n",
       "<td> (0.0/36139.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>2303.0</td>\n",
       "<td>78896.0</td>\n",
       "<td>0.5266</td>\n",
       "<td> (42757.0/81199.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0     1      Error    Rate\n",
       "-----  ----  -----  -------  -----------------\n",
       "0      2303  42757  0.9489   (42757.0/45060.0)\n",
       "1      0     36139  0        (0.0/36139.0)\n",
       "Total  2303  78896  0.5266   (42757.0/81199.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Act/Pred) for max precision @ threshold = 0.9880326363197696: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>45060.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td> (0.0/45060.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>36137.0</td>\n",
       "<td>2.0</td>\n",
       "<td>0.9999</td>\n",
       "<td> (36137.0/36139.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>81197.0</td>\n",
       "<td>2.0</td>\n",
       "<td>0.445</td>\n",
       "<td> (36137.0/81199.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1    Error    Rate\n",
       "-----  -----  ---  -------  -----------------\n",
       "0      45060  0    0        (0.0/45060.0)\n",
       "1      36137  2    0.9999   (36137.0/36139.0)\n",
       "Total  81197  2    0.445    (36137.0/81199.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Act/Pred) for max absolute_mcc @ threshold = 0.7985921088551544: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>45054.0</td>\n",
       "<td>6.0</td>\n",
       "<td>0.0001</td>\n",
       "<td> (6.0/45060.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>32281.0</td>\n",
       "<td>3858.0</td>\n",
       "<td>0.8932</td>\n",
       "<td> (32281.0/36139.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>77335.0</td>\n",
       "<td>3864.0</td>\n",
       "<td>0.3976</td>\n",
       "<td> (32287.0/81199.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1     Error    Rate\n",
       "-----  -----  ----  -------  -----------------\n",
       "0      45054  6     0.0001   (6.0/45060.0)\n",
       "1      32281  3858  0.8932   (32281.0/36139.0)\n",
       "Total  77335  3864  0.3976   (32287.0/81199.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Act/Pred) for max min_per_class_accuracy @ threshold = 0.43094382581115653: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>26514.0</td>\n",
       "<td>18546.0</td>\n",
       "<td>0.4116</td>\n",
       "<td> (18546.0/45060.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>14729.0</td>\n",
       "<td>21410.0</td>\n",
       "<td>0.4076</td>\n",
       "<td> (14729.0/36139.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>41243.0</td>\n",
       "<td>39956.0</td>\n",
       "<td>0.4098</td>\n",
       "<td> (33275.0/81199.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  -----------------\n",
       "0      26514  18546  0.4116   (18546.0/45060.0)\n",
       "1      14729  21410  0.4076   (14729.0/36139.0)\n",
       "Total  41243  39956  0.4098   (33275.0/81199.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Act/Pred) for max mean_per_class_accuracy @ threshold = 0.4441490224921936: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>32015.0</td>\n",
       "<td>13045.0</td>\n",
       "<td>0.2895</td>\n",
       "<td> (13045.0/45060.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>18870.0</td>\n",
       "<td>17269.0</td>\n",
       "<td>0.5222</td>\n",
       "<td> (18870.0/36139.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>50885.0</td>\n",
       "<td>30314.0</td>\n",
       "<td>0.393</td>\n",
       "<td> (31915.0/81199.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  -----------------\n",
       "0      32015  13045  0.2895   (13045.0/45060.0)\n",
       "1      18870  17269  0.5222   (18870.0/36139.0)\n",
       "Total  50885  30314  0.393    (31915.0/81199.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[, , , , , , , , , ]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm_bi_v1.confusion_matrix(metrics=[\"f1\",\"f2\",\"f0point5\",\"accuracy\",\"precision\",\"recall\",\"specificity\",\"absolute_mcc\",\"min_per_class_accuracy\",\"mean_per_class_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': {'actual': [0.0], 'default': None},\n",
       " 'balance_classes': {'actual': False, 'default': False},\n",
       " 'beta_constraints': {'actual': None, 'default': None},\n",
       " 'beta_epsilon': {'actual': 0.0001, 'default': 0.0001},\n",
       " 'class_sampling_factors': {'actual': None, 'default': None},\n",
       " 'compute_p_values': {'actual': False, 'default': False},\n",
       " 'custom_metric_func': {'actual': None, 'default': None},\n",
       " 'early_stopping': {'actual': True, 'default': True},\n",
       " 'export_checkpoints_dir': {'actual': None, 'default': None},\n",
       " 'family': {'actual': 'binomial', 'default': 'gaussian'},\n",
       " 'fold_assignment': {'actual': 'AUTO', 'default': 'AUTO'},\n",
       " 'fold_column': {'actual': None, 'default': None},\n",
       " 'gradient_epsilon': {'actual': 0.0001, 'default': -1.0},\n",
       " 'ignore_const_cols': {'actual': True, 'default': True},\n",
       " 'ignored_columns': {'actual': ['logerror'], 'default': None},\n",
       " 'interaction_pairs': {'actual': None, 'default': None},\n",
       " 'interactions': {'actual': None, 'default': None},\n",
       " 'intercept': {'actual': True, 'default': True},\n",
       " 'keep_cross_validation_fold_assignment': {'actual': False, 'default': False},\n",
       " 'keep_cross_validation_models': {'actual': True, 'default': True},\n",
       " 'keep_cross_validation_predictions': {'actual': False, 'default': False},\n",
       " 'lambda': {'actual': [0.003806316759372804], 'default': None},\n",
       " 'lambda_min_ratio': {'actual': 0.0001, 'default': -1.0},\n",
       " 'lambda_search': {'actual': False, 'default': False},\n",
       " 'link': {'actual': 'logit', 'default': 'family_default'},\n",
       " 'max_active_predictors': {'actual': -1, 'default': -1},\n",
       " 'max_after_balance_size': {'actual': 5.0, 'default': 5.0},\n",
       " 'max_confusion_matrix_size': {'actual': 20, 'default': 20},\n",
       " 'max_hit_ratio_k': {'actual': 0, 'default': 0},\n",
       " 'max_iterations': {'actual': 1560, 'default': -1},\n",
       " 'max_runtime_secs': {'actual': 0.0, 'default': 0.0},\n",
       " 'missing_values_handling': {'actual': 'MeanImputation',\n",
       "  'default': 'MeanImputation'},\n",
       " 'model_id': {'actual': {'URL': '/3/Models/glm__bi_v1',\n",
       "   '__meta': {'schema_name': 'ModelKeyV3',\n",
       "    'schema_type': 'Key<Model>',\n",
       "    'schema_version': 3},\n",
       "   'name': 'glm__bi_v1',\n",
       "   'type': 'Key<Model>'},\n",
       "  'default': None},\n",
       " 'nfolds': {'actual': 0, 'default': 0},\n",
       " 'nlambdas': {'actual': -1, 'default': -1},\n",
       " 'non_negative': {'actual': False, 'default': False},\n",
       " 'obj_reg': {'actual': 1.2315422603726647e-05, 'default': -1.0},\n",
       " 'objective_epsilon': {'actual': 0.0001, 'default': -1.0},\n",
       " 'offset_column': {'actual': None, 'default': None},\n",
       " 'prior': {'actual': -1.0, 'default': -1.0},\n",
       " 'remove_collinear_columns': {'actual': False, 'default': False},\n",
       " 'response_column': {'actual': {'__meta': {'schema_name': 'ColSpecifierV3',\n",
       "    'schema_type': 'VecSpecifier',\n",
       "    'schema_version': 3},\n",
       "   'column_name': 'logerror_highlow',\n",
       "   'is_member_of_frames': None},\n",
       "  'default': None},\n",
       " 'score_each_iteration': {'actual': False, 'default': False},\n",
       " 'seed': {'actual': -3222201996195017136, 'default': -1},\n",
       " 'solver': {'actual': 'L_BFGS', 'default': 'AUTO'},\n",
       " 'standardize': {'actual': True, 'default': True},\n",
       " 'training_frame': {'actual': {'URL': '/3/Frames/py_20_sid_a17d',\n",
       "   '__meta': {'schema_name': 'FrameKeyV3',\n",
       "    'schema_type': 'Key<Frame>',\n",
       "    'schema_version': 3},\n",
       "   'name': 'py_20_sid_a17d',\n",
       "   'type': 'Key<Frame>'},\n",
       "  'default': None},\n",
       " 'tweedie_link_power': {'actual': 1.0, 'default': 1.0},\n",
       " 'tweedie_variance_power': {'actual': 0.0, 'default': 0.0},\n",
       " 'validation_frame': {'actual': None, 'default': None},\n",
       " 'weights_column': {'actual': None, 'default': None}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm_bi_v1.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The First Multinomial Model\n",
    "\n",
    "Our goal is to perform classification on logerror_three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glm_multi_v1 = H2OGeneralizedLinearEstimator(\n",
    "                    model_id='glm_multi_v1',            \n",
    "                    family='multinomial',\n",
    "                    solver='L_BFGS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_multi='logerror_three'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  logerror_three</th><th style=\"text-align: right;\">  Count</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">               0</td><td style=\"text-align: right;\">   3279</td></tr>\n",
       "<tr><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">  74065</td></tr>\n",
       "<tr><td style=\"text-align: right;\">               2</td><td style=\"text-align: right;\">   3855</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['logerror_three'].table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glm Model Build progress: |███████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "glm_multi_v1.train(X, y_multi, training_frame=train,validation_frame=valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details\n",
      "=============\n",
      "H2OGeneralizedLinearEstimator :  Generalized Linear Modeling\n",
      "Model Key:  glm_multi_v1\n",
      "\n",
      "\n",
      "ModelMetricsMultinomialGLM: glm\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.0782950686954553\n",
      "RMSE: 0.27981255993156434\n",
      "\n",
      "ModelMetricsMultinomialGLM: glm\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 0.0771305207836345\n",
      "RMSE: 0.2777238210590415\n",
      "Scoring History: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>timestamp</b></td>\n",
       "<td><b>duration</b></td>\n",
       "<td><b>iterations</b></td>\n",
       "<td><b>negative_log_likelihood</b></td>\n",
       "<td><b>objective</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:31</td>\n",
       "<td> 0.000 sec</td>\n",
       "<td>0</td>\n",
       "<td>29082.7475273</td>\n",
       "<td>0.3581663</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:31</td>\n",
       "<td> 0.245 sec</td>\n",
       "<td>1</td>\n",
       "<td>27591.4464804</td>\n",
       "<td>0.3401143</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:31</td>\n",
       "<td> 0.526 sec</td>\n",
       "<td>2</td>\n",
       "<td>26794.2602264</td>\n",
       "<td>0.3308572</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:31</td>\n",
       "<td> 0.661 sec</td>\n",
       "<td>3</td>\n",
       "<td>25894.0745051</td>\n",
       "<td>0.3210827</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:31</td>\n",
       "<td> 0.717 sec</td>\n",
       "<td>4</td>\n",
       "<td>25257.3993601</td>\n",
       "<td>0.3151848</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:32</td>\n",
       "<td> 0.801 sec</td>\n",
       "<td>5</td>\n",
       "<td>24511.5909341</td>\n",
       "<td>0.3067779</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:32</td>\n",
       "<td> 0.919 sec</td>\n",
       "<td>6</td>\n",
       "<td>23974.7551035</td>\n",
       "<td>0.3022854</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:32</td>\n",
       "<td> 1.012 sec</td>\n",
       "<td>7</td>\n",
       "<td>23634.9022172</td>\n",
       "<td>0.2998932</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:32</td>\n",
       "<td> 1.103 sec</td>\n",
       "<td>8</td>\n",
       "<td>23479.9972365</td>\n",
       "<td>0.2990673</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:32</td>\n",
       "<td> 1.188 sec</td>\n",
       "<td>9</td>\n",
       "<td>23376.0079610</td>\n",
       "<td>0.2985514</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:32</td>\n",
       "<td> 1.268 sec</td>\n",
       "<td>10</td>\n",
       "<td>23307.2506358</td>\n",
       "<td>0.2982352</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:32</td>\n",
       "<td> 1.352 sec</td>\n",
       "<td>11</td>\n",
       "<td>23339.2821321</td>\n",
       "<td>0.2980550</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:32</td>\n",
       "<td> 1.443 sec</td>\n",
       "<td>12</td>\n",
       "<td>23300.4657770</td>\n",
       "<td>0.2979161</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:32</td>\n",
       "<td> 1.521 sec</td>\n",
       "<td>13</td>\n",
       "<td>23301.1962983</td>\n",
       "<td>0.2978655</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:32</td>\n",
       "<td> 1.596 sec</td>\n",
       "<td>14</td>\n",
       "<td>23251.0090535</td>\n",
       "<td>0.2977565</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:32</td>\n",
       "<td> 1.686 sec</td>\n",
       "<td>15</td>\n",
       "<td>23265.0223663</td>\n",
       "<td>0.2977061</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:33</td>\n",
       "<td> 1.780 sec</td>\n",
       "<td>16</td>\n",
       "<td>23274.3271493</td>\n",
       "<td>0.2976878</td></tr></table></div>"
      ],
      "text/plain": [
       "    timestamp            duration    iterations    negative_log_likelihood    objective\n",
       "--  -------------------  ----------  ------------  -------------------------  -----------\n",
       "    2019-02-20 01:34:31  0.000 sec   0             29082.7                    0.358166\n",
       "    2019-02-20 01:34:31  0.245 sec   1             27591.4                    0.340114\n",
       "    2019-02-20 01:34:31  0.526 sec   2             26794.3                    0.330857\n",
       "    2019-02-20 01:34:31  0.661 sec   3             25894.1                    0.321083\n",
       "    2019-02-20 01:34:31  0.717 sec   4             25257.4                    0.315185\n",
       "    2019-02-20 01:34:32  0.801 sec   5             24511.6                    0.306778\n",
       "    2019-02-20 01:34:32  0.919 sec   6             23974.8                    0.302285\n",
       "    2019-02-20 01:34:32  1.012 sec   7             23634.9                    0.299893\n",
       "    2019-02-20 01:34:32  1.103 sec   8             23480                      0.299067\n",
       "    2019-02-20 01:34:32  1.188 sec   9             23376                      0.298551\n",
       "    2019-02-20 01:34:32  1.268 sec   10            23307.3                    0.298235\n",
       "    2019-02-20 01:34:32  1.352 sec   11            23339.3                    0.298055\n",
       "    2019-02-20 01:34:32  1.443 sec   12            23300.5                    0.297916\n",
       "    2019-02-20 01:34:32  1.521 sec   13            23301.2                    0.297865\n",
       "    2019-02-20 01:34:32  1.596 sec   14            23251                      0.297757\n",
       "    2019-02-20 01:34:32  1.686 sec   15            23265                      0.297706\n",
       "    2019-02-20 01:34:33  1.780 sec   16            23274.3                    0.297688"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm_multi_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-3 Hit Ratios: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>k</b></td>\n",
       "<td><b>hit_ratio</b></td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>0.9143896</td></tr>\n",
       "<tr><td>2</td>\n",
       "<td>0.9997796</td></tr>\n",
       "<tr><td>3</td>\n",
       "<td>1.0</td></tr></table></div>"
      ],
      "text/plain": [
       "k    hit_ratio\n",
       "---  -----------\n",
       "1    0.91439\n",
       "2    0.99978\n",
       "3    1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm_multi_v1.hit_ratio_table(valid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>2</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0.0</td>\n",
       "<td>337.0</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>337 / 337</td></tr>\n",
       "<tr><td>3.0</td>\n",
       "<td>8299.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0003614</td>\n",
       "<td>3 / 8,302</td></tr>\n",
       "<tr><td>0.0</td>\n",
       "<td>437.0</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>437 / 437</td></tr>\n",
       "<tr><td>3.0</td>\n",
       "<td>9073.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0856104</td>\n",
       "<td>777 / 9,076</td></tr></table></div>"
      ],
      "text/plain": [
       "0    1     2    Error        Rate\n",
       "---  ----  ---  -----------  -----------\n",
       "0    337   0    1            337 / 337\n",
       "3    8299  0    0.000361359  3 / 8,302\n",
       "0    437   0    1            437 / 437\n",
       "3    9073  0    0.0856104    777 / 9,076"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm_multi_v1.confusion_matrix(data=valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "alpha\n",
    "\n",
    "You choose lasso regression by setting alpha to 1.0, and you choose ridge regression by setting alpha to 0.0.\n",
    "\n",
    "\n",
    "lambda\n",
    "\n",
    "Regularization strength. The default is chosen based on lambda max it is often useful to try lambda search to automatically find a good value. If you want to explicitly choose values for lambda that lambda search should try, specify them as a list here.\n",
    "\n",
    "\n",
    "lambda_search\n",
    "If true, then it will try multiple values of lambda for you.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glm_gaussian_ridge_v1= H2OGeneralizedLinearEstimator(\n",
    "                    model_id='glm_gaussian_ridge_v1',           \n",
    "                    family='gaussian',\n",
    "                    solver='L_BFGS',\n",
    "                    alpha=0.0,\n",
    "                    lambda_search=True, \n",
    "                    nlambdas=55      \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glm Model Build progress: |███████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "glm_gaussian_ridge_v1.train(X, y, training_frame=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details\n",
      "=============\n",
      "H2OGeneralizedLinearEstimator :  Generalized Linear Modeling\n",
      "Model Key:  glm_gaussian_ridge_v1\n",
      "\n",
      "\n",
      "ModelMetricsRegressionGLM: glm\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.012203493286508117\n",
      "RMSE: 0.11046942240506247\n",
      "MAE: 0.042285870464095945\n",
      "RMSLE: NaN\n",
      "R^2: 0.5247600916566801\n",
      "Mean Residual Deviance: 0.012203493286508117\n",
      "Null degrees of freedom: 81198\n",
      "Residual degrees of freedom: 75205\n",
      "Null deviance: 2085.076261430717\n",
      "Residual deviance: 990.9114513711726\n",
      "AIC: -115342.69635305945\n",
      "Scoring History: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>timestamp</b></td>\n",
       "<td><b>duration</b></td>\n",
       "<td><b>iteration</b></td>\n",
       "<td><b>lambda</b></td>\n",
       "<td><b>predictors</b></td>\n",
       "<td><b>deviance_train</b></td>\n",
       "<td><b>deviance_test</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:34</td>\n",
       "<td> 0.000 sec</td>\n",
       "<td>4</td>\n",
       "<td>.34E1</td>\n",
       "<td>5994</td>\n",
       "<td>0.0484573</td>\n",
       "<td>nan</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:34</td>\n",
       "<td> 0.088 sec</td>\n",
       "<td>7</td>\n",
       "<td>.29E1</td>\n",
       "<td>5994</td>\n",
       "<td>0.0480235</td>\n",
       "<td>nan</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:34</td>\n",
       "<td> 0.127 sec</td>\n",
       "<td>9</td>\n",
       "<td>.24E1</td>\n",
       "<td>5994</td>\n",
       "<td>0.0475509</td>\n",
       "<td>nan</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:35</td>\n",
       "<td> 0.166 sec</td>\n",
       "<td>11</td>\n",
       "<td>.21E1</td>\n",
       "<td>5994</td>\n",
       "<td>0.0470225</td>\n",
       "<td>nan</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:35</td>\n",
       "<td> 0.206 sec</td>\n",
       "<td>13</td>\n",
       "<td>.17E1</td>\n",
       "<td>5994</td>\n",
       "<td>0.0464490</td>\n",
       "<td>nan</td></tr>\n",
       "<tr><td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:39</td>\n",
       "<td> 4.495 sec</td>\n",
       "<td>195</td>\n",
       "<td>.19E-2</td>\n",
       "<td>5994</td>\n",
       "<td>0.0244094</td>\n",
       "<td>nan</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:39</td>\n",
       "<td> 4.587 sec</td>\n",
       "<td>196</td>\n",
       "<td>.16E-2</td>\n",
       "<td>5994</td>\n",
       "<td>0.0244090</td>\n",
       "<td>nan</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:39</td>\n",
       "<td> 4.609 sec</td>\n",
       "<td>197</td>\n",
       "<td>.13E-2</td>\n",
       "<td>5994</td>\n",
       "<td>0.0244084</td>\n",
       "<td>nan</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:39</td>\n",
       "<td> 4.671 sec</td>\n",
       "<td>198</td>\n",
       "<td>.11E-2</td>\n",
       "<td>5994</td>\n",
       "<td>0.0244078</td>\n",
       "<td>nan</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-20 01:34:39</td>\n",
       "<td> 4.710 sec</td>\n",
       "<td>199</td>\n",
       "<td>.95E-3</td>\n",
       "<td>5994</td>\n",
       "<td>0.0244070</td>\n",
       "<td>nan</td></tr></table></div>"
      ],
      "text/plain": [
       "     timestamp            duration    iteration    lambda    predictors    deviance_train        deviance_test\n",
       "---  -------------------  ----------  -----------  --------  ------------  --------------------  ---------------\n",
       "     2019-02-20 01:34:34  0.000 sec   4            .34E1     5994          0.048457332398541365  nan\n",
       "     2019-02-20 01:34:34  0.088 sec   7            .29E1     5994          0.04802347476629449   nan\n",
       "     2019-02-20 01:34:34  0.127 sec   9            .24E1     5994          0.04755085381787862   nan\n",
       "     2019-02-20 01:34:35  0.166 sec   11           .21E1     5994          0.047022514760927704  nan\n",
       "     2019-02-20 01:34:35  0.206 sec   13           .17E1     5994          0.046448993829197635  nan\n",
       "---  ---                  ---         ---          ---       ---           ---                   ---\n",
       "     2019-02-20 01:34:39  4.495 sec   195          .19E-2    5994          0.0244094050122476    nan\n",
       "     2019-02-20 01:34:39  4.587 sec   196          .16E-2    5994          0.02440903029021894   nan\n",
       "     2019-02-20 01:34:39  4.609 sec   197          .13E-2    5994          0.024408352657639923  nan\n",
       "     2019-02-20 01:34:39  4.671 sec   198          .11E-2    5994          0.02440777868692261   nan\n",
       "     2019-02-20 01:34:39  4.710 sec   199          .95E-3    5994          0.024406986546478396  nan"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "See the whole table with table.as_data_frame()\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm_gaussian_ridge_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lambda',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_bc',\n",
       " '_bcin',\n",
       " '_check_and_save_parm',\n",
       " '_check_targets',\n",
       " '_compute_algo',\n",
       " '_end_time',\n",
       " '_estimator_type',\n",
       " '_future',\n",
       " '_get_metrics',\n",
       " '_have_mojo',\n",
       " '_have_pojo',\n",
       " '_id',\n",
       " '_is_xvalidated',\n",
       " '_job',\n",
       " '_keyify_if_h2oframe',\n",
       " '_make_model',\n",
       " '_metrics_class',\n",
       " '_model_json',\n",
       " '_parms',\n",
       " '_plot',\n",
       " '_requires_training_frame',\n",
       " '_resolve_model',\n",
       " '_run_time',\n",
       " '_start_time',\n",
       " '_verify_training_frame_params',\n",
       " '_xval_keys',\n",
       " 'actual_params',\n",
       " 'aic',\n",
       " 'algo',\n",
       " 'alpha',\n",
       " 'auc',\n",
       " 'balance_classes',\n",
       " 'beta_constraints',\n",
       " 'beta_epsilon',\n",
       " 'biases',\n",
       " 'catoffsets',\n",
       " 'class_sampling_factors',\n",
       " 'coef',\n",
       " 'coef_norm',\n",
       " 'compute_p_values',\n",
       " 'convert_H2OXGBoostParams_2_XGBoostParams',\n",
       " 'cross_validation_fold_assignment',\n",
       " 'cross_validation_holdout_predictions',\n",
       " 'cross_validation_metrics_summary',\n",
       " 'cross_validation_models',\n",
       " 'cross_validation_predictions',\n",
       " 'custom_metric_func',\n",
       " 'deepfeatures',\n",
       " 'default_params',\n",
       " 'download_mojo',\n",
       " 'download_pojo',\n",
       " 'early_stopping',\n",
       " 'end_time',\n",
       " 'export_checkpoints_dir',\n",
       " 'family',\n",
       " 'fit',\n",
       " 'fold_assignment',\n",
       " 'fold_column',\n",
       " 'full_parameters',\n",
       " 'getGLMRegularizationPath',\n",
       " 'get_params',\n",
       " 'get_xval_models',\n",
       " 'gini',\n",
       " 'gradient_epsilon',\n",
       " 'have_mojo',\n",
       " 'have_pojo',\n",
       " 'ignore_const_cols',\n",
       " 'ignored_columns',\n",
       " 'interaction_pairs',\n",
       " 'interactions',\n",
       " 'intercept',\n",
       " 'is_cross_validated',\n",
       " 'join',\n",
       " 'keep_cross_validation_fold_assignment',\n",
       " 'keep_cross_validation_models',\n",
       " 'keep_cross_validation_predictions',\n",
       " 'lambda_',\n",
       " 'lambda_min_ratio',\n",
       " 'lambda_search',\n",
       " 'link',\n",
       " 'logloss',\n",
       " 'mae',\n",
       " 'makeGLMModel',\n",
       " 'max_active_predictors',\n",
       " 'max_after_balance_size',\n",
       " 'max_confusion_matrix_size',\n",
       " 'max_hit_ratio_k',\n",
       " 'max_iterations',\n",
       " 'max_runtime_secs',\n",
       " 'mean_residual_deviance',\n",
       " 'missing_values_handling',\n",
       " 'mixin',\n",
       " 'model_id',\n",
       " 'model_performance',\n",
       " 'mse',\n",
       " 'nfolds',\n",
       " 'nlambdas',\n",
       " 'non_negative',\n",
       " 'normmul',\n",
       " 'normsub',\n",
       " 'null_degrees_of_freedom',\n",
       " 'null_deviance',\n",
       " 'obj_reg',\n",
       " 'objective_epsilon',\n",
       " 'offset_column',\n",
       " 'params',\n",
       " 'parms',\n",
       " 'partial_plot',\n",
       " 'plot',\n",
       " 'pprint_coef',\n",
       " 'predict',\n",
       " 'predict_leaf_node_assignment',\n",
       " 'prior',\n",
       " 'r2',\n",
       " 'remove_collinear_columns',\n",
       " 'residual_degrees_of_freedom',\n",
       " 'residual_deviance',\n",
       " 'respmul',\n",
       " 'response_column',\n",
       " 'respsub',\n",
       " 'rmse',\n",
       " 'rmsle',\n",
       " 'rotation',\n",
       " 'run_time',\n",
       " 'save_model_details',\n",
       " 'save_mojo',\n",
       " 'score_each_iteration',\n",
       " 'score_history',\n",
       " 'scoring_history',\n",
       " 'seed',\n",
       " 'set_params',\n",
       " 'show',\n",
       " 'solver',\n",
       " 'staged_predict_proba',\n",
       " 'standardize',\n",
       " 'start',\n",
       " 'start_time',\n",
       " 'std_coef_plot',\n",
       " 'summary',\n",
       " 'train',\n",
       " 'training_frame',\n",
       " 'tweedie_link_power',\n",
       " 'tweedie_variance_power',\n",
       " 'type',\n",
       " 'validation_frame',\n",
       " 'varimp',\n",
       " 'varimp_plot',\n",
       " 'weights',\n",
       " 'weights_column',\n",
       " 'xval_keys',\n",
       " 'xvals']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(glm_gaussian_ridge_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': {'actual': [0.0], 'default': None},\n",
       " 'balance_classes': {'actual': False, 'default': False},\n",
       " 'beta_constraints': {'actual': None, 'default': None},\n",
       " 'beta_epsilon': {'actual': 0.0001, 'default': 0.0001},\n",
       " 'class_sampling_factors': {'actual': None, 'default': None},\n",
       " 'compute_p_values': {'actual': False, 'default': False},\n",
       " 'custom_metric_func': {'actual': None, 'default': None},\n",
       " 'early_stopping': {'actual': True, 'default': True},\n",
       " 'export_checkpoints_dir': {'actual': None, 'default': None},\n",
       " 'family': {'actual': 'gaussian', 'default': 'gaussian'},\n",
       " 'fold_assignment': {'actual': 'AUTO', 'default': 'AUTO'},\n",
       " 'fold_column': {'actual': None, 'default': None},\n",
       " 'gradient_epsilon': {'actual': 1.0000000000000002e-06, 'default': -1.0},\n",
       " 'ignore_const_cols': {'actual': True, 'default': True},\n",
       " 'ignored_columns': {'actual': None, 'default': None},\n",
       " 'interaction_pairs': {'actual': None, 'default': None},\n",
       " 'interactions': {'actual': None, 'default': None},\n",
       " 'intercept': {'actual': True, 'default': True},\n",
       " 'keep_cross_validation_fold_assignment': {'actual': False, 'default': False},\n",
       " 'keep_cross_validation_models': {'actual': True, 'default': True},\n",
       " 'keep_cross_validation_predictions': {'actual': False, 'default': False},\n",
       " 'lambda': {'actual': [3.4242290227118284,\n",
       "   2.887278851750621,\n",
       "   2.4345273381172277,\n",
       "   2.0527713686007605,\n",
       "   1.7308781979034538,\n",
       "   1.4594607961721733,\n",
       "   1.2306041049818135,\n",
       "   1.0376342188635517,\n",
       "   0.8749237612631602,\n",
       "   0.7377277793144339,\n",
       "   0.6220453718006963,\n",
       "   0.5245030151070738,\n",
       "   0.44225618472177064,\n",
       "   0.3729064033783074,\n",
       "   0.31443129680148835,\n",
       "   0.2651256173468458,\n",
       "   0.22355151566837728,\n",
       "   0.1884966102398527,\n",
       "   0.1589386319555203,\n",
       "   0.13401561277812016,\n",
       "   0.11300074907729973,\n",
       "   0.09528120662457318,\n",
       "   0.08034024915732486,\n",
       "   0.0677421693460838,\n",
       "   0.05711958272281638,\n",
       "   0.048162714036515834,\n",
       "   0.0406103636054158,\n",
       "   0.03424229022711831,\n",
       "   0.028872788517506232,\n",
       "   0.0243452733811723,\n",
       "   0.020527713686007625,\n",
       "   0.017308781979034554,\n",
       "   0.014594607961721746,\n",
       "   0.012306041049818146,\n",
       "   0.010376342188635526,\n",
       "   0.008749237612631609,\n",
       "   0.007377277793144345,\n",
       "   0.006220453718006968,\n",
       "   0.005245030151070741,\n",
       "   0.004422561847217709,\n",
       "   0.0037290640337830764,\n",
       "   0.0031443129680148855,\n",
       "   0.0026512561734684595,\n",
       "   0.002235515156683774,\n",
       "   0.001884966102398528,\n",
       "   0.0015893863195552038,\n",
       "   0.0013401561277812022,\n",
       "   0.001130007490772998,\n",
       "   0.0009528120662457322,\n",
       "   0.000803402491573249,\n",
       "   0.0006774216934608383,\n",
       "   0.000571195827228164,\n",
       "   0.00048162714036515856,\n",
       "   0.0004061036360541582,\n",
       "   0.00034242290227118327],\n",
       "  'default': None},\n",
       " 'lambda_min_ratio': {'actual': 0.0001, 'default': -1.0},\n",
       " 'lambda_search': {'actual': True, 'default': False},\n",
       " 'link': {'actual': 'identity', 'default': 'family_default'},\n",
       " 'max_active_predictors': {'actual': -1, 'default': -1},\n",
       " 'max_after_balance_size': {'actual': 5.0, 'default': 5.0},\n",
       " 'max_confusion_matrix_size': {'actual': 20, 'default': 20},\n",
       " 'max_hit_ratio_k': {'actual': 0, 'default': 0},\n",
       " 'max_iterations': {'actual': 5500, 'default': -1},\n",
       " 'max_runtime_secs': {'actual': 0.0, 'default': 0.0},\n",
       " 'missing_values_handling': {'actual': 'MeanImputation',\n",
       "  'default': 'MeanImputation'},\n",
       " 'model_id': {'actual': {'URL': '/3/Models/glm_gaussian_ridge_v1',\n",
       "   '__meta': {'schema_name': 'ModelKeyV3',\n",
       "    'schema_type': 'Key<Model>',\n",
       "    'schema_version': 3},\n",
       "   'name': 'glm_gaussian_ridge_v1',\n",
       "   'type': 'Key<Model>'},\n",
       "  'default': None},\n",
       " 'nfolds': {'actual': 0, 'default': 0},\n",
       " 'nlambdas': {'actual': 55, 'default': -1},\n",
       " 'non_negative': {'actual': False, 'default': False},\n",
       " 'obj_reg': {'actual': 1.2315422603726647e-05, 'default': -1.0},\n",
       " 'objective_epsilon': {'actual': 0.0001, 'default': -1.0},\n",
       " 'offset_column': {'actual': None, 'default': None},\n",
       " 'prior': {'actual': -1.0, 'default': -1.0},\n",
       " 'remove_collinear_columns': {'actual': False, 'default': False},\n",
       " 'response_column': {'actual': {'__meta': {'schema_name': 'ColSpecifierV3',\n",
       "    'schema_type': 'VecSpecifier',\n",
       "    'schema_version': 3},\n",
       "   'column_name': 'logerror',\n",
       "   'is_member_of_frames': None},\n",
       "  'default': None},\n",
       " 'score_each_iteration': {'actual': False, 'default': False},\n",
       " 'seed': {'actual': -6402997081346990640, 'default': -1},\n",
       " 'solver': {'actual': 'L_BFGS', 'default': 'AUTO'},\n",
       " 'standardize': {'actual': True, 'default': True},\n",
       " 'training_frame': {'actual': {'URL': '/3/Frames/py_20_sid_a17d',\n",
       "   '__meta': {'schema_name': 'FrameKeyV3',\n",
       "    'schema_type': 'Key<Frame>',\n",
       "    'schema_version': 3},\n",
       "   'name': 'py_20_sid_a17d',\n",
       "   'type': 'Key<Frame>'},\n",
       "  'default': None},\n",
       " 'tweedie_link_power': {'actual': 1.0, 'default': 1.0},\n",
       " 'tweedie_variance_power': {'actual': 0.0, 'default': 0.0},\n",
       " 'validation_frame': {'actual': None, 'default': None},\n",
       " 'weights_column': {'actual': None, 'default': None}}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm_gaussian_ridge_v1.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11076795336177944\n",
      "-0.11076795336177944\n",
      "0.11046942240506247\n",
      "-0.11046942240506247\n"
     ]
    }
   ],
   "source": [
    "rmse=glm_gaussian_v1.rmse()\n",
    "print(rmse)\n",
    "rmse_neg=(-1*rmse)\n",
    "print(rmse_neg)\n",
    "ridge_rsme=glm_gaussian_ridge_v1.rmse()\n",
    "print(ridge_rsme)\n",
    "ridge_rsme_neg=(-1*ridge_rsme)\n",
    "print(ridge_rsme_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAFyCAYAAADYhIJtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJCkRJm1JkhJh0pYkKRH/D8pUcnr+BgLcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show the graph of logerror with lines\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(range(logerror_array.shape[0]), np.sort(logerror_array))\n",
    "plt.xlabel('index', fontsize=12)\n",
    "plt.ylabel('logerror', fontsize=12)\n",
    "#####horizontal line\n",
    "cols=hf.shape[0]\n",
    "steps=100\n",
    "xs = np.arange(0,cols,(cols/steps))\n",
    "horiz_line_data = np.array([rmse for i in list(range(steps))])\n",
    "plt.plot(xs, horiz_line_data, 'r--') \n",
    "horiz_line_data = np.array([rmse_neg for i in list(range(steps))])\n",
    "plt.plot(xs, horiz_line_data, 'r--') \n",
    "horiz_line_data = np.array([ridge_rsme for i in list(range(steps))])\n",
    "plt.plot(xs, horiz_line_data, 'go') \n",
    "horiz_line_data = np.array([ridge_rsme_neg for i in list(range(steps))])\n",
    "plt.plot(xs, horiz_line_data, 'go') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_lambda(n):\n",
    "  l=np.linspace(0.1, 999.0, num=555)\n",
    "  indices = random.sample(range(len(l)), n)\n",
    "  lam=[l[i] for i in sorted(indices)] \n",
    "  lam.insert(0,0.0)\n",
    "  lam.append(1000.0)\n",
    "  return lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 12.72148014440433, 14.524548736462092, 21.736823104693141, 25.342960288808662, 30.752166064981949, 43.37364620938628, 65.010469314079415, 66.813537906137171]\n"
     ]
    }
   ],
   "source": [
    "lambd=gen_lambda(99)\n",
    "print(lambd[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select the values for lambda_ to grid over\n",
    "hyper_params = {'lambda': lambd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import Grid Search\n",
    "from h2o.grid.grid_search import H2OGridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_gaussian_ridge_v2= H2OGeneralizedLinearEstimator(\n",
    "                    model_id='glm_gaussian_ridge_v2',           \n",
    "                    family='gaussian',\n",
    "                    solver='L_BFGS',\n",
    "                    alpha=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build grid search with previously made GLM and hyperparameters\n",
    "grid = H2OGridSearch(model = glm_gaussian_ridge_v2, hyper_params = hyper_params,\n",
    "                     search_criteria = {'strategy': \"Cartesian\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glm Grid Build progress: |████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# train using the grid\n",
    "grid.train(x = X, y = y, training_frame = train, validation_frame = valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      lambda  \\\n",
      "0                   [1000.0]   \n",
      "1        [997.1969314079422]   \n",
      "2        [961.1355595667869]   \n",
      "3        [950.3171480144404]   \n",
      "4        [943.1048736462093]   \n",
      "5         [923.271119133574]   \n",
      "6        [916.0588447653429]   \n",
      "7        [907.0435018050541]   \n",
      "8        [905.2404332129963]   \n",
      "9        [901.6342960288808]   \n",
      "10       [896.2250902527076]   \n",
      "11       [874.5882671480144]   \n",
      "12       [829.5115523465704]   \n",
      "13       [820.4962093862815]   \n",
      "14        [816.890072202166]   \n",
      "15       [811.4808664259928]   \n",
      "16       [797.0563176895306]   \n",
      "17       [791.6471119133573]   \n",
      "18       [784.4348375451264]   \n",
      "19       [779.0256317689531]   \n",
      "20       [751.9796028880866]   \n",
      "21       [742.9642599277978]   \n",
      "22       [735.7519855595667]   \n",
      "23       [732.1458483754512]   \n",
      "24       [728.5397111913358]   \n",
      "25       [721.3274368231047]   \n",
      "26       [710.5090252707581]   \n",
      "27       [694.2814079422383]   \n",
      "28       [685.2660649819494]   \n",
      "29       [681.6599277978339]   \n",
      "..  ..                   ...   \n",
      "71      [252.52960288808663]   \n",
      "72      [250.72653429602886]   \n",
      "73      [230.89277978339348]   \n",
      "74      [214.66516245487364]   \n",
      "75      [209.25595667870033]   \n",
      "76        [196.634476534296]   \n",
      "77      [189.42220216606495]   \n",
      "78      [171.39151624548734]   \n",
      "79      [167.78537906137183]   \n",
      "80       [164.1792418772563]   \n",
      "81      [162.37617328519855]   \n",
      "82      [153.36083032490973]   \n",
      "83       [137.1332129963899]   \n",
      "84      [131.72400722021658]   \n",
      "85      [119.10252707581226]   \n",
      "86      [108.28411552346569]   \n",
      "87      [101.07184115523464]   \n",
      "88       [95.66263537906136]   \n",
      "89       [86.64729241877255]   \n",
      "90        [84.8442238267148]   \n",
      "91       [68.61660649819494]   \n",
      "92       [66.81353790613717]   \n",
      "93       [65.01046931407942]   \n",
      "94       [43.37364620938628]   \n",
      "95       [30.75216606498195]   \n",
      "96      [25.342960288808662]   \n",
      "97       [21.73682310469314]   \n",
      "98      [14.524548736462092]   \n",
      "99       [12.72148014440433]   \n",
      "100                    [0.0]   \n",
      "\n",
      "                                                          model_ids  \\\n",
      "0    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_101   \n",
      "1    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_100   \n",
      "2     Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_99   \n",
      "3     Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_98   \n",
      "4     Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_97   \n",
      "5     Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_96   \n",
      "6     Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_95   \n",
      "7     Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_94   \n",
      "8     Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_93   \n",
      "9     Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_92   \n",
      "10    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_91   \n",
      "11    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_90   \n",
      "12    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_89   \n",
      "13    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_88   \n",
      "14    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_87   \n",
      "15    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_86   \n",
      "16    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_85   \n",
      "17    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_84   \n",
      "18    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_83   \n",
      "19    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_82   \n",
      "20    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_81   \n",
      "21    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_80   \n",
      "22    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_79   \n",
      "23    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_78   \n",
      "24    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_77   \n",
      "25    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_76   \n",
      "26    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_75   \n",
      "27    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_74   \n",
      "28    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_73   \n",
      "29    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_72   \n",
      "..                                                              ...   \n",
      "71    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_30   \n",
      "72    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_29   \n",
      "73    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_28   \n",
      "74    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_27   \n",
      "75    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_26   \n",
      "76    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_25   \n",
      "77    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_24   \n",
      "78    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_23   \n",
      "79    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_22   \n",
      "80    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_21   \n",
      "81    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_20   \n",
      "82    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_19   \n",
      "83    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_18   \n",
      "84    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_17   \n",
      "85    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_16   \n",
      "86    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_15   \n",
      "87    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_14   \n",
      "88    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_13   \n",
      "89    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_12   \n",
      "90    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_11   \n",
      "91    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_10   \n",
      "92     Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_9   \n",
      "93     Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_8   \n",
      "94     Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_7   \n",
      "95     Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_6   \n",
      "96     Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_5   \n",
      "97     Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_4   \n",
      "98     Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_3   \n",
      "99     Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_2   \n",
      "100    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_1   \n",
      "\n",
      "                    rmse  \n",
      "0    0.16832696725641452  \n",
      "1    0.16832691714555964  \n",
      "2    0.16832624637223026  \n",
      "3    0.16832603520136502  \n",
      "4    0.16832589172601778  \n",
      "5     0.1683254855964429  \n",
      "6    0.16832533354725632  \n",
      "7     0.1683251400811547  \n",
      "8    0.16832510092493203  \n",
      "9     0.1683250221420677  \n",
      "10   0.16832490277750944  \n",
      "11   0.16832441053489214  \n",
      "12    0.1683233024392689  \n",
      "13   0.16832306618929394  \n",
      "14   0.16832297022717224  \n",
      "15    0.1683228246825627  \n",
      "16    0.1683224268918783  \n",
      "17   0.16832227397765798  \n",
      "18    0.1683220668067447  \n",
      "19   0.16832190890748244  \n",
      "20   0.16832108528486375  \n",
      "21   0.16832079739803057  \n",
      "22   0.16832056200099715  \n",
      "23   0.16832044256061252  \n",
      "24    0.1683203219359547  \n",
      "25    0.1683200770626603  \n",
      "26    0.1683197004164291  \n",
      "27   0.16831911340218078  \n",
      "28     0.168318775249933  \n",
      "29   0.16831863748036519  \n",
      "..                   ...  \n",
      "71   0.16827402153511345  \n",
      "72   0.16827351040427796  \n",
      "73   0.16826735848135072  \n",
      "74   0.16826147503355887  \n",
      "75    0.1682593099964812  \n",
      "76   0.16825379232396215  \n",
      "77   0.16825030724211362  \n",
      "78   0.16824030275688648  \n",
      "79   0.16823804207757306  \n",
      "80   0.16823568139787223  \n",
      "81   0.16823446146124027  \n",
      "82   0.16822792829093133  \n",
      "83   0.16821398588581124  \n",
      "84   0.16820856841962328  \n",
      "85   0.16819399568746243  \n",
      "86   0.16817877245013205  \n",
      "87   0.16816679273144478  \n",
      "88    0.1681566084154992  \n",
      "89   0.16813677176450154  \n",
      "90    0.1681322917926667  \n",
      "91   0.16808120174711416  \n",
      "92   0.16807396717033907  \n",
      "93    0.1680663243194784  \n",
      "94   0.16792375524191636  \n",
      "95    0.1677445721401441  \n",
      "96   0.16761079356212435  \n",
      "97   0.16755875138250287  \n",
      "98   0.16718855643912506  \n",
      "99    0.1670323015735265  \n",
      "100  0.13447460407523354  \n",
      "\n",
      "[101 rows x 4 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sort the grid models by decreasing AUC\n",
    "sorted_grid = grid.get_grid(sort_by = 'rmse', decreasing = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      lambda  \\\n",
      "0                   [1000.0]   \n",
      "1        [997.1969314079422]   \n",
      "2        [961.1355595667869]   \n",
      "3        [950.3171480144404]   \n",
      "4        [943.1048736462093]   \n",
      "5         [923.271119133574]   \n",
      "6        [916.0588447653429]   \n",
      "7        [907.0435018050541]   \n",
      "8        [905.2404332129963]   \n",
      "9        [901.6342960288808]   \n",
      "10       [896.2250902527076]   \n",
      "11       [874.5882671480144]   \n",
      "12       [829.5115523465704]   \n",
      "13       [820.4962093862815]   \n",
      "14        [816.890072202166]   \n",
      "15       [811.4808664259928]   \n",
      "16       [797.0563176895306]   \n",
      "17       [791.6471119133573]   \n",
      "18       [784.4348375451264]   \n",
      "19       [779.0256317689531]   \n",
      "20       [751.9796028880866]   \n",
      "21       [742.9642599277978]   \n",
      "22       [735.7519855595667]   \n",
      "23       [732.1458483754512]   \n",
      "24       [728.5397111913358]   \n",
      "25       [721.3274368231047]   \n",
      "26       [710.5090252707581]   \n",
      "27       [694.2814079422383]   \n",
      "28       [685.2660649819494]   \n",
      "29       [681.6599277978339]   \n",
      "..  ..                   ...   \n",
      "71      [252.52960288808663]   \n",
      "72      [250.72653429602886]   \n",
      "73      [230.89277978339348]   \n",
      "74      [214.66516245487364]   \n",
      "75      [209.25595667870033]   \n",
      "76        [196.634476534296]   \n",
      "77      [189.42220216606495]   \n",
      "78      [171.39151624548734]   \n",
      "79      [167.78537906137183]   \n",
      "80       [164.1792418772563]   \n",
      "81      [162.37617328519855]   \n",
      "82      [153.36083032490973]   \n",
      "83       [137.1332129963899]   \n",
      "84      [131.72400722021658]   \n",
      "85      [119.10252707581226]   \n",
      "86      [108.28411552346569]   \n",
      "87      [101.07184115523464]   \n",
      "88       [95.66263537906136]   \n",
      "89       [86.64729241877255]   \n",
      "90        [84.8442238267148]   \n",
      "91       [68.61660649819494]   \n",
      "92       [66.81353790613717]   \n",
      "93       [65.01046931407942]   \n",
      "94       [43.37364620938628]   \n",
      "95       [30.75216606498195]   \n",
      "96      [25.342960288808662]   \n",
      "97       [21.73682310469314]   \n",
      "98      [14.524548736462092]   \n",
      "99       [12.72148014440433]   \n",
      "100                    [0.0]   \n",
      "\n",
      "                                                          model_ids  \\\n",
      "0    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_101   \n",
      "1    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_100   \n",
      "2     Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_99   \n",
      "3     Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_98   \n",
      "4     Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_97   \n",
      "5     Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_96   \n",
      "6     Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_95   \n",
      "7     Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_94   \n",
      "8     Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_93   \n",
      "9     Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_92   \n",
      "10    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_91   \n",
      "11    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_90   \n",
      "12    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_89   \n",
      "13    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_88   \n",
      "14    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_87   \n",
      "15    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_86   \n",
      "16    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_85   \n",
      "17    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_84   \n",
      "18    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_83   \n",
      "19    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_82   \n",
      "20    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_81   \n",
      "21    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_80   \n",
      "22    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_79   \n",
      "23    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_78   \n",
      "24    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_77   \n",
      "25    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_76   \n",
      "26    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_75   \n",
      "27    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_74   \n",
      "28    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_73   \n",
      "29    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_72   \n",
      "..                                                              ...   \n",
      "71    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_30   \n",
      "72    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_29   \n",
      "73    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_28   \n",
      "74    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_27   \n",
      "75    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_26   \n",
      "76    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_25   \n",
      "77    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_24   \n",
      "78    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_23   \n",
      "79    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_22   \n",
      "80    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_21   \n",
      "81    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_20   \n",
      "82    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_19   \n",
      "83    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_18   \n",
      "84    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_17   \n",
      "85    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_16   \n",
      "86    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_15   \n",
      "87    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_14   \n",
      "88    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_13   \n",
      "89    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_12   \n",
      "90    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_11   \n",
      "91    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_10   \n",
      "92     Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_9   \n",
      "93     Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_8   \n",
      "94     Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_7   \n",
      "95     Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_6   \n",
      "96     Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_5   \n",
      "97     Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_4   \n",
      "98     Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_3   \n",
      "99     Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_2   \n",
      "100    Grid_GLM_py_20_sid_a17d_model_python_1550644443911_2_model_1   \n",
      "\n",
      "                    rmse  \n",
      "0    0.16832696725641452  \n",
      "1    0.16832691714555964  \n",
      "2    0.16832624637223026  \n",
      "3    0.16832603520136502  \n",
      "4    0.16832589172601778  \n",
      "5     0.1683254855964429  \n",
      "6    0.16832533354725632  \n",
      "7     0.1683251400811547  \n",
      "8    0.16832510092493203  \n",
      "9     0.1683250221420677  \n",
      "10   0.16832490277750944  \n",
      "11   0.16832441053489214  \n",
      "12    0.1683233024392689  \n",
      "13   0.16832306618929394  \n",
      "14   0.16832297022717224  \n",
      "15    0.1683228246825627  \n",
      "16    0.1683224268918783  \n",
      "17   0.16832227397765798  \n",
      "18    0.1683220668067447  \n",
      "19   0.16832190890748244  \n",
      "20   0.16832108528486375  \n",
      "21   0.16832079739803057  \n",
      "22   0.16832056200099715  \n",
      "23   0.16832044256061252  \n",
      "24    0.1683203219359547  \n",
      "25    0.1683200770626603  \n",
      "26    0.1683197004164291  \n",
      "27   0.16831911340218078  \n",
      "28     0.168318775249933  \n",
      "29   0.16831863748036519  \n",
      "..                   ...  \n",
      "71   0.16827402153511345  \n",
      "72   0.16827351040427796  \n",
      "73   0.16826735848135072  \n",
      "74   0.16826147503355887  \n",
      "75    0.1682593099964812  \n",
      "76   0.16825379232396215  \n",
      "77   0.16825030724211362  \n",
      "78   0.16824030275688648  \n",
      "79   0.16823804207757306  \n",
      "80   0.16823568139787223  \n",
      "81   0.16823446146124027  \n",
      "82   0.16822792829093133  \n",
      "83   0.16821398588581124  \n",
      "84   0.16820856841962328  \n",
      "85   0.16819399568746243  \n",
      "86   0.16817877245013205  \n",
      "87   0.16816679273144478  \n",
      "88    0.1681566084154992  \n",
      "89   0.16813677176450154  \n",
      "90    0.1681322917926667  \n",
      "91   0.16808120174711416  \n",
      "92   0.16807396717033907  \n",
      "93    0.1680663243194784  \n",
      "94   0.16792375524191636  \n",
      "95    0.1677445721401441  \n",
      "96   0.16761079356212435  \n",
      "97   0.16755875138250287  \n",
      "98   0.16718855643912506  \n",
      "99    0.1670323015735265  \n",
      "100  0.13447460407523354  \n",
      "\n",
      "[101 rows x 4 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.cluster().shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
