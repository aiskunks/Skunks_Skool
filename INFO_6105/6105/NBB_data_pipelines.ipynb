{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import random\n",
    "from datetime import datetime\n",
    "random.seed(datetime.now())\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Make plots larger\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A data pipeline is a set of data processing elements connected in series, where the output of one element is the input of the next one. The elements of a pipeline are often executed in parallel or in time-sliced fashion; in that case, some amount of buffer storage is often inserted between elements.\n",
    "\n",
    "Software pipelines, where commands can be written where the output of one operation is automatically fed to the next, following operation. The Unix system call pipe is a classic example of this concept, although other operating systems do support pipes as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Queue\n",
    "\n",
    "\n",
    "In computer science, a [queue](https://en.wikipedia.org/wiki/Queue_(abstract_data_type)) is a particular kind of abstract data type or collection in which the entities in the collection are kept in order and the principle (or only) operations on the collection are the addition of entities to the rear terminal position, known as *enqueue,* and removal of entities from the front terminal position, known as *dequeue*. This makes the queue a First-In-First-Out (FIFO) data structure. In a FIFO data structure, the first element added to the queue will be the first one to be removed. This is equivalent to the requirement that once a new element is added, all elements that were\n",
    "added before have to be removed before the new element can be removed.\n",
    "Often a *peek* or *front* operation is also entered, returning the\n",
    "value of the front element without dequeuing it. A queue is an example\n",
    "of a linear data structure, or more abstractly a sequential\n",
    "collection.\n",
    "\n",
    "![Representation of a FIFO (first in, first out) queue](http://nikbearbrown.com/YouTube/MachineLearning/IMG/405px-Data_Queue.svg.png)  \n",
    "\n",
    "_Representation of a FIFO (first in, first out) queue_\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Directed acyclic graph\n",
    "\n",
    "In mathematics and computer science, a [directed acyclic graph](https://en.wikipedia.org/wiki/Directed_acyclic_graph)\n",
    "(**DAG**), is a finite directed graph with no directed cycles. That\n",
    "is, it consists of finitely many vertices and edges, with each edge\n",
    "directed from one vertex to another, such that there is no way to start\n",
    "at any vertex and follow a consistently-directed sequence of edges that\n",
    "eventually loops back to again. Equivalently, a DAG is a directed graph\n",
    "that has a topological ordering, a sequence of the vertices such that\n",
    "every edge is directed from earlier to later in the sequence.\n",
    "\n",
    "![A topological ordering of a directed acyclic graph](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Topological_Ordering.svg.png)\n",
    "\n",
    "_A topological ordering of a directed acyclic graph_  \n",
    "\n",
    "\n",
    "![A DAG](http://nikbearbrown.com/YouTube/MachineLearning/IMG/252px-DAG-Tred-G.svg.png)\n",
    "\n",
    "_A DAG_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RabbitMQ \n",
    "\n",
    "[RabbitMQ](https://www.rabbitmq.com) is a message broker: it accepts and forwards messages. You can think about it as a post office: when you put the mail that you want posting in a post box, you can be sure that Mr. Postman will eventually deliver the mail to your recipient. In this analogy, RabbitMQ is a post box, a post office and a postman.\n",
    "\n",
    "The major difference between RabbitMQ and the post office is that it doesn't deal with paper, instead it accepts, stores and forwards binary blobs of data ‒ messages.\n",
    "\n",
    "RabbitMQ, and messaging in general, uses some jargon.\n",
    "\n",
    "* Producing - _Producing_ means nothing more than sending. A program that sends messages is a producer.\n",
    "\n",
    "* Queue - A _queue_ is the name for a post box which lives inside RabbitMQ. Although messages flow through RabbitMQ and your applications, they can only be stored inside a queue. A queue is only bound by the host's memory & disk limits, it's essentially a large message buffer. Many producers can send messages that go to one queue, and many consumers can try to receive data from one queue. This is how we represent a queue:\n",
    "\n",
    "*Consuming - _Consuming_ has a similar meaning to receiving. A consumer is a program that mostly waits to receive messages.\n",
    "\n",
    "### RabbitMQ   Tutorials   \n",
    "\n",
    "* [https://www.rabbitmq.com/tutorials/tutorial-one-python.html]( https://www.rabbitmq.com/tutorials/tutorial-one-python.html)     \n",
    "* [https://www.rabbitmq.com/getstarted.html](https://www.rabbitmq.com/getstarted.html)  \n",
    "* [gitHub rabbitmq-tutorials](https://github.com/rabbitmq/rabbitmq-tutorials) \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Celery\n",
    "\n",
    "**Celery** is an open source asynchronous task queue or job queue\n",
    "which is based on distributed message passing. While it supports\n",
    "scheduling, its focus is on operations in real time. Celery is a distributed task queue built in Python and heavily used by the Python community for task-based workloads.\n",
    "\n",
    "\n",
    "\n",
    "### Overview   \n",
    "\n",
    "The execution units, called *tasks,* are executed concurrently on one or\n",
    "more worker nodes using multiprocessing, eventlet or gevent. Tasks\n",
    "can execute asynchronously (in the background) or synchronously (wait\n",
    "until ready). Celery is used in production systems, for instance\n",
    "Instagram, to process millions of tasks every day\n",
    "\n",
    "### Technology    \n",
    "\n",
    "Celery is written in Python, but the protocol can be implemented in\n",
    "any language. It can also operate with other languages using\n",
    "webhooks.^1 \n",
    "\n",
    "The recommended message broker is RabbitMQ, or Redis. Additionally,\n",
    "MongoDB, Beanstalk, Amazon SQS, CouchDB, IronMQ, and databases\n",
    "(using SQLAlchemy or the Django ORM) are supported in status\n",
    "*experimental*.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Steps with Celery  \n",
    "\n",
    "Celery is a task queue with batteries included. It’s easy to use so that\n",
    "you can get started without learning the full complexities of the\n",
    "problem it solves. It’s designed around best practices so that your\n",
    "product can scale and integrate with other languages, and it comes with\n",
    "the tools and support you need to run such a system in production.\n",
    "\n",
    "In this tutorial you’ll learn the absolute basics of using Celery.\n",
    "\n",
    "Learn about;\n",
    "\n",
    "-   Choosing and installing a message transport (broker).   \n",
    "-   Installing Celery and creating your first task.   \n",
    "-   Starting the worker and calling tasks.    \n",
    "-   Keeping track of tasks as they transition through different states,\n",
    "    and inspecting return values.   \n",
    "\n",
    "Celery may seem daunting at first - but don’t worry - this tutorial will\n",
    "get you started in no time. It’s deliberately kept simple, so as to not\n",
    "confuse you with advanced features. After you have finished this\n",
    "tutorial, it’s a good idea to browse the rest of the documentation. For\n",
    "example the [[Next Steps]{.std .std-ref}] tutorial will showcase\n",
    "Celery’s capabilities.\n",
    "\n",
    "\n",
    "### Choosing a Broker \n",
    "\n",
    "Celery requires a solution to send and receive\n",
    "messages; usually this comes in the form of a separate service called a\n",
    "message broker.\n",
    "\n",
    "There are several choices available, including:\n",
    "\n",
    "RabbitMQ RabbitMQ is feature-complete, stable, durable and easy to\n",
    "install. It’s an excellent choice for a production environment. Detailed\n",
    "information about using RabbitMQ with Celery:\n",
    "\n",
    "Using RabbitMQ If you’re using Ubuntu or Debian install RabbitMQ by\n",
    "executing this command:\n",
    "\n",
    "```bash\n",
    "   sudo apt-get install rabbitmq-server \n",
    "```\n",
    "\n",
    "When the command completes, the\n",
    "broker will already be running in the background, ready to move messages\n",
    "for you: Starting rabbitmq-server: SUCCESS.\n",
    "\n",
    "Don’t worry if you’re not running Ubuntu or Debian, you can go to this\n",
    "website to find similarly simple installation instructions for other\n",
    "platforms, including Microsoft Windows:\n",
    "\n",
    "[http://www.rabbitmq.com/download.html](http://www.rabbitmq.com/download.html) \n",
    "\n",
    "#### Redis \n",
    "\n",
    "Redis is also feature-complete, but is more susceptible to data\n",
    "loss in the event of abrupt termination or power failures. Detailed\n",
    "information about using Redis:\n",
    "\n",
    "[Using Redis](http://docs.celeryproject.org/en/latest/getting-started/brokers/redis.html#broker-redis)   \n",
    "\n",
    "#### Other brokers   \n",
    "\n",
    "In addition to the above, there are other experimental\n",
    "transport implementations to choose from, including [Amazon SQS](http://docs.celeryproject.org/en/latest/getting-started/brokers/sqs.html#broker-sqs).     \n",
    "\n",
    "See [Broker Overview](http://docs.celeryproject.org/en/latest/getting-started/brokers/index.html#broker-overview) for a full list.\n",
    "\n",
    "\n",
    "### Installing Celery  \n",
    "\n",
    "Celery is on the Python Package Index (PyPI), so it\n",
    "can be installed with standard Python tools like pip or easy\\_install:\n",
    "\n",
    "```bash\n",
    "   pip install celery \n",
    "```\n",
    "\n",
    "### Application \n",
    "\n",
    "The first thing you need is a Celery instance. We call this the Celery application or just app for short. As\n",
    "this instance is used as the entry-point for everything you want to do\n",
    "in Celery, like creating tasks and managing workers, it must be possible\n",
    "for other modules to import it.\n",
    "\n",
    "In this tutorial we keep everything contained in a single module, but\n",
    "for larger projects you want to create a dedicated module.\n",
    "\n",
    "Let’s create the file tasks.py:\n",
    "\n",
    "\n",
    "```python\n",
    "from celery import Celery\n",
    "\n",
    "app = Celery('tasks', broker='pyamqp://guest@localhost//')\n",
    "\n",
    "@app.task\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "```\n",
    "\n",
    "The first argument to Celery is the name of the current module. This is\n",
    "only needed so that names can be automatically generated when the tasks\n",
    "are defined in the \\_\\_main\\_\\_ module.\n",
    "\n",
    "The second argument is the broker keyword argument, specifying the URL\n",
    "of the message broker you want to use. Here using RabbitMQ (also the\n",
    "default option).\n",
    "\n",
    "See Choosing a Broker above for more choices – for RabbitMQ you can use\n",
    "amqp://localhost, or for Redis you can use redis://localhost.\n",
    "\n",
    "You defined a single task, called add, returning the sum of two numbers.\n",
    "\n",
    "\n",
    "### Running the Celery worker server  \n",
    "\n",
    "You can now run the worker by executing our program with the\n",
    "`worker` argument:\n",
    "\n",
    "```bash\n",
    "\n",
    "   celery -A tasks worker --loglevel=info\n",
    "\n",
    "```\n",
    "\n",
    "In production you’ll want to run the worker in the background as a\n",
    "daemon. To do this you need to use the tools provided by your platform,\n",
    "or something like supervisord.\n",
    "\n",
    "For a complete listing of the command-line options available, do:\n",
    "\n",
    "```bash\n",
    "\n",
    "      celery worker --help\n",
    "\n",
    "```\n",
    "\n",
    "There are also several other commands available, and help is also\n",
    "available:\n",
    "\n",
    "```bash\n",
    "\n",
    "    celery help\n",
    "```\n",
    "\n",
    "### Calling the task  \n",
    "\n",
    "To call our task you can use the `delay()`method.\n",
    "\n",
    "This is a handy shortcut to the `apply_async()` method that gives greater control of the task execution:\n",
    "\n",
    "```python\n",
    "\n",
    "    >>> from tasks import add\n",
    "    >>> add.delay(4, 4)\n",
    "\n",
    "```\n",
    "\n",
    "The task has now been processed by the worker you started earlier. You\n",
    "can verify this by looking at the worker’s console output.\n",
    "\n",
    "Calling a task returns an `AsyncResult` instance. This can be used to check the state of the task, wait for the task to finish, or get its return value (or if the task failed, to get the exception and traceback).\n",
    "\n",
    "Results are not enabled by default. In order to do remote procedure\n",
    "calls or keep track of task results in a database, you will need to\n",
    "configure Celery to use a result backend. This is described in the next\n",
    "section.\n",
    "\n",
    "### Keeping Results\n",
    "\n",
    "If you want to keep track of the tasks’ states, Celery needs to store or\n",
    "send the states somewhere. There are several built-in result backends to\n",
    "choose from: [SQLAlchemy](http://www.sqlalchemy.org/)/[Django](https://www.djangoproject.com/) ORM, [Memcached](http://memcached.org/), [Redis](https://redis.io/),\n",
    "[RPC](http://docs.celeryproject.org/en/latest/userguide/configuration.html#conf-rpc-result-backend) ([RabbitMQ](http://www.rabbitmq.com/)/AMQP), and – or you can define your own.\n",
    "\n",
    "For this example we use the `rpc` result backend, that sends\n",
    "states back as transient messages. The backend is specified via the\n",
    "`backend` argument to `Celery`, (or via the `result_backend` setting if you choose to use a configuration module):\n",
    "\n",
    "```python\n",
    "\n",
    "    app = Celery('tasks', backend='rpc://', broker='pyamqp://')\n",
    "\n",
    "```\n",
    "\n",
    "Or if you want to use Redis as the result backend, but still use\n",
    "RabbitMQ as the message broker (a popular combination):\n",
    "\n",
    "```python\n",
    "\n",
    "    app = Celery('tasks', backend='redis://localhost', broker='pyamqp://')\n",
    "\n",
    "```\n",
    "\n",
    "Now with the result backend configured, let’s call the task again. This\n",
    "time you’ll hold on to the `AsyncResult` instance returned when you call a task:\n",
    "\n",
    "```python\n",
    "\n",
    "    >>> result = add.delay(4, 4)\n",
    "\n",
    "```\n",
    "\n",
    "The `ready()` method returns whether the task has finished processing or not:\n",
    "\n",
    "```python\n",
    "\n",
    "    >>> result.ready()\n",
    "    False\n",
    "\n",
    "```\n",
    "\n",
    "You can wait for the result to complete, but this is rarely used since\n",
    "it turns the asynchronous call into a synchronous one:\n",
    "\n",
    "```python\n",
    "\n",
    "    >>> result.get(timeout=1)\n",
    "    8\n",
    "\n",
    "```\n",
    "\n",
    "In case the task raised an exception, `get()` will re-raise the exception, but you can override\n",
    "this by specifying the `propagate` argument:\n",
    "\n",
    "```python\n",
    "\n",
    "    >>> result.get(propagate=False)\n",
    "\n",
    "```\n",
    "\n",
    "If the task raised an exception, you can also gain access to the\n",
    "original traceback:\n",
    "\n",
    "```python\n",
    "\n",
    "    >>> result.traceback\n",
    "    ?\n",
    "\n",
    "```\n",
    "\n",
    "See `celery.result` for the complete result object reference.\n",
    "\n",
    "### Configuration  \n",
    "\n",
    "Celery, like a consumer appliance, doesn’t need much configuration to\n",
    "operate. It has an input and an output. The input must be connected to a\n",
    "broker, and the output can be optionally connected to a result backend.\n",
    "However, if you look closely at the back, there’s a lid revealing loads\n",
    "of sliders, dials, and buttons: this is the configuration.\n",
    "\n",
    "The default configuration should be good enough for most use cases, but\n",
    "there are many options that can be configured to make Celery work\n",
    "exactly as needed. Reading about the options available is a good idea to\n",
    "familiarize yourself with what can be configured. You can read about the\n",
    "options in the Configuration and defaults reference.\n",
    "\n",
    "The configuration can be set on the app directly or by using a dedicated\n",
    "configuration module. As an example you can configure the default\n",
    "serializer used for serializing task payloads by changing the\n",
    "`task_serializer` setting:\n",
    "\n",
    "```python\n",
    "\n",
    "    app.conf.task_serializer = 'json'\n",
    "\n",
    "```\n",
    "\n",
    "If you’re configuring many settings at once you can use `update`:\n",
    "\n",
    "```python\n",
    "\n",
    "    app.conf.update(\n",
    "        task_serializer='json',\n",
    "        accept_content=['json'],  # Ignore other content\n",
    "        result_serializer='json',\n",
    "        timezone='Europe/Oslo',\n",
    "        enable_utc=True\n",
    "\n",
    "```\n",
    "\n",
    "For larger projects, a dedicated configuration module is recommended.\n",
    "Hard coding periodic task intervals and task routing options is\n",
    "discouraged. It is much better to keep these in a centralized location.\n",
    "This is especially true for libraries, as it enables users to control\n",
    "how their tasks behave. A centralized configuration will also allow your\n",
    "SysAdmin to make simple changes in the event of system trouble.\n",
    "\n",
    "You can tell your Celery instance to use a configuration module by\n",
    "calling the `app.config_from_object()` method:\n",
    "\n",
    "```python\n",
    "\n",
    "    app.config_from_object('celeryconfig')\n",
    "\n",
    "```\n",
    "\n",
    "This module is often called “`celeryconfig`”, but\n",
    "you can use any module name.\n",
    "\n",
    "In the above case, a module named `celeryconfig.py`\n",
    "must be available to load from the current directory or on the Python\n",
    "path. It could look something like this:\n",
    "\n",
    "`celeryconfig.py`:\n",
    "\n",
    "```python\n",
    "\n",
    "    broker_url = 'pyamqp://'\n",
    "    result_backend = 'rpc://'\n",
    "\n",
    "    task_serializer = 'json'\n",
    "    result_serializer = 'json'\n",
    "    accept_content = ['json']\n",
    "    timezone = 'Europe/Oslo'\n",
    "    enable_utc = True\n",
    "\n",
    "```\n",
    "\n",
    "To verify that your configuration file works properly and doesn’t\n",
    "contain any syntax errors, you can try to import it:\n",
    "\n",
    "```bash\n",
    "\n",
    "    $ python -m celeryconfig\n",
    "\n",
    "```\n",
    "\n",
    "For a complete reference of configuration options, see [Configuration\n",
    "and defaults](http://docs.celeryproject.org/en/latest/userguide/configuration.html#configuration).\n",
    "\n",
    "To demonstrate the power of configuration files, this is how you’d route\n",
    "a misbehaving task to a dedicated queue:\n",
    "\n",
    "`celeryconfig.py`:\n",
    "\n",
    "```python\n",
    "\n",
    "    task_routes = {\n",
    "        'tasks.add': 'low-priority',\n",
    "    }\n",
    "\n",
    "```\n",
    "Or instead of routing it you could rate limit the task instead, so that\n",
    "only 10 tasks of this type can be processed in a minute (10/m):\n",
    "\n",
    "`celeryconfig.py`:\n",
    "\n",
    "```python\n",
    "\n",
    "    task_annotations = {\n",
    "        'tasks.add': {'rate_limit': '10/m'}\n",
    "    }\n",
    "\n",
    "```\n",
    "\n",
    "If you’re using RabbitMQ or Redis as the broker then you can also direct\n",
    "the workers to set a new rate limit for the task at runtime:\n",
    "\n",
    "```python\n",
    "\n",
    "    $ celery -A tasks control rate_limit tasks.add 10/m\n",
    "    worker@example.com: OK\n",
    "        new rate limit set successfully\n",
    "\n",
    "```\n",
    "\n",
    "See [Routing Tasks](http://docs.celeryproject.org/en/latest/userguide/routing.html#guide-routing) to read more about task routing,\n",
    "and the `task_annotations`\n",
    "setting for more about annotations, or [Monitoring and Management\n",
    "Guide](http://docs.celeryproject.org/en/latest/userguide/monitoring.html#guide-monitoring) for more about remote control commands and how to\n",
    "monitor what your workers are doing.\n",
    "\n",
    "\n",
    "### Where to go from here\n",
    "\n",
    "If you want to learn more you should continue to the [Next Steps](http://docs.celeryproject.org/en/latest/getting-started/next-steps.html#next-steps) tutorial, and after that you can read the [User Guide](http://docs.celeryproject.org/en/latest/userguide/index.html#guide).  \n",
    "\n",
    "### Celery Tutorials   \n",
    "\n",
    "\n",
    "* [First Steps With Celery tutorial](http://docs.celeryproject.org/en/latest/getting-started/first-steps-with-celery.html)   \n",
    "* [Getting Started Scheduling Tasks with Celery](https://www.caktusgroup.com/blog/2014/06/23/scheduling-tasks-celery/)  \n",
    "* [Introducing Celery for Python+Django](http://opensourceforu.com/2013/12/introducing-celery-pythondjango/)   \n",
    "* [How To Use Celery with RabbitMQ to Queue Tasks on an Ubuntu VPS](https://www.digitalocean.com/community/tutorials/how-to-use-celery-with-rabbitmq-to-queue-tasks-on-an-ubuntu-vps) \n",
    "* [A 4 Minute Intro to Celery ](https://www.youtube.com/watch?v=68QWZU_gCDA) \n",
    "* [Asynchronous Tasks in Python - Getting Started With Celery](https://www.youtube.com/watch?v=fg-JfZBetpM) \n",
    "* [Introduction to Celery](https://www.youtube.com/watch?v=3cyq5DHjymw) \n",
    "* [3 Gotchas for Working with Celery](https://wiredcraft.com/blog/3-gotchas-for-celery/) \n",
    "* [Setting up an asynchronous task queue for Django using Celery and Redis](http://michal.karzynski.pl/blog/2014/05/18/setting-up-an-asynchronous-task-queue-for-django-using-celery-redis/) \n",
    "* [Python Celery & RabbitMQ Tutorial](https://tests4geeks.com/python-celery-rabbitmq-tutorial/) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Dask\n",
    "\n",
    "Dask is a parallel computing library popular within the PyData community that has grown a fairly sophisticated distributed task scheduler. \n",
    "\n",
    "Dask is a flexible parallel computing library for analytic computing.\n",
    "\n",
    "Dask is composed of two components:\n",
    "\n",
    "1 - Dynamic task scheduling optimized for computation. This is similar to Airflow, Luigi, Celery, or Make, but optimized for interactive computational workloads.\n",
    "\n",
    "2 - “Big Data” collections like parallel arrays, dataframes, and lists that extend common interfaces like NumPy, Pandas, or Python iterators to larger-than-memory or distributed environments. These parallel collections run on top of the dynamic task schedulers.\n",
    "\n",
    "Dask emphasizes the following virtues:\n",
    "\n",
    "* Familiar: Provides parallelized NumPy array and Pandas DataFrame objects.  \n",
    "* Flexible: Provides a task scheduling interface for more custom workloads and integration with other projects.\n",
    "* Native: Enables distributed computing   in Pure Python with access to the PyData stack.  \n",
    "* Fast: Operates with low overhead, low latency, and minimal serialization necessary for fast numerical algorithms.  \n",
    "* Scales up: Runs resiliently on clusters with 1000s of cores.  \n",
    "* Scales down: Trivial to set up and run on a laptop in a single process. \n",
    "* Responsive: Designed with interactive computing in mind it provides rapid feedback and diagnostics to aid humans.   \n",
    "\n",
    "### Dask documentation  \n",
    "\n",
    "The Dask documentation is at [http://dask.pydata.org/en/latest/](http://dask.pydata.org/en/latest/)\n",
    "\n",
    "### Dask Tutorials   \n",
    "\n",
    "* [Dask and Celery](http://matthewrocklin.com/blog/work/2016/09/13/dask-and-celery)   \n",
    "* [GitHub - dask/dask-tutorial: Dask tutorial](https://github.com/dask/dask-tutorial)  \n",
    "* [dask-tutorial-pydata-seattle-2017](https://github.com/jcrist/dask-tutorial-pydata-seattle-2017) \n",
    "* [dask tutorial](http://dask.github.io/dask-tutorial/introduction.html#/) \n",
    "* [Distributed Pandas on a Cluster with Dask DataFrames](http://matthewrocklin.com/blog/work/2017/01/12/dask-dataframes) \n",
    "* [Parallelizing Scientific Python with Dask | SciPy 2017 Tutorial | James Crist](https://www.youtube.com/watch?v=mbfsog3e5DA) \n",
    "* [Matthew Rocklin | Using Dask for Parallel Computing in Python](https://www.youtube.com/watch?v=s4ChP7tc3tA) \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Luigi\n",
    "\n",
    "Luigi is a Python package that helps you build complex pipelines of batch\n",
    "jobs. It handles dependency resolution, workflow management, visualization, handling failures, command line integration, and much more.\n",
    "\n",
    "###  Getting Started\n",
    "\n",
    "Run ``pip install luigi`` to install the latest stable version from `PyPI\n",
    "[https://pypi.python.org/pypi/luigi](https://pypi.python.org/pypi/luigi)\n",
    "\n",
    "###  Background\n",
    "\n",
    "The purpose of Luigi is to address all the plumbing typically associated\n",
    "with long-running batch processes. You want to chain many tasks, automate them, and failures *will* happen. These tasks can be anything,\n",
    "but are typically long running things like [Hadoop](http://hadoop.apache.org/) jobs, dumping data to/from\n",
    "databases, running machine learning algorithms, or anything else.\n",
    "\n",
    "There are other software packages that focus on lower level aspects of\n",
    "data processing, like [Hive](http://hive.apache.org/), [Pig](http://pig.apache.org/), or [Cascading](http://www.cascading.org/). \n",
    "\n",
    "Luigi is not a framework to replace these. Instead it helps you stitch many tasks together, where each task can be a [Hive query](https://luigi.readthedocs.io/en/latest/api/luigi.contrib.hive.html), a [Hadoop job in Java] (https://luigi.readthedocs.io/en/latest/api/luigi.contrib.hadoop_jar.html),\n",
    "a [Spark job in Scala or Python] (https://luigi.readthedocs.io/en/latest/api/luigi.contrib.spark.html),\n",
    "a Python snippet, [SQL] (https://luigi.readthedocs.io/en/latest/api/luigi.contrib.sqla.html)\n",
    "from a database, or anything else. It's easy to build up\n",
    "long-running pipelines that comprise thousands of tasks and take days or\n",
    "weeks to complete. Luigi takes care of a lot of the workflow management\n",
    "so that you can focus on the tasks themselves and their dependencies.\n",
    "\n",
    "You can build pretty much any task you want, but Luigi also comes with a\n",
    "*toolbox* of several common task templates that you use. It includes\n",
    "support for running [Python mapreduce jobs] (https://luigi.readthedocs.io/en/latest/api/luigi.contrib.hadoop.html) in Hadoop, as well as [Hive] (https://luigi.readthedocs.io/en/latest/api/luigi.contrib.hive.html),\n",
    "and [Pig] (https://luigi.readthedocs.io/en/latest/api/luigi.contrib.pig.html),\n",
    "jobs. It also comes with file system abstractions for [HDFS] (https://luigi.readthedocs.io/en/latest/api/luigi.contrib.hdfs.html),\n",
    "and local files that ensures all file system operations are atomic. This\n",
    "is important because it means your data pipeline will not crash in a\n",
    "state containing partial data.\n",
    "\n",
    "### Visualiser page   \n",
    "\n",
    "The Luigi server comes with a web interface too, so you can search and filter among all your tasks.\n",
    "\n",
    "![Visualiser page]( https://raw.githubusercontent.com/spotify/luigi/master/doc/visualiser_front_page.png)\n",
    "\n",
    "### Dependency graph example   \n",
    "\n",
    "Just to give you an idea of what Luigi does, this is a screen shot from\n",
    "something we are running in production. Using Luigi's visualiser, we get\n",
    "a nice visual overview of the dependency graph of the workflow. Each\n",
    "node represents a task which has to be run. Green tasks are already\n",
    "completed whereas yellow tasks are yet to be run. Most of these tasks\n",
    "are Hadoop jobs, but there are also some things that run locally and\n",
    "build up data files.\n",
    "\n",
    "![Dependency graph]( https://raw.githubusercontent.com/spotify/luigi/master/doc/user_recs.png)\n",
    "\n",
    "\n",
    "### Philosophy  \n",
    "\n",
    "Conceptually, Luigi is similar to [GNU\n",
    "Make](http://www.gnu.org/software/make/) where you have certain tasks\n",
    "and these tasks in turn may have dependencies on other tasks. There are\n",
    "also some similarities to [Oozie](http://oozie.apache.org/) and [Azkaban] (http://data.linkedin.com/opensource/azkaban). One major\n",
    "difference is that Luigi is not just built specifically for Hadoop, and\n",
    "it's easy to extend it with other kinds of tasks.\n",
    "\n",
    "Everything in Luigi is in Python. Instead of XML configuration or\n",
    "similar external data files, the dependency graph is specified *within\n",
    "Python*. This makes it easy to build up complex dependency graphs of\n",
    "tasks, where the dependencies can involve date algebra or recursive\n",
    "references to other versions of the same task. However, the workflow can\n",
    "trigger things not in Python, such as running [Pig scripts] (https://luigi.readthedocs.io/en/latest/api/luigi.contrib.pig.html) or [scp'ing files] (https://luigi.readthedocs.io/en/latest/api/luigi.contrib.ssh.html).\n",
    "\n",
    "### Who uses Luigi?\n",
    "\n",
    "Luigi is used internally at [Spotify](https://www.spotify.com) to run\n",
    "thousands of tasks every day, organized in complex dependency graphs.\n",
    "Most of these tasks are Hadoop jobs. Luigi provides an infrastructure\n",
    "that powers all kinds of stuff including recommendations, toplists, A/B\n",
    "test analysis, external reports, internal dashboards, etc.\n",
    "\n",
    "Since Luigi is open source and without any registration walls, the exact number of Luigi users is unknown. But based on the number of unique contributors, we expect hundreds of enterprises to use it. Some users have written blog posts or held presentations about Luigi:\n",
    "\n",
    "* [Spotify](https://www.spotify.com) [Luigi presentation, 2014] (http://www.slideshare.net/erikbern/luigi-presentation-nyc-data-science)   \n",
    "* [Foursquare](https://foursquare.com/) (Luigi presentation, 2013) (http://www.slideshare.net/OpenAnayticsMeetup/luigi-presentation-17-23199897)  \n",
    "* Mortar Data [Datadog](https://www.datadoghq.com/) [Luigi documentation / tutorial](http://help.mortardata.com/technologies/luigi)  \n",
    "* [Stripe](https://stripe.com/) [Luigi presentation, 2014] (http://www.slideshare.net/PyData/python-as-part-of-a-production-machine-learning-stack-by-michael-manapat-pydata-sv-2014)   \n",
    "* [Asana](https://asana.com/) [Luigi blog, 2014] (https://eng.asana.com/2014/11/stable-accessible-data-infrastructure-startup/)   \n",
    "* [Buffer](https://buffer.com/) [Luigi blog, 2014] (https://overflow.bufferapp.com/2014/10/31/buffers-new-data-architecture/)  \n",
    "* [SeatGeek](https://seatgeek.com/) [Luigi blog, 2015] (http://chairnerd.seatgeek.com/building-out-the-seatgeek-data-pipeline/)  \n",
    "* [Treasure Data](https://www.treasuredata.com/) [Luigi blog, 2015] (http://blog.treasuredata.com/blog/2015/02/25/managing-the-data-pipeline-with-git-luigi/)  \n",
    "* [Growth Intelligence](http://growthintel.com/) [Luigi presentation, 2015](http://www.slideshare.net/growthintel/a-beginners-guide-to-building-data-pipelines-with-luigi)  \n",
    "* [AdRoll](https://www.adroll.com/) [Luigi blog, 2015] (http://tech.adroll.com/blog/data/2015/09/22/data-pipelines-docker.html)  \n",
    "* [17zuoye Luigi presentation, 2015](https://speakerdeck.com/mvj3/luiti-an-offline-task-management-framework)  \n",
    "* [Custobar](https://www.custobar.com/) [Luigi presentation, 2016] (http://www.slideshare.net/teemukurppa/managing-data-workflows-with-luigi)  \n",
    "* [Blendle](https://launch.blendle.com/) [Luigi presentation] (http://www.anneschuth.nl/wp-content/uploads/sea-anneschuth-streamingblendle.pdf#page=126)  \n",
    "* [TrustYou](http://www.trustyou.com/) [Luigi presentation, 2015] (https://speakerdeck.com/mfcabrera/pydata-berlin-2015-processing-hotel-reviews-with-python)  \n",
    "* [Groupon](https://www.groupon.com/) and [OrderUp](https://orderup.com) [Luigi alternative implementation](https://github.com/groupon/luigi-warehouse)  \n",
    "* [Red Hat - Marketing Operations](https://www.redhat.com) [Luigi blog, 2017](https://github.com/rh-marketingops/rh-mo-scc-luigi)  \n",
    "* [GetNinjas](https://www.getninjas.com.br/) [Luigi blog, 2017] (https://labs.getninjas.com.br/using-luigi-to-create-and-monitor-pipelines-of-batch-jobs-eb8b3cd2a574)   \n",
    "\n",
    "Some more companies are using Luigi but haven't had a chance yet to write about it:\n",
    "\n",
    "* [Schibsted](http://www.schibsted.com/)    \n",
    "* [enbrite.ly](http://enbrite.ly/)    \n",
    "* [Dow Jones / The Wall Street Journal](http://wsj.com)    \n",
    "* [Hotels.com](https://hotels.com)    \n",
    "* [Newsela](https://newsela.com)    \n",
    "* [Squarespace](https://www.squarespace.com/)    \n",
    "* [OAO](https://adops.com/)    \n",
    "* [Grovo](https://grovo.com/)    \n",
    "* [Weebly](https://www.weebly.com/)    \n",
    "* [Deloitte](https://www.Deloitte.co.uk/)    \n",
    "\n",
    "\n",
    "### External links  \n",
    "\n",
    "* [Mailing List](https://groups.google.com/d/forum/luigi-user/) for discussions and asking questions. _Google Groups_   \n",
    "* [Releases](https://pypi.python.org/pypi/luigi) _PyPI_   \n",
    "* [Source code](https://github.com/spotify/luigi) _Github_    \n",
    "* [Hubot Integration](https://github.com/houzz/hubot-luigi) plugin for Slack, Hipchat, etc] _Github_    \n",
    "\n",
    "### Luigi  Tutorials   \n",
    "\n",
    "\n",
    "* [Getting Started — Luigi 2.7.1 documentation](https://luigi.readthedocs.io/en/stable/)   \n",
    "* [Tutorial: Luigi for Scientific Workflows | Bionics IT](http://bionics.it/posts/luigi-tutorial)  \n",
    "* [Building Data Pipelines with Python and Luigi – Marco Bonzanini](https://marcobonzanini.com/2015/10/24/building-data-pipelines-with-python-and-luigi/) \n",
    "* [\n",
    "Intro to Building Data Pipelines in Python with Luigi - YouTube](https://www.youtube.com/watch?v=ymF2R_tY1f8) \n",
    "* [Building Data Pipelines with Python and Luigi - Erik Bernhardsson](https://vimeo.com/79533035) \n",
    "* [Building Task Pipelines Using Luigi - G B](http://gouthamanbalaraman.com/blog/building-luigi-task-pipeline.html) \n",
    "* [Managing the Data Pipeline with Git + Luigi](https://blog.treasuredata.com/blog/2015/02/25/managing-the-data-pipeline-with-git-luigi/) \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spotify Luigi example - Top Artists\n",
    "\n",
    "Spotify Luigi example - Top Artists from [https://luigi.readthedocs.io/en/stable/example_top_artists.html](https://luigi.readthedocs.io/en/stable/example_top_artists.html)  \n",
    "\n",
    "\n",
    "**Purpose**\n",
    "\n",
    "Aggregate all streams, find the top 10 artists and then put the results into Postgres.\n",
    "\n",
    "### Step 1 - Aggregate Artist Streams  \n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "class AggregateArtists(luigi.Task):\n",
    "    date_interval = luigi.DateIntervalParameter()\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\"data/artist_streams_%s.tsv\" % self.date_interval)\n",
    "\n",
    "    def requires(self):\n",
    "        return [Streams(date) for date in self.date_interval]\n",
    "\n",
    "    def run(self):\n",
    "        artist_count = defaultdict(int)\n",
    "\n",
    "        for input in self.input():\n",
    "            with input.open('r') as in_file:\n",
    "                for line in in_file:\n",
    "                    timestamp, artist, track = line.strip().split()\n",
    "                    artist_count[artist] += 1\n",
    "\n",
    "        with self.output().open('w') as out_file:\n",
    "            for artist, count in artist_count.iteritems():\n",
    "                print >> out_file, artist, count\n",
    "```\n",
    "\n",
    "Note that this is just a portion of the file examples/top_artists.py. In particular, Streams is defined as a Task, acting as a dependency for AggregateArtists. In addition, luigi.run() is called if the script is executed directly, allowing it to be run from the command line.\n",
    "\n",
    "There are several pieces of this snippet that deserve more explanation.\n",
    "\n",
    "* Any Task may be customized by instantiating one or more Parameter objects on the class level.  \n",
    "* The output() method tells Luigi where the result of running the task will end up. The path can be some function of the parameters.   \n",
    "* The requires() tasks specifies other tasks that we need to perform this task. In this case it’s an external dump named Streams which takes the date as the argument.  \n",
    "* For plain Tasks, the run() method implements the task. This could be anything, including calling subprocesses, performing long running number crunching, etc. For some subclasses of Task you don’t have to implement the run method. For instance, for the JobTask subclass you implement a mapper and reducer instead.   \n",
    "* LocalTarget is a built in class that makes it easy to read/write from/to the local filesystem. It also makes all file operations atomic, which is nice in case your script crashes for any reason.  \n",
    "\n",
    "#### Running this Locally   \n",
    "\n",
    "```bash\n",
    "    cd examples\n",
    "    luigi --module top_artists AggregateArtists --local-scheduler --date-interval 2012-06\n",
    "```\n",
    "\n",
    "### Step 2 – Find the Top Artists     \n",
    "\n",
    "At this point, we’ve counted the number of streams for each artists, for the full time period. We are left with a large file that contains mappings of artist -> count data, and we want to find the top 10 artists. Since we only have a few hundred thousand artists, and calculating artists is nontrivial to parallelize, we choose to do this not as a Hadoop job, but just as a plain old for-loop in Python.  \n",
    "\n",
    "\n",
    "```python\n",
    "class Top10Artists(luigi.Task):\n",
    "    date_interval = luigi.DateIntervalParameter()\n",
    "    use_hadoop = luigi.BoolParameter()\n",
    "\n",
    "    def requires(self):\n",
    "        if self.use_hadoop:\n",
    "            return AggregateArtistsHadoop(self.date_interval)\n",
    "        else:\n",
    "            return AggregateArtists(self.date_interval)\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\"data/top_artists_%s.tsv\" % self.date_interval)\n",
    "\n",
    "    def run(self):\n",
    "        top_10 = nlargest(10, self._input_iterator())\n",
    "        with self.output().open('w') as out_file:\n",
    "            for streams, artist in top_10:\n",
    "                print >> out_file, self.date_interval.date_a, self.date_interval.date_b, artist, streams\n",
    "\n",
    "    def _input_iterator(self):\n",
    "        with self.input().open('r') as in_file:\n",
    "            for line in in_file:\n",
    "                artist, streams = line.strip().split()\n",
    "                yield int(streams), int(artist)\n",
    "```\n",
    "\n",
    "The most interesting thing here is that this task (Top10Artists) defines a dependency on the previous task (AggregateArtists). This means that if the output of AggregateArtists does not exist, the task will run before Top10Artists.\n",
    "\n",
    "```bash\n",
    "    luigi --module examples.top_artists Top10Artists --local-scheduler --date-interval 2012-07\n",
    "```\n",
    "This will run both tasks.\n",
    "\n",
    "### Step 3 - Insert into Postgres  \n",
    "\n",
    "This mainly serves as an example of a specific subclass Task that doesn’t require any code to be written. It’s also an example of how you can define task templates that you can reuse for a lot of different tasks.  \n",
    "\n",
    "\n",
    "```python\n",
    "class ArtistToplistToDatabase(luigi.contrib.postgres.CopyToTable):\n",
    "    date_interval = luigi.DateIntervalParameter()\n",
    "    use_hadoop = luigi.BoolParameter()\n",
    "\n",
    "    host = \"localhost\"\n",
    "    database = \"toplists\"\n",
    "    user = \"luigi\"\n",
    "    password = \"abc123\"  # ;)\n",
    "    table = \"top10\"\n",
    "\n",
    "    columns = [(\"date_from\", \"DATE\"),\n",
    "               (\"date_to\", \"DATE\"),\n",
    "               (\"artist\", \"TEXT\"),\n",
    "               (\"streams\", \"INT\")]\n",
    "\n",
    "    def requires(self):\n",
    "        return Top10Artists(self.date_interval, self.use_hadoop)\n",
    "```\n",
    "\n",
    "Just like previously, this defines a recursive dependency on the previous task. If you try to build the task, that will also trigger building all its upstream dependencies.\n",
    "\n",
    "The _–local-scheduler flag_ tells Luigi not to connect to a central scheduler. This is recommended in order to get started and or for development purposes. At the point where you start putting things in production we strongly recommend running the central scheduler server. In addition to providing locking so that the same task is not run by multiple processes at the same time, this server also provides a pretty nice visualization of your current work flow.\n",
    "\n",
    "If you drop the –local-scheduler flag, your script will try to connect to the central planner, by default at localhost port 8082. If you run\n",
    "\n",
    "\n",
    "```bash\n",
    "    luigid\n",
    "```\n",
    "\n",
    "in the background and then run your task without the --local-scheduler flag, then your script will now schedule through a centralized server. You need [Tornado](http://www.tornadoweb.org/en/stable/) for this to work.\n",
    "\n",
    "Launching http://localhost:8082 should show something like this:\n",
    "\n",
    "\n",
    "![luigi web server](http://nikbearbrown.com/YouTube/MachineLearning/IMG/luigi_web_server.png)\n",
    "\n",
    "Web server screenshot Looking at the dependency graph for any of the tasks yields something like this:  \n",
    "\n",
    "![luigi aggregate artists](http://nikbearbrown.com/YouTube/MachineLearning/IMG/luigi_aggregate_artists.png)\n",
    "\n",
    "Aggregate artists screenshot\n",
    "\n",
    "In production, you’ll want to run the centralized scheduler. See: [Using the Central Scheduler](https://luigi.readthedocs.io/en/stable/central_scheduler.html) for more information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Airflow \n",
    "\n",
    "\n",
    "Use airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.\n",
    "\n",
    "When workflows are defined as code, they become more maintainable, versionable, testable, and collaborative.\n",
    "\n",
    "![Apache Airflow](http://nikbearbrown.com/YouTube/MachineLearning/IMG/airflow.gif)\n",
    "\n",
    "_Apache Airflow_  \n",
    "\n",
    "### Principles\n",
    "\n",
    "Dynamic: Airflow pipelines are configuration as code (Python), allowing for dynamic pipeline generation. This allows for writing code that instantiates pipelines dynamically.\n",
    "Extensible: Easily define your own operators, executors and extend the library so that it fits the level of abstraction that suits your environment.\n",
    "Elegant: Airflow pipelines are lean and explicit. Parameterizing your scripts is built into the core of Airflow using the powerful Jinja templating engine.\n",
    "Scalable: Airflow has a modular architecture and uses a message queue to orchestrate an arbitrary number of workers. Airflow is ready to scale to infinity.\n",
    "\n",
    "### Airflow Operators  \n",
    "\n",
    "Airflow uses Operators as the fundamental unit of abstraction to define tasks, and uses a DAG (Directed Acyclic Graph) to define workflows using a set of operators. Operators are extensible which makes customizing workflows easy. Operators are divided into 3 types:   \n",
    "\n",
    "* Action operators that perform some action such as executing a Python function or submitting a Spark Job.\n",
    "* Transfer operators that move data between systems such as from Hive to Mysql or from S3 to Hive.  \n",
    "* Sensors which trigger downstream tasks in the dependency graph when a certain criteria is met, for example checking for a certain file becoming available on S3 before using it downstream. Sensors are a powerful feature of Airflow allowing us to create complex workflows and easily manage their preconditions.   \n",
    "\n",
    "\n",
    "### Beyond the Horizon\n",
    "\n",
    "Airflow is not a data streaming solution. Tasks do not move data from one to the other (though tasks can exchange metadata!). Airflow is not in the Spark Streaming or Storm space, it is more comparable to Oozie or Azkaban.\n",
    "\n",
    "Workflows are expected to be mostly static or slowly changing. You can think of the structure of the tasks in your workflow as slightly more dynamic than a database structure would be. Airflow workflows are expected to look similar from a run to the next, this allows for clarity around unit of work and continuity.\n",
    "\n",
    "### Who uses Airflow?\n",
    "\n",
    "* [Airbnb](https://www.airbnb.com/)  \n",
    "* [Agari](https://www.agari.com/)    \n",
    "* [Robinhood Engineering](https://robinhood.engineering/)  \n",
    "* [Blue Yonder](https://www.blue-yonder.com/en)  \n",
    "* [Glassdoor](https://www.glassdoor.com)  \n",
    "* [Lyft](https://www.lyft.com/)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Airflow Quick Start    \n",
    "\n",
    "The is form the [Apache Airflow Quick Start](https://airflow.incubator.apache.org/start.html).  \n",
    "\n",
    "The installation is quick and straightforward.   \n",
    "\n",
    "```bash\n",
    "# airflow needs a home, ~/airflow is the default,\n",
    "# but you can lay foundation somewhere else if you prefer\n",
    "# (optional)\n",
    "export AIRFLOW_HOME=~/airflow\n",
    "\n",
    "# install from pypi using pip\n",
    "pip install airflow\n",
    "\n",
    "# initialize the database\n",
    "airflow initdb\n",
    "\n",
    "# start the web server, default port is 8080\n",
    "airflow webserver -p 8080\n",
    "```\n",
    "\n",
    "Upon running these commands, Airflow will create the \\$AIRFLOW_HOME folder and lay an “airflow.cfg” file with defaults that get you going fast. You can inspect the file either in \\$AIRFLOW_HOME/airflow.cfg, or through the UI in the Admin->Configuration menu. The PID file for the webserver will be stored in \\$AIRFLOW_HOME/airflow-webserver.pid or in /run/airflow/webserver.pid if started by systemd.    \n",
    "\n",
    "Out of the box, Airflow uses a sqlite database, which you should outgrow fairly quickly since no parallelization is possible using this database backend. It works in conjunction with the SequentialExecutor which will only run task instances sequentially. While this is very limiting, it allows you to get up and running quickly and take a tour of the UI and the command line utilities.   \n",
    "\n",
    "Here are a few commands that will trigger a few task instances. You should be able to see the status of the jobs change in the example1 DAG as you run the commands below.  \n",
    "\n",
    "```bash\n",
    "# run your first task instance\n",
    "airflow run example_bash_operator runme_0 2015-01-01\n",
    "# run a backfill over 2 days\n",
    "airflow backfill example_bash_operator -s 2015-01-01 -e 2015-01-02\n",
    "```\n",
    "\n",
    "### Example Pipeline definition  \n",
    "\n",
    "Here is an example of a basic pipeline definition. Do not worry if this looks complicated, a line by line explanation follows below.\n",
    "\n",
    "```python  \n",
    "\"\"\"\n",
    "Code that goes along with the Airflow tutorial located at:\n",
    "https://github.com/airbnb/airflow/blob/master/airflow/example_dags/tutorial.py\n",
    "\"\"\"\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2015, 6, 1),\n",
    "    'email': ['airflow@airflow.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    # 'queue': 'bash_queue',\n",
    "    # 'pool': 'backfill',\n",
    "    # 'priority_weight': 10,\n",
    "    # 'end_date': datetime(2016, 1, 1),\n",
    "}\n",
    "\n",
    "dag = DAG('tutorial', default_args=default_args)\n",
    "\n",
    "# t1, t2 and t3 are examples of tasks created by instantiating operators\n",
    "t1 = BashOperator(\n",
    "    task_id='print_date',\n",
    "    bash_command='date',\n",
    "    dag=dag)\n",
    "\n",
    "t2 = BashOperator(\n",
    "    task_id='sleep',\n",
    "    bash_command='sleep 5',\n",
    "    retries=3,\n",
    "    dag=dag)\n",
    "\n",
    "templated_command = \"\"\"\n",
    "    {% for i in range(5) %}\n",
    "        echo \"{{ ds }}\"\n",
    "        echo \"{{ macros.ds_add(ds, 7)}}\"\n",
    "        echo \"{{ params.my_param }}\"\n",
    "    {% endfor %}\n",
    "\"\"\"\n",
    "\n",
    "t3 = BashOperator(\n",
    "    task_id='templated',\n",
    "    bash_command=templated_command,\n",
    "    params={'my_param': 'Parameter I passed in'},\n",
    "    dag=dag)\n",
    "\n",
    "t2.set_upstream(t1)\n",
    "t3.set_upstream(t1)\n",
    "```\n",
    "\n",
    "###  It’s a DAG definition file  \n",
    "\n",
    "One thing to wrap your head around (it may not be very intuitive for everyone at first) is that this Airflow Python script is really just a configuration file specifying the DAG’s structure as code. The actual tasks defined here will run in a different context from the context of this script. Different tasks run on different workers at different points in time, which means that this script cannot be used to cross communicate between tasks. Note that for this purpose we have a more advanced feature called XCom.\n",
    "\n",
    "People sometimes think of the DAG definition file as a place where they can do some actual data processing - that is not the case at all! The script’s purpose is to define a DAG object. It needs to evaluate quickly (seconds, not minutes) since the scheduler will execute it periodically to reflect the changes if any.\n",
    "\n",
    "### Importing Modules  \n",
    "\n",
    "An Airflow pipeline is just a Python script that happens to define an Airflow DAG object. Let’s start by importing the libraries we will need.\n",
    "\n",
    "```python  \n",
    "# The DAG object; we'll need this to instantiate a DAG\n",
    "from airflow import DAG\n",
    "\n",
    "# Operators; we need this to operate!\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "```\n",
    "\n",
    "\n",
    "### Default Arguments  \n",
    "\n",
    "We’re about to create a DAG and some tasks, and we have the choice to explicitly pass a set of arguments to each task’s constructor (which would become redundant), or (better!) we can define a dictionary of default parameters that we can use when creating tasks.  \n",
    "\n",
    "```python  \n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2015, 6, 1),\n",
    "    'email': ['airflow@airflow.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    # 'queue': 'bash_queue',\n",
    "    # 'pool': 'backfill',\n",
    "    # 'priority_weight': 10,\n",
    "    # 'end_date': datetime(2016, 1, 1),\n",
    "}\n",
    "```\n",
    "\n",
    "For more information about the BaseOperator’s parameters and what they do, refer to the :py:class:airflow.models.BaseOperator documentation.  \n",
    "\n",
    "Also, note that you could easily define different sets of arguments that would serve different purposes. An example of that would be to have different settings between a production and development environment.  \n",
    "\n",
    "### Instantiate a DAG  \n",
    "\n",
    "We’ll need a DAG object to nest our tasks into. Here we pass a string that defines the dag_id, which serves as a unique identifier for your DAG. We also pass the default argument dictionary that we just defined and define a schedule_interval of 1 day for the DAG.\n",
    "\n",
    "```python  \n",
    "dag = DAG(\n",
    "    'tutorial', default_args=default_args, schedule_interval=timedelta(1))\n",
    "```\n",
    "\n",
    "### Tasks   \n",
    "\n",
    "Tasks are generated when instantiating operator objects. An object instantiated from an operator is called a constructor. The first argument task_id acts as a unique identifier for the task.\n",
    "\n",
    "```python  \n",
    "t1 = BashOperator(\n",
    "    task_id='print_date',\n",
    "    bash_command='date',\n",
    "    dag=dag)\n",
    "\n",
    "t2 = BashOperator(\n",
    "    task_id='sleep',\n",
    "    bash_command='sleep 5',\n",
    "    retries=3,\n",
    "    dag=dag)\n",
    "```\n",
    "\n",
    "Notice how we pass a mix of operator specific arguments (bash_command) and an argument common to all operators (retries) inherited from BaseOperator to the operator’s constructor. This is simpler than passing every argument for every constructor call. Also, notice that in the second task we override the retries parameter with 3.\n",
    "\n",
    "The precedence rules for a task are as follows:\n",
    "\n",
    "1. Explicitly passed arguments    \n",
    "2. Values that exist in the default_args dictionary   \n",
    "3. The operator’s default value, if one exists   \n",
    "\n",
    "\n",
    "A task must include or inherit the arguments task_id and owner, otherwise Airflow will raise an exception.\n",
    "\n",
    "### Templating with Jinja   \n",
    "\n",
    "Airflow leverages the power of Jinja Templating and provides the pipeline author with a set of built-in parameters and macros. Airflow also provides hooks for the pipeline author to define their own parameters, macros and templates.\n",
    "\n",
    "This tutorial barely scratches the surface of what you can do with templating in Airflow, but the goal of this section is to let you know this feature exists, get you familiar with double curly brackets, and point to the most common template variable: {{ ds }}.\n",
    "\n",
    "```python  \n",
    "    {% for i in range(5) %}\n",
    "        echo \"{{ ds }}\"\n",
    "        echo \"{{ macros.ds_add(ds, 7) }}\"\n",
    "        echo \"{{ params.my_param }}\"\n",
    "    {% endfor %}\n",
    "\"\"\"\n",
    "\n",
    "t3 = BashOperator(\n",
    "    task_id='templated',\n",
    "    bash_command=templated_command,\n",
    "    params={'my_param': 'Parameter I passed in'},\n",
    "    dag=dag)\n",
    "```\n",
    "\n",
    "Notice that the templated_command contains code logic in {% %} blocks, references parameters like {{ ds }}, calls a function as in {{ macros.ds_add(ds, 7)}}, and references a user-defined parameter in {{ params.my_param }}.   \n",
    "\n",
    "The params hook in BaseOperator allows you to pass a dictionary of parameters and/or objects to your templates. Please take the time to understand how the parameter my_param makes it through to the template.  \n",
    "\n",
    "Files can also be passed to the bash_command argument, like bash_command='templated_command.sh', where the file location is relative to the directory containing the pipeline file (tutorial.py in this case). This may be desirable for many reasons, like separating your script’s logic and pipeline code, allowing for proper code highlighting in files composed in different languages, and general flexibility in structuring pipelines. It is also possible to define your template_searchpath as pointing to any folder locations in the DAG constructor call.   \n",
    "\n",
    "For more information on the variables and macros that can be referenced in templates, make sure to read through the Macros section.   \n",
    "\n",
    "### Setting up Dependencies  \n",
    "\n",
    "We have two simple tasks that do not depend on each other. Here’s a few ways you can define dependencies between them:\n",
    "\n",
    "```python  \n",
    "t2.set_upstream(t1)\n",
    "\n",
    "# This means that t2 will depend on t1\n",
    "# running successfully to run\n",
    "# It is equivalent to\n",
    "# t1.set_downstream(t2)\n",
    "\n",
    "t3.set_upstream(t1)\n",
    "\n",
    "# all of this is equivalent to\n",
    "# dag.set_dependency('print_date', 'sleep')\n",
    "# dag.set_dependency('print_date', 'templated')\n",
    "```\n",
    "\n",
    "Note that when executing your script, Airflow will raise exceptions when it finds cycles in your DAG or when a dependency is referenced more than once.   \n",
    "\n",
    "### Recap  \n",
    "\n",
    "Alright, so we have a pretty basic DAG. At this point your code should look something like this:\n",
    "\n",
    "```python  \n",
    "\"\"\"\n",
    "Code that goes along with the Airflow located at:\n",
    "http://airflow.readthedocs.org/en/latest/tutorial.html\n",
    "\"\"\"\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2015, 6, 1),\n",
    "    'email': ['airflow@airflow.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    # 'queue': 'bash_queue',\n",
    "    # 'pool': 'backfill',\n",
    "    # 'priority_weight': 10,\n",
    "    # 'end_date': datetime(2016, 1, 1),\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'tutorial', default_args=default_args, schedule_interval=timedelta(1))\n",
    "\n",
    "# t1, t2 and t3 are examples of tasks created by instantiating operators\n",
    "t1 = BashOperator(\n",
    "    task_id='print_date',\n",
    "    bash_command='date',\n",
    "    dag=dag)\n",
    "\n",
    "t2 = BashOperator(\n",
    "    task_id='sleep',\n",
    "    bash_command='sleep 5',\n",
    "    retries=3,\n",
    "    dag=dag)\n",
    "\n",
    "templated_command = \"\"\"\n",
    "    {% for i in range(5) %}\n",
    "        echo \"{{ ds }}\"\n",
    "        echo \"{{ macros.ds_add(ds, 7)}}\"\n",
    "        echo \"{{ params.my_param }}\"\n",
    "    {% endfor %}\n",
    "\"\"\"\n",
    "\n",
    "t3 = BashOperator(\n",
    "    task_id='templated',\n",
    "    bash_command=templated_command,\n",
    "    params={'my_param': 'Parameter I passed in'},\n",
    "    dag=dag)\n",
    "\n",
    "t2.set_upstream(t1)\n",
    "t3.set_upstream(t1)\n",
    "```\n",
    "\n",
    "### Testing    \n",
    "\n",
    "**Running the Script**\n",
    "\n",
    "Time to run some tests. First let’s make sure that the pipeline parses. Let’s assume we’re saving the code from the previous step in tutorial.py in the DAGs folder referenced in your airflow.cfg. The default location for your DAGs is ~/airflow/dags.  \n",
    "\n",
    "```bash  \n",
    "    python ~/airflow/dags/tutorial.py\n",
    "```\n",
    "\n",
    "If the script does not raise an exception it means that you haven’t done anything horribly wrong, and that your Airflow environment is somewhat sound.  \n",
    "\n",
    "### Command Line Metadata Validation   \n",
    "\n",
    "Let’s run a few commands to validate this script further.\n",
    "\n",
    "```bash  \n",
    "# print the list of active DAGs\n",
    "airflow list_dags\n",
    "\n",
    "# prints the list of tasks the \"tutorial\" dag_id\n",
    "airflow list_tasks tutorial\n",
    "\n",
    "# prints the hierarchy of tasks in the tutorial DAG\n",
    "airflow list_tasks tutorial --tree\n",
    "```\n",
    "\n",
    "### Testing  \n",
    "\n",
    "Let’s test by running the actual task instances on a specific date. The date specified in this context is an execution_date, which simulates the scheduler running your task or dag at a specific date + time:\n",
    "\n",
    "```bash  \n",
    "# command layout: command subcommand dag_id task_id date\n",
    "\n",
    "# testing print_date\n",
    "airflow test tutorial print_date 2015-06-01\n",
    "\n",
    "# testing sleep\n",
    "airflow test tutorial sleep 2015-06-01\n",
    "```\n",
    "\n",
    "Now remember what we did with templating earlier? See how this template gets rendered and executed by running this command:   \n",
    "\n",
    "```bash  \n",
    "# testing templated\n",
    "airflow test tutorial templated 2015-06-01   \n",
    "```\n",
    "This should result in displaying a verbose log of events and ultimately running your bash command and printing the result.   \n",
    "\n",
    "Note that the airflow test command runs task instances locally, outputs their log to stdout (on screen), doesn’t bother with dependencies, and doesn’t communicate state (running, success, failed, ...) to the database. It simply allows testing a single task instance.   \n",
    "\n",
    "\n",
    "### Backfill   \n",
    "\n",
    "Everything looks like it’s running fine so let’s run a backfill. backfill will respect your dependencies, emit logs into files and talk to the database to record status. If you do have a webserver up, you’ll be able to track the progress. airflow webserver will start a web server if you are interested in tracking the progress visually as your backfill progresses.\n",
    "\n",
    "Note that if you use depends_on_past=True, individual task instances will depend on the success of the preceding task instance, except for the start_date specified itself, for which this dependency is disregarded.\n",
    "\n",
    "The date range in this context is a start_date and optionally an end_date, which are used to populate the run schedule with task instances from this dag.  \n",
    "\n",
    "```bash  \n",
    "# optional, start a web server in debug mode in the background\n",
    "# airflow webserver --debug &\n",
    "\n",
    "# start your backfill on a date range\n",
    "airflow backfill tutorial -s 2015-06-01 -e 2015-06-07\n",
    "```\n",
    "\n",
    "### What’s Next?  \n",
    "\n",
    "That’s it, you’ve written, tested and backfilled your very first Airflow pipeline. Merging your code into a code repository that has a master scheduler running against it should get it to get triggered and run every day.\n",
    "\n",
    "Here’s a few things you might want to do next:\n",
    "\n",
    "Take an in-depth tour of the UI - click all the things!\n",
    "Keep reading the docs! Especially the sections on:\n",
    "\n",
    "* Command line interface   \n",
    "* Operators   \n",
    "* Macros   \n",
    "\n",
    "Write your first pipeline!\n",
    "\n",
    "\n",
    "### Airflow Tutorials   \n",
    "\n",
    "* [Tutorial — Airflow Documentation](https://airflow.incubator.apache.org/tutorial.html)   \n",
    "* [hgrif/airflow-tutorial: Basic Airflow tutorial](https://github.com/hgrif/airflow-tutorial)  \n",
    "* [Get started developing workflows with Apache Airflow - Michał Karzyński](http://michal.karzynski.pl/blog/2017/03/19/developing-workflows-with-apache-airflow/)\n",
    "* [Technology: Airflow - Beginners Tutorial](http://tech.lalitbhatt.net/2016/04/airflow-beginners-tutorial.html) \n",
    "* [Building a Data Pipeline with Airflow - Mark Litwintschik](http://tech.marksblogg.com/airflow-postgres-redis-forex.html) \n",
    "* [ETL example — ETL Best Practices with Airflow v1.8 - GitHub Pages](https://gtoonstra.github.io/etl-with-airflow/etlexample.html) \n",
    "* [Apache Airflow introduction and best practices - YouTube](https://www.youtube.com/watch?v=Pr0FrvIIfTU) \n",
    "* [Matt Davis | A Pratctical Introduction to Airflow - YouTube](https://www.youtube.com/watch?v=cHATHSB_450) \n",
    "* [I'm sorry Cron, I've met AirBnB's Airflow – dani del valle](https://danidelvalle.me/2016/09/12/im-sorry-cron-ive-met-airbnbs-airflow/) \n",
    "* [Scheduling Spark jobs with Airflow – Insight Data](https://blog.insightdatascience.com/scheduling-spark-jobs-with-airflow-4c66f3144660)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chron jobs\n",
    "\n",
    "Chron jobs from [https://help.ubuntu.com/community/CronHowto](https://help.ubuntu.com/community/CronHowto)   \n",
    "\n",
    "Cron is a system daemon used to execute desired tasks (in the\n",
    "background) at designated times.\n",
    "\n",
    "A crontab file is a simple text file containing a list of commands meant\n",
    "to be run at specified times. It is edited using the crontab command.\n",
    "The commands in the crontab file (and their run times) are checked by\n",
    "the cron daemon, which executes them in the system background.\n",
    "\n",
    "Each user (including root) has a crontab file. The cron daemon checks a\n",
    "user’s crontab file regardless of whether the user is actually logged\n",
    "into the system or not.\n",
    "\n",
    "To display the on-line help for crontab enter: \n",
    "\n",
    "```bash\n",
    "     man crontab\n",
    "```\n",
    "\n",
    "On Gnome-based Ubuntu systems Gnome *Scheduled tasks* tool (from the\n",
    "*gnome-schedule* package) in *Applications* –&gt; *System Tools*\n",
    "provides a graphical interface with prompting for using Cron. The\n",
    "project website is at <http://gnome-schedule.sourceforge.net/>; the\n",
    "software is installable from the Software Center or by typing:\n",
    "\n",
    "```bash\n",
    "    sudo apt-get install gnome-schedule\n",
    "```\n",
    "\n",
    "### Starting to Use Cron  \n",
    "\n",
    "To use cron for tasks meant to run only for your user profile, add\n",
    "entries to your own user’s crontab file. To edit the crontab file enter:\n",
    "\n",
    "```bash\n",
    "    crontab -e \n",
    "```\n",
    "\n",
    "Edit the crontab using the format described in the next sections. Save\n",
    "your changes. (Exiting without saving will leave your crontab\n",
    "unchanged.) To display the on-line help describing the format of the\n",
    "crontab file enter:  \n",
    "\n",
    "```bash\n",
    "    man 5 crontab\n",
    "```\n",
    "\n",
    "\n",
    "Commands that normally run with administrative privileges (i.e. they are\n",
    "generally run using sudo) should be added to the root crontab. To edit\n",
    "the root crontab enter: \n",
    "\n",
    "```bash\n",
    "     sudo crontab -e\n",
    "```\n",
    "\n",
    "\n",
    "###  Crontab Lines   \n",
    "\n",
    "Each line has five time-and-date fields, followed by a command, followed\n",
    "by a newline character (’\\\\n’). []{#line-45 .anchor}The fields are\n",
    "separated by spaces. The five time-and-date fields cannot contain\n",
    "spaces. The five time-and-date fields are as follows: minute (0-59),\n",
    "hour (0-23, 0 = midnight), day (1-31), month (1-12), weekday (0-6, 0 =\n",
    "Sunday).  \n",
    "\n",
    "```bash\n",
    "    01 04 1 1 1 /usr/bin/somedirectory/somecommand\n",
    "```\n",
    "\n",
    "The above example will run /usr/bin/somedirectory/somecommand at 4:01am\n",
    "on January 1st plus every Monday in January.   \n",
    "\n",
    "An asterisk (\\*) can be used so that every instance (every hour, every\n",
    "weekday, every month, etc.) of a time period is used.   \n",
    "\n",
    "```bash\n",
    "    01 04 * * * /usr/bin/somedirectory/somecommand\n",
    "```\n",
    "\n",
    "The above example will run /usr/bin/somedirectory/somecommand at 4:01am\n",
    "on every day of every month. \n",
    "\n",
    "Comma-separated values can be used to run more than one instance of a\n",
    "particular command within a time period. Dash-separated values can be\n",
    "used to run a command continuously. \n",
    "\n",
    "```bash\n",
    "    01,31 04,05 1-15 1,6 * /usr/bin/somedirectory/somecommand\n",
    "```\n",
    "\n",
    "The above example will run /usr/bin/somedirectory/somecommand at 01 and\n",
    "31 past the hours of 4:00am and 5:00am on the 1st through the 15th of\n",
    "every January and June.   \n",
    "\n",
    "The “/usr/bin/somedirectory/somecommand” text in the above examples\n",
    "indicates the task which will be run at the specified times. It is\n",
    "recommended that you use the full path to the desired commands as shown\n",
    "in the above examples. Enter *which somecommand* in the terminal to find\n",
    "the full path to *somecommand*. The crontab will begin running as soon\n",
    "as it is properly edited and saved.   \n",
    "\n",
    "You may want to run a script some number of times per time unit. For\n",
    "example if you want to run it every 10 minutes use the following crontab\n",
    "entry (runs on minutes divisible by 10: 0, 10, 20, 30, etc.)  \n",
    "\n",
    "```bash\n",
    "    */10 * * * * /usr/bin/somedirectory/somecommand\n",
    "```\n",
    "\n",
    "which is also equivalent to the more cumbersome  \n",
    "\n",
    "```bash\n",
    "    0,10,20,30,40,50 * * * * /usr/bin/somedirectory/somecommand\n",
    "```\n",
    "\n",
    "Cron also offers some special strings, which can be used in place of the\n",
    "five time-and-date fields:\n",
    "\n",
    "| **string**    | **meaning**                        |\n",
    "|---------------|------------------------------------|\n",
    "|  @reboot      | Run once, at startup.              |\n",
    "|  @yearly      | Run once a year, “0 0 1 1 \\*”.     |\n",
    "|  @annually    | (same as @yearly)                  |\n",
    "|  @monthly     | Run once a month, “0 0 1 \\* \\*”.   |\n",
    "|  @weekly      | Run once a week, “0 0 \\* \\* 0”.    |\n",
    "|  @daily       | Run once a day, “0 0 \\* \\* \\*”.    |\n",
    "|  @midnight    | (same as @daily)                   |\n",
    "|  @hourly      | Run once an hour, “0 \\* \\* \\* \\*”. |\n",
    "\n",
    "\n",
    "\n",
    "```bash\n",
    "    @reboot /path/to/execuable1\n",
    "```\n",
    "\n",
    "The above example will execute /path/to/executable1 when the system\n",
    "starts.  \n",
    "\n",
    "For more information on special strings enter “man 5 crontab”.\n",
    "\n",
    "### Crontab Options  \n",
    "\n",
    "The -l option causes the current crontab to be displayed on standard\n",
    "output. \n",
    "\n",
    "* The -r option causes the current crontab to be removed.  \n",
    "* The -e option is used to edit the current crontab using the editor\n",
    "specified by the EDITOR environment variable.    \n",
    "\n",
    "After you exit from the editor, the modified crontab is checked for\n",
    "errors and, if there are no errors, it is installed automatically. The\n",
    "file is stored in */var/spool/cron/crontabs* but should only be edited\n",
    "using the crontab command.   \n",
    "\n",
    "### Allowing/Denying User-Level Cron  \n",
    "\n",
    "If the **/etc/cron.allow** file exists, then users must be listed in it\n",
    "in order to be allowed to run the **crontab** command. If the\n",
    "**/etc/cron.allow** file does not exist but the **/etc/cron.deny** file\n",
    "does, then users must not be listed in the **/etc/cron.deny** file in\n",
    "order to run **crontab**. \n",
    "\n",
    "In the case where neither file exists, the default on current Ubuntu\n",
    "(and Debian, but not some other Linux and UNIX systems) is to allow all\n",
    "users to run jobs with **crontab**.   \n",
    "\n",
    "No cron.allow or cron.deny files exist in a standard Ubuntu install, so\n",
    "all users should have cron available by default, until one of those\n",
    "files is created. If a blank cron.deny file has been created, that will\n",
    "change to the standard behavior users of other operating systems might\n",
    "expect: cron only available to root or users in cron.allow.  \n",
    "\n",
    "Note, userids on your system which do not appear in /etc/shadow will NOT\n",
    "have operational crontabs, if you desire to enter a user in /etc/passwd,\n",
    "but NOT /etc/shadow that user’s crontab will never run. Place an entry\n",
    "in /etc/shadow for the user with a \\* for the password crypt, eg:\n",
    "\n",
    "\n",
    "```bash\n",
    "    joeuser:*:15169::::::\n",
    "```\n",
    "\n",
    "###  Further Considerations   \n",
    "\n",
    "Crontab commands are generally stored in the crontab file belonging to\n",
    "your user account (and executed with your user’s level of permissions).\n",
    "If you want to regularly run a command requiring administrative\n",
    "permissions, edit the root crontab file:\n",
    "\n",
    "```bash\n",
    "    sudo crontab -e\n",
    "```\n",
    "\n",
    "Depending on the commands being run, you may need to expand the root\n",
    "users PATH variable by putting the following line at the top of the root\n",
    "crontab file:  \n",
    "\n",
    "```bash\n",
    "    PATH=/usr/sbin:/usr/bin:/sbin:/bin\n",
    "```\n",
    "\n",
    "**crontab -e** uses the EDITOR environment variable. To change the\n",
    "editor to your own choice, just set that variable. You may want to set\n",
    "EDITOR in your .bashrc because many commands use this variable. For\n",
    "example, in order to set the editor to be nano (a very easy editor to\n",
    "use) add this line to .bashrc:   \n",
    "\n",
    "\n",
    "```bash\n",
    "    export EDITOR=nano\n",
    "```\n",
    "\n",
    "It is sensible to test that your cron jobs work as intended. One method\n",
    "for doing this is to set up the job to run a couple of minutes in the\n",
    "future and then check the results before finalising the timing. You may\n",
    "also find it useful to put the commands into script files that log their\n",
    "success or failure, for example:   \n",
    "\n",
    "```bash\n",
    "    echo \"Nightly Backup Successful: $(date)\" >> /tmp/mybackup.log\n",
    "```\n",
    "\n",
    "If your machine is regularly switched off, you may also be interested in\n",
    "**at** and **anacron**, which provide other approaches to scheduled\n",
    "tasks. For example, **anacron** offers simple system-wide directories\n",
    "for running commands hourly, daily, weekly, and monthly. Scripts to be\n",
    "executed in said times can be placed in **/etc/cron.hourly/**,\n",
    "**/etc/cron.daily/**, **/etc/cron.weekly/**, and **/etc/cron.monthly/**.\n",
    "All scripts in each directory are run as root, and a specific order to\n",
    "running the scripts can be specified by prefixing the scripts’ filenames\n",
    "with numbers (see the **man** page for **run‑parts** for more details).\n",
    "Although the directories contain periods in their names, run‑parts\n",
    "**will not** accept a file name containing a period and will fail\n",
    "silently when encountering them Either rename the file or use a symlink (without a period) to it instead (see, for example, *[python + cron without\n",
    "login?]* and *[Problems with Hourly Cron Job]*).   \n",
    "\n",
    "\n",
    "### Common Problems   \n",
    "\n",
    "Edits to a user’s crontab and the cron jobs run are all logged by\n",
    "default to **/var/log/syslog** and that’s the first place to check if\n",
    "things are not running as you expect.   \n",
    "\n",
    "If a user was not allowed to execute jobs when their crontab was last\n",
    "edited, just adding them to the allow list won’t do anything. The user\n",
    "needs to re-edit their crontab after being added to cron.allow before\n",
    "their jobs will run.   \n",
    "\n",
    "Note that user-specific crontabs (including the root crontab) do not\n",
    "specify the user name after the date/time fields. If you accidentally\n",
    "include the user name in a user-specific crontab, the system will try to\n",
    "run the user name as a command.   \n",
    "\n",
    "Cron jobs may not run with the environment, in particular the PATH, that\n",
    "you expect. Try using full paths to files and programs if they’re not\n",
    "being located as you expect.    \n",
    "\n",
    "The “%” character is used as newline delimiter in cron commands. If you\n",
    "need to pass that character into a script, you need to escape it as\n",
    "“\\\\%”.   \n",
    "\n",
    "If you’re having trouble running a GUI application using cron, see the\n",
    "GUI Applications section below.   \n",
    "\n",
    "\n",
    "### Two Other Types of Crontab  \n",
    "\n",
    "The crontab files discussed above are **user** crontabs. Each of the\n",
    "above crontabs is associated with a user, even the root crontab, which\n",
    "is associated with the root user. There are two other types of crontab,\n",
    "with syntax as follows:  \n",
    "\n",
    "\n",
    "```bash\n",
    "  minute(s) hour(s) day(s)_of_month month(s) day(s)_of_week user command\n",
    "```\n",
    "\n",
    "Note that the only difference from the syntax of the user crontabs is\n",
    "that the line specifies the user to run the job as.  \n",
    "\n",
    "The first type is as follows. As mentioned above **anacron** uses the\n",
    "**run‑parts** command and **/etc/cron.hourly**, **/etc/cron.weekly**,\n",
    "and **/etc/cron.monthly** directories. However **anacron** itself is\n",
    "invoked from the **/etc/crontab** file. This file could be used for\n",
    "other cron commands, but probably shouldn’t be. Here’s an example line\n",
    "from a fictitious **/etc/crontab**: \n",
    "\n",
    "\n",
    "```bash\n",
    "    00 01 * * * rusty /home/rusty/rusty-list-files.sh\n",
    "```\n",
    "\n",
    "This would run Rusty’s command script as user **rusty** from his home\n",
    "directory. However, it is not usual to add commands to this file. While\n",
    "an experienced user should know about it, it is not recommended that you\n",
    "add anything to **/etc/crontab**. Apart from anything else, this could\n",
    "cause a problem if the **/etc/crontab** file is affected by updates!\n",
    "Rusty could lose his command. \n",
    "\n",
    "The second type is to be found in the directory /etc/cron.d. This\n",
    "directory can contain crontab files. The directory is often used by\n",
    "packages, and the crontab files allow a user to be associated with the\n",
    "commands in them.   \n",
    "\n",
    "Example: Instead of adding a line to /etc/crontab, which Rusty knows is\n",
    "not a good idea, he might well add a file to the directory /etc/cron.d\n",
    "with the name **rusty**, containing his cron line above. This would not\n",
    "be affected by updates but is a **well known** location.  \n",
    "\n",
    "When would you use these alternate crontab locations? Well, on a single\n",
    "user machine or a shared machine such as a school or college server, a\n",
    "**user** crontab would be the way to go. But in a large IT department, where several people might look after\n",
    "a server, then the directory /etc/cron.d is probably the best place to\n",
    "install crontabs - it’s a central point and saves searching for them!\n",
    "\n",
    "You may not need to look at **/etc/crontab** or **/etc/cron.d**, let\n",
    "alone edit them by hand. But an experienced user should perhaps know\n",
    "about them and that the packages that he/she installs may use these\n",
    "locations for their crontabs.  \n",
    "\n",
    "### GUI Applications \n",
    "\n",
    "It is possible to run gui applications via cronjobs. This can be done by\n",
    "telling cron which display to use.  \n",
    "\n",
    "```bash\n",
    "    00 06 * * * env DISPLAY=:0 gui_appname\n",
    "```\n",
    "\n",
    "The *env DISPLAY=:0* portion will tell cron to use the current display\n",
    "(desktop) for the program “gui\\_appname”. \n",
    "\n",
    "And if you have multiple monitors, don’t forget to specify on which one\n",
    "the program is to be run. For example, to run it on the first screen\n",
    "(default screen) use :  \n",
    "\n",
    "```bash\n",
    "    00 06 * * * env DISPLAY=:0.0 gui_appname\n",
    "```\n",
    "The *env DISPLAY=:0.0* portion will tell cron to use the first screen of\n",
    "the current display for the program “gui\\_appname”.   \n",
    "\n",
    "\n",
    "**Note:** GUI users may prefer to use gnome-schedule (aka “Scheduled\n",
    "tasks”) to configure GUI cron jobs. In gnome-schedule, when editing a\n",
    "GUI task, you have to select “X application” in a dropdown next to the\n",
    "command field.   \n",
    "\n",
    "**Note:** In Karmic(9.10), you have to enable X ACL for localhost to\n",
    "connect to for GUI applications to work.\n",
    "\n",
    "     ~$ xhost +local:\n",
    "    non-network local connections being added to access control list\n",
    "     ~$ xhost\n",
    "    access control enabled, only authorized clients can connect\n",
    "    LOCAL:\n",
    "    ...\n",
    "\n",
    "\n",
    "### Crontab Example  \n",
    "\n",
    "Below is an example of how to setup a crontab to run updatedb, which\n",
    "updates the slocate database: Open a terminal, type “crontab -e”\n",
    "(without the double quotes) and press enter. Type the following line,\n",
    "substituting the full path of the application you wish to run for the\n",
    "one shown below, into the editor: \n",
    "\n",
    "```bash\n",
    "    45 04 * * * /usr/bin/updatedb\n",
    "```\n",
    "\n",
    "Save your changes and exit the editor. \n",
    "\n",
    "Crontab will let you know if you made any mistakes. The crontab will be\n",
    "installed and begin running if there are no errors. That’s it. You now\n",
    "have a cronjob setup to run updatedb, which updates the slocate\n",
    "database, every morning at 4:45. \n",
    "\n",
    "Note: The double-ampersand (&&) can also be used in the “command”\n",
    "section to run multiple commands consecutively, but only if the previous\n",
    "command exits successfully. A string of commands joined by the\n",
    "double-ampersand will only get to the last command if all the previous\n",
    "commands are run successfully. If exit error-checking is not required,\n",
    "string commands together, separated with a semi-colon (;). \n",
    "\n",
    "```bash\n",
    "    45 04 * * * /usr/sbin/chkrootkit && /usr/bin/updatedb\n",
    "```\n",
    "\n",
    "The above example will run chkrootkit followed by updatedb at 4:45am\n",
    "daily - providing you have all listed apps installed. If chkrootkit\n",
    "fails, updatedb will NOT be run. \n",
    "\n",
    "### Chron Job Tutorials   \n",
    "\n",
    "* [Newbie: Intro to cron - unixgeeks.org](http://www.unixgeeks.org/security/newbie/unix/cron-1.html)   \n",
    "* [How to Run a Cron Job Every Day on a Linux System](https://www.tutorialspoint.com/articles/how-to-run-a-cron-job-every-day-on-a-linux-system)  \n",
    "* [Cron Jobs and how to use them - an introduction - HowtoForge](https://www.howtoforge.com/a-short-introduction-to-cron-jobs)\n",
    "* [Cron Jobs Tutorial - SiteGround](https://www.siteground.com/tutorials/cpanel/cron-jobs/) \n",
    "* [Crontab Basic Tutorial - BASH - Linux - YouTube](https://www.youtube.com/watch?v=7MFMnsnfBJs) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last update October 3, 2017 \n",
    "\n",
    "The text is released under the [CC-BY-NC-ND license](https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode), and code is released under the [MIT license](https://opensource.org/licenses/MIT)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
