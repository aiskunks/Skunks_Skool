{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"C2_W2_Lab_2_Feature_Engineering_Pipeline.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"23R0Z9RojXYW"},"source":["Feature Engineering Pipeline\n","\n","\n","In this lab, you will continue exploring [Tensorflow Transform](https://www.tensorflow.org/tfx/transform/get_started). This time, it will be in the context of a machine learning (ML) pipeline. In production-grade projects, you want to streamline tasks so you can more easily improve your model or find issues that may arise. [Tensorflow Extended (TFX)](https://www.tensorflow.org/tfx) provides components that work together to execute the most common steps in a machine learning project. If you want to dig deeper into the motivations behind TFX and the need for machine learning pipelines, you can read about it in [this paper](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/b500d77bc4f518a1165c0ab43c8fac5d2948bc14.pdf) and in this [blog post](https://blog.tensorflow.org/2020/09/brief-history-of-tensorflow-extended-tfx.html).\n","\n","You will build end-to-end pipelines in future courses but for this one, you will only build up to the feature engineering part. Specifically, you will:\n","\n","* ingest data from a base directory with `ExampleGen`\n","* compute the statistics of the training data with `StatisticsGen`\n","* infer a schema with `SchemaGen`\n","* detect anomalies in the evaluation data with `ExampleValidator`\n","* preprocess the data into features suitable for model training with `Transform`\n","\n","If several steps mentioned above sound familiar, it's because the TFX components that deal with data validation and analysis (i.e. `StatisticsGen`, `SchemaGen`, `ExampleValidator`) uses [Tensorflow Data Validation (TFDV)](https://www.tensorflow.org/tfx/data_validation/get_started) under the hood. You're already familiar with this library from the exercises in Week 1 and for this week, you'll see how it fits within an ML pipeline.\n","\n","The components you will use are the orange boxes highlighted in the figure below:\n","\n","<img src='img/feature_eng_pipeline.png'>\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"02LpTBRn4P1-","executionInfo":{"status":"ok","timestamp":1629736881758,"user_tz":240,"elapsed":137313,"user":{"displayName":"Nik Bear Brown","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiM1Q26nVUtugBRkregYHIFfrTZNtlP8O-WMVvR=s64","userId":"17467854027018001755"}},"outputId":"f202f37a-4dca-4627-b3ee-6b394c4d0e64"},"source":["!pip install tfx"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting tfx\n","  Downloading tfx-1.2.0-py3-none-any.whl (2.4 MB)\n","\u001b[K     |████████████████████████████████| 2.4 MB 8.3 MB/s \n","\u001b[?25hCollecting kubernetes<13,>=10.0.1\n","  Downloading kubernetes-12.0.1-py2.py3-none-any.whl (1.7 MB)\n","\u001b[K     |████████████████████████████████| 1.7 MB 79.1 MB/s \n","\u001b[?25hRequirement already satisfied: grpcio<2,>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from tfx) (1.39.0)\n","Collecting google-cloud-aiplatform<0.8,>=0.5.0\n","  Downloading google_cloud_aiplatform-0.7.1-py2.py3-none-any.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 65.3 MB/s \n","\u001b[?25hCollecting tensorflow-data-validation<1.3.0,>=1.2.0\n","  Downloading tensorflow_data_validation-1.2.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.4 MB)\n","\u001b[K     |████████████████████████████████| 1.4 MB 66.0 MB/s \n","\u001b[?25hCollecting apache-beam[gcp]<3,>=2.31\n","  Downloading apache_beam-2.31.0-cp37-cp37m-manylinux2010_x86_64.whl (9.7 MB)\n","\u001b[K     |████████████████████████████████| 9.7 MB 40.6 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml<6,>=3.12 in /usr/local/lib/python3.7/dist-packages (from tfx) (3.13)\n","Collecting tfx-bsl<1.3.0,>=1.2.0\n","  Downloading tfx_bsl-1.2.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (19.0 MB)\n","\u001b[K     |████████████████████████████████| 19.0 MB 1.1 MB/s \n","\u001b[?25hCollecting google-apitools<1,>=0.5\n","  Downloading google_apitools-0.5.32-py3-none-any.whl (135 kB)\n","\u001b[K     |████████████████████████████████| 135 kB 66.7 MB/s \n","\u001b[?25hCollecting tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2\n","  Downloading tensorflow-2.5.1-cp37-cp37m-manylinux2010_x86_64.whl (454.4 MB)\n","\u001b[K     |████████████████████████████████| 454.4 MB 10 kB/s \n","\u001b[?25hCollecting tensorflow-model-analysis<0.34,>=0.33\n","  Downloading tensorflow_model_analysis-0.33.0-py3-none-any.whl (1.7 MB)\n","\u001b[K     |████████████████████████████████| 1.7 MB 68.8 MB/s \n","\u001b[?25hRequirement already satisfied: portpicker<2,>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from tfx) (1.3.9)\n","Collecting ml-pipelines-sdk==1.2.0\n","  Downloading ml_pipelines_sdk-1.2.0-py3-none-any.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 67.4 MB/s \n","\u001b[?25hCollecting google-cloud-bigquery<2.21,>=1.28.0\n","  Downloading google_cloud_bigquery-2.20.0-py2.py3-none-any.whl (189 kB)\n","\u001b[K     |████████████████████████████████| 189 kB 60.0 MB/s \n","\u001b[?25hCollecting packaging<21,>=20\n","  Downloading packaging-20.9-py2.py3-none-any.whl (40 kB)\n","\u001b[K     |████████████████████████████████| 40 kB 6.7 MB/s \n","\u001b[?25hCollecting keras-tuner<1.0.2,>=1\n","  Downloading keras-tuner-1.0.1.tar.gz (54 kB)\n","\u001b[K     |████████████████████████████████| 54 kB 3.4 MB/s \n","\u001b[?25hCollecting tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15\n","  Downloading tensorflow_serving_api-2.6.0-py2.py3-none-any.whl (37 kB)\n","Collecting docker<5,>=4.1\n","  Downloading docker-4.4.4-py2.py3-none-any.whl (147 kB)\n","\u001b[K     |████████████████████████████████| 147 kB 62.2 MB/s \n","\u001b[?25hRequirement already satisfied: google-api-python-client<2,>=1.8 in /usr/local/lib/python3.7/dist-packages (from tfx) (1.12.8)\n","Requirement already satisfied: jinja2<3,>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from tfx) (2.11.3)\n","Collecting attrs<21,>=19.3.0\n","  Downloading attrs-20.3.0-py2.py3-none-any.whl (49 kB)\n","\u001b[K     |████████████████████████████████| 49 kB 7.8 MB/s \n","\u001b[?25hCollecting ml-metadata<1.3.0,>=1.2.0\n","  Downloading ml_metadata-1.2.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 22.7 MB/s \n","\u001b[?25hRequirement already satisfied: click<8,>=7 in /usr/local/lib/python3.7/dist-packages (from tfx) (7.1.2)\n","Collecting tensorflow-transform<1.3.0,>=1.2.0\n","  Downloading tensorflow_transform-1.2.0-py3-none-any.whl (406 kB)\n","\u001b[K     |████████████████████████████████| 406 kB 61.8 MB/s \n","\u001b[?25hRequirement already satisfied: absl-py<0.13,>=0.9 in /usr/local/lib/python3.7/dist-packages (from tfx) (0.12.0)\n","Requirement already satisfied: protobuf<4,>=3.13 in /usr/local/lib/python3.7/dist-packages (from tfx) (3.17.3)\n","Requirement already satisfied: numpy<1.20,>=1.16 in /usr/local/lib/python3.7/dist-packages (from tfx) (1.19.5)\n","Requirement already satisfied: tensorflow-hub<0.13,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from tfx) (0.12.0)\n","Collecting pyarrow<3,>=1\n","  Downloading pyarrow-2.0.0-cp37-cp37m-manylinux2014_x86_64.whl (17.7 MB)\n","\u001b[K     |████████████████████████████████| 17.7 MB 76 kB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py<0.13,>=0.9->tfx) (1.15.0)\n","Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.31->tfx) (1.7)\n","Collecting dill<0.3.2,>=0.3.1.1\n","  Downloading dill-0.3.1.1.tar.gz (151 kB)\n","\u001b[K     |████████████████████████████████| 151 kB 78.1 MB/s \n","\u001b[?25hRequirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.31->tfx) (2018.9)\n","Collecting fastavro<2,>=0.21.4\n","  Downloading fastavro-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n","\u001b[K     |████████████████████████████████| 2.3 MB 67.9 MB/s \n","\u001b[?25hCollecting avro-python3!=1.9.2,<1.10.0,>=1.8.1\n","  Downloading avro-python3-1.9.2.1.tar.gz (37 kB)\n","Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.31->tfx) (2.8.2)\n","Collecting requests<3.0.0,>=2.24.0\n","  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n","\u001b[K     |████████████████████████████████| 62 kB 1.2 MB/s \n","\u001b[?25hCollecting hdfs<3.0.0,>=2.1.0\n","  Downloading hdfs-2.6.0-py3-none-any.whl (33 kB)\n","Requirement already satisfied: typing-extensions<3.8.0,>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.31->tfx) (3.7.4.3)\n","Collecting future<1.0.0,>=0.18.2\n","  Downloading future-0.18.2.tar.gz (829 kB)\n","\u001b[K     |████████████████████████████████| 829 kB 62.3 MB/s \n","\u001b[?25hRequirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.31->tfx) (3.12.0)\n","Requirement already satisfied: oauth2client<5,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.31->tfx) (4.1.3)\n","Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.31->tfx) (1.3.0)\n","Requirement already satisfied: httplib2<0.20.0,>=0.8 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.31->tfx) (0.17.4)\n","Collecting google-cloud-bigtable<2,>=0.31.1\n","  Downloading google_cloud_bigtable-1.7.0-py2.py3-none-any.whl (267 kB)\n","\u001b[K     |████████████████████████████████| 267 kB 66.2 MB/s \n","\u001b[?25hCollecting grpcio-gcp<1,>=0.2.2\n","  Downloading grpcio_gcp-0.2.2-py2.py3-none-any.whl (9.4 kB)\n","Collecting google-cloud-language<2,>=1.3.0\n","  Downloading google_cloud_language-1.3.0-py2.py3-none-any.whl (83 kB)\n","\u001b[K     |████████████████████████████████| 83 kB 3.0 MB/s \n","\u001b[?25hCollecting google-cloud-pubsub<2,>=0.39.0\n","  Downloading google_cloud_pubsub-1.7.0-py2.py3-none-any.whl (144 kB)\n","\u001b[K     |████████████████████████████████| 144 kB 92.6 MB/s \n","\u001b[?25hRequirement already satisfied: google-cloud-datastore<2,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.31->tfx) (1.8.0)\n","Requirement already satisfied: cachetools<5,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.31->tfx) (4.2.2)\n","Requirement already satisfied: google-auth<2,>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.31->tfx) (1.34.0)\n","Collecting google-cloud-videointelligence<2,>=1.8.0\n","  Downloading google_cloud_videointelligence-1.16.1-py2.py3-none-any.whl (183 kB)\n","\u001b[K     |████████████████████████████████| 183 kB 87.2 MB/s \n","\u001b[?25hCollecting google-cloud-dlp<2,>=0.12.0\n","  Downloading google_cloud_dlp-1.0.0-py2.py3-none-any.whl (169 kB)\n","\u001b[K     |████████████████████████████████| 169 kB 87.1 MB/s \n","\u001b[?25hCollecting google-cloud-vision<2,>=0.38.0\n","  Downloading google_cloud_vision-1.0.0-py2.py3-none-any.whl (435 kB)\n","\u001b[K     |████████████████████████████████| 435 kB 59.8 MB/s \n","\u001b[?25hCollecting google-apitools<1,>=0.5\n","  Downloading google-apitools-0.5.31.tar.gz (173 kB)\n","\u001b[K     |████████████████████████████████| 173 kB 95.7 MB/s \n","\u001b[?25hCollecting google-cloud-spanner<2,>=1.13.0\n","  Downloading google_cloud_spanner-1.19.1-py2.py3-none-any.whl (255 kB)\n","\u001b[K     |████████████████████████████████| 255 kB 62.4 MB/s \n","\u001b[?25hCollecting google-cloud-profiler<4,>=3.0.4\n","  Downloading google-cloud-profiler-3.0.5.tar.gz (34 kB)\n","Requirement already satisfied: google-cloud-core<2,>=0.28.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.31->tfx) (1.0.3)\n","Collecting websocket-client>=0.32.0\n","  Downloading websocket_client-1.2.1-py2.py3-none-any.whl (52 kB)\n","\u001b[K     |████████████████████████████████| 52 kB 1.9 MB/s \n","\u001b[?25hRequirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.8->tfx) (0.0.4)\n","Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.8->tfx) (3.0.1)\n","Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.8->tfx) (1.26.3)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client<2,>=1.8->tfx) (1.53.0)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client<2,>=1.8->tfx) (57.4.0)\n","Collecting fasteners>=0.14\n","  Downloading fasteners-0.16.3-py2.py3-none-any.whl (28 kB)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.18.0->apache-beam[gcp]<3,>=2.31->tfx) (4.7.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.18.0->apache-beam[gcp]<3,>=2.31->tfx) (0.2.8)\n","Collecting google-cloud-storage<2.0.0dev,>=1.26.0\n","  Downloading google_cloud_storage-1.42.0-py2.py3-none-any.whl (105 kB)\n","\u001b[K     |████████████████████████████████| 105 kB 68.5 MB/s \n","\u001b[?25hCollecting proto-plus>=1.10.1\n","  Downloading proto_plus-1.19.0-py3-none-any.whl (42 kB)\n","\u001b[K     |████████████████████████████████| 42 kB 2.1 MB/s \n","\u001b[?25hCollecting google-resumable-media<2.0dev,>=0.6.0\n","  Downloading google_resumable_media-1.3.3-py2.py3-none-any.whl (75 kB)\n","\u001b[K     |████████████████████████████████| 75 kB 6.3 MB/s \n","\u001b[?25hCollecting google-cloud-core<2,>=0.28.1\n","  Downloading google_cloud_core-1.7.2-py2.py3-none-any.whl (28 kB)\n","Collecting google-cloud-bigquery<2.21,>=1.28.0\n","  Downloading google_cloud_bigquery-2.19.0-py2.py3-none-any.whl (188 kB)\n","\u001b[K     |████████████████████████████████| 188 kB 87.2 MB/s \n","\u001b[?25h  Downloading google_cloud_bigquery-2.18.0-py2.py3-none-any.whl (188 kB)\n","\u001b[K     |████████████████████████████████| 188 kB 82.5 MB/s \n","\u001b[?25hCollecting grpc-google-iam-v1<0.13dev,>=0.12.3\n","  Downloading grpc-google-iam-v1-0.12.3.tar.gz (13 kB)\n","Collecting google-cloud-storage<2.0.0dev,>=1.26.0\n","  Downloading google_cloud_storage-1.41.1-py2.py3-none-any.whl (105 kB)\n","\u001b[K     |████████████████████████████████| 105 kB 76.9 MB/s \n","\u001b[?25hCollecting google-crc32c<2.0dev,>=1.0\n","  Downloading google_crc32c-1.1.2-cp37-cp37m-manylinux2014_x86_64.whl (38 kB)\n","Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<2.21,>=1.28.0->tfx) (1.14.6)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<2.21,>=1.28.0->tfx) (2.20)\n","Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.31->tfx) (0.6.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2<3,>=2.7.3->tfx) (2.0.1)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from keras-tuner<1.0.2,>=1->tfx) (0.8.9)\n","Collecting terminaltables\n","  Downloading terminaltables-3.1.0.tar.gz (12 kB)\n","Collecting colorama\n","  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from keras-tuner<1.0.2,>=1->tfx) (4.62.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner<1.0.2,>=1->tfx) (1.4.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from keras-tuner<1.0.2,>=1->tfx) (0.22.2.post1)\n","Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.7/dist-packages (from kubernetes<13,>=10.0.1->tfx) (1.24.3)\n","Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.7/dist-packages (from kubernetes<13,>=10.0.1->tfx) (1.3.0)\n","Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.7/dist-packages (from kubernetes<13,>=10.0.1->tfx) (2021.5.30)\n","Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5,>=2.0.1->apache-beam[gcp]<3,>=2.31->tfx) (0.4.8)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging<21,>=20->tfx) (2.4.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.31->tfx) (2.10)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.31->tfx) (2.0.4)\n","Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tfx) (3.1.0)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tfx) (0.37.0)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tfx) (1.1.2)\n","Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tfx) (0.4.0)\n","Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tfx) (1.6.3)\n","Collecting keras-nightly~=2.5.0.dev\n","  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 29.4 MB/s \n","\u001b[?25hRequirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tfx) (3.3.0)\n","Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tfx) (1.1.0)\n","Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tfx) (1.12)\n","Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tfx) (2.6.0)\n","Collecting tensorflow-estimator<2.6.0,>=2.5.0\n","  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n","\u001b[K     |████████████████████████████████| 462 kB 52.8 MB/s \n","\u001b[?25hCollecting grpcio<2,>=1.28.1\n","  Downloading grpcio-1.34.1-cp37-cp37m-manylinux2014_x86_64.whl (4.0 MB)\n","\u001b[K     |████████████████████████████████| 4.0 MB 43.2 MB/s \n","\u001b[?25hRequirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tfx) (1.12.1)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tfx) (0.2.0)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tfx) (1.5.2)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tfx) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tfx) (1.8.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tfx) (0.4.5)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tfx) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tfx) (3.3.4)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tfx) (4.6.4)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib->kubernetes<13,>=10.0.1->tfx) (3.1.1)\n","Requirement already satisfied: tensorflow-metadata<1.3,>=1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-data-validation<1.3.0,>=1.2.0->tfx) (1.2.0)\n","Collecting joblib<0.15,>=0.12\n","  Downloading joblib-0.14.1-py2.py3-none-any.whl (294 kB)\n","\u001b[K     |████████████████████████████████| 294 kB 88.1 MB/s \n","\u001b[?25hRequirement already satisfied: pandas<2,>=1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-data-validation<1.3.0,>=1.2.0->tfx) (1.1.5)\n","Collecting ipython<8,>=7\n","  Downloading ipython-7.26.0-py3-none-any.whl (786 kB)\n","\u001b[K     |████████████████████████████████| 786 kB 50.2 MB/s \n","\u001b[?25hRequirement already satisfied: ipywidgets<8,>=7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-analysis<0.34,>=0.33->tfx) (7.6.3)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.34,>=0.33->tfx) (4.8.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.34,>=0.33->tfx) (0.1.2)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.34,>=0.33->tfx) (4.4.2)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.34,>=0.33->tfx) (0.18.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.34,>=0.33->tfx) (0.7.5)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.34,>=0.33->tfx) (0.2.0)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.34,>=0.33->tfx) (2.6.1)\n","Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n","  Downloading prompt_toolkit-3.0.20-py3-none-any.whl (370 kB)\n","\u001b[K     |████████████████████████████████| 370 kB 41.7 MB/s \n","\u001b[?25hRequirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.34,>=0.33->tfx) (5.0.5)\n","Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.34,>=0.33->tfx) (3.5.1)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.34,>=0.33->tfx) (1.0.0)\n","Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.34,>=0.33->tfx) (5.1.3)\n","Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.34,>=0.33->tfx) (4.10.1)\n","Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.34,>=0.33->tfx) (5.1.1)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.34,>=0.33->tfx) (5.3.5)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython<8,>=7->tensorflow-model-analysis<0.34,>=0.33->tfx) (0.8.2)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.34,>=0.33->tfx) (0.2.0)\n","Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.34,>=0.33->tfx) (4.7.1)\n","Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.34,>=0.33->tfx) (2.6.0)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython<8,>=7->tensorflow-model-analysis<0.34,>=0.33->tfx) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8,>=7->tensorflow-model-analysis<0.34,>=0.33->tfx) (0.2.5)\n","Collecting tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15\n","  Downloading tensorflow_serving_api-2.5.2-py2.py3-none-any.whl (38 kB)\n","Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.34,>=0.33->tfx) (5.3.1)\n","Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.34,>=0.33->tfx) (0.11.0)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.34,>=0.33->tfx) (1.8.0)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.34,>=0.33->tfx) (5.6.1)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.34,>=0.33->tfx) (22.2.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tfx) (3.5.0)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.34,>=0.33->tfx) (0.8.4)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.34,>=0.33->tfx) (0.3)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.34,>=0.33->tfx) (1.4.3)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.34,>=0.33->tfx) (0.5.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.34,>=0.33->tfx) (0.7.1)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.34,>=0.33->tfx) (4.0.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.34,>=0.33->tfx) (0.5.1)\n","Building wheels for collected packages: avro-python3, dill, future, google-apitools, google-cloud-profiler, grpc-google-iam-v1, keras-tuner, terminaltables\n","  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for avro-python3: filename=avro_python3-1.9.2.1-py3-none-any.whl size=43512 sha256=b10bebb2d87a75dac3a46e8d072e873afc6cefd06679d54c8617903df1ea1097\n","  Stored in directory: /root/.cache/pip/wheels/bc/49/5f/fdb5b9d85055c478213e0158ac122b596816149a02d82e0ab1\n","  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78546 sha256=2f5bd284f08d127a8cc54f21edbf8a2ef7a56fe87e963feba154d0dff65a95fb\n","  Stored in directory: /root/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n","  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=f470531db3ca9a23a7d819036744db8286a5b2137023269d433d47c1f02fb759\n","  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n","  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for google-apitools: filename=google_apitools-0.5.31-py3-none-any.whl size=131040 sha256=dd4ed9c1135b45551175ffea4b9ebf0499fa19c08cf53b1710049ed939beb3ef\n","  Stored in directory: /root/.cache/pip/wheels/19/b5/2f/1cc3cf2b31e7a9cd1508731212526d9550271274d351c96f16\n","  Building wheel for google-cloud-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for google-cloud-profiler: filename=google_cloud_profiler-3.0.5-cp37-cp37m-linux_x86_64.whl size=705152 sha256=4094499709bc9a1c04b781718d5755774dc2bb6876b082081af08a85275addcc\n","  Stored in directory: /root/.cache/pip/wheels/9f/24/e7/54a8a406d778d2d81aafc6d45b6cc9cca479373d2037f9102a\n","  Building wheel for grpc-google-iam-v1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for grpc-google-iam-v1: filename=grpc_google_iam_v1-0.12.3-py3-none-any.whl size=18515 sha256=ba50d419833235b2dc8c2892aef2a76307bfa651cb894aff060831d01130b565\n","  Stored in directory: /root/.cache/pip/wheels/b9/ee/67/2e444183030cb8d31ce8b34cee34a7afdbd3ba5959ea846380\n","  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-tuner: filename=keras_tuner-1.0.1-py3-none-any.whl size=73198 sha256=95844a700f6d72ac1fedebc735d5c94f4444f9336f478fbc536b8c93207c963c\n","  Stored in directory: /root/.cache/pip/wheels/0b/cf/2f/1a1749d3a3650fac3305a8d7f9237b6de7c41068e2f8520ca2\n","  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for terminaltables: filename=terminaltables-3.1.0-py3-none-any.whl size=15354 sha256=2b62b1341a5689b862cb90526bac9e00f5d2ca7c59e34b0020d0f9a3ae6fb152\n","  Stored in directory: /root/.cache/pip/wheels/ba/ad/c8/2d98360791161cd3db6daf6b5e730f34021fc9367d5879f497\n","Successfully built avro-python3 dill future google-apitools google-cloud-profiler grpc-google-iam-v1 keras-tuner terminaltables\n","Installing collected packages: requests, prompt-toolkit, packaging, grpcio, ipython, grpcio-gcp, google-crc32c, tensorflow-estimator, pyarrow, proto-plus, keras-nightly, hdfs, grpc-google-iam-v1, google-resumable-media, google-cloud-core, future, fasteners, fastavro, dill, avro-python3, tensorflow, google-cloud-vision, google-cloud-videointelligence, google-cloud-spanner, google-cloud-pubsub, google-cloud-profiler, google-cloud-language, google-cloud-dlp, google-cloud-bigtable, google-cloud-bigquery, google-apitools, apache-beam, websocket-client, tensorflow-serving-api, joblib, attrs, tfx-bsl, terminaltables, ml-metadata, google-cloud-storage, docker, colorama, tensorflow-transform, tensorflow-model-analysis, tensorflow-data-validation, ml-pipelines-sdk, kubernetes, keras-tuner, google-cloud-aiplatform, tfx\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.23.0\n","    Uninstalling requests-2.23.0:\n","      Successfully uninstalled requests-2.23.0\n","  Attempting uninstall: prompt-toolkit\n","    Found existing installation: prompt-toolkit 1.0.18\n","    Uninstalling prompt-toolkit-1.0.18:\n","      Successfully uninstalled prompt-toolkit-1.0.18\n","  Attempting uninstall: packaging\n","    Found existing installation: packaging 21.0\n","    Uninstalling packaging-21.0:\n","      Successfully uninstalled packaging-21.0\n","  Attempting uninstall: grpcio\n","    Found existing installation: grpcio 1.39.0\n","    Uninstalling grpcio-1.39.0:\n","      Successfully uninstalled grpcio-1.39.0\n","  Attempting uninstall: ipython\n","    Found existing installation: ipython 5.5.0\n","    Uninstalling ipython-5.5.0:\n","      Successfully uninstalled ipython-5.5.0\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.6.0\n","    Uninstalling tensorflow-estimator-2.6.0:\n","      Successfully uninstalled tensorflow-estimator-2.6.0\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 3.0.0\n","    Uninstalling pyarrow-3.0.0:\n","      Successfully uninstalled pyarrow-3.0.0\n","  Attempting uninstall: google-resumable-media\n","    Found existing installation: google-resumable-media 0.4.1\n","    Uninstalling google-resumable-media-0.4.1:\n","      Successfully uninstalled google-resumable-media-0.4.1\n","  Attempting uninstall: google-cloud-core\n","    Found existing installation: google-cloud-core 1.0.3\n","    Uninstalling google-cloud-core-1.0.3:\n","      Successfully uninstalled google-cloud-core-1.0.3\n","  Attempting uninstall: future\n","    Found existing installation: future 0.16.0\n","    Uninstalling future-0.16.0:\n","      Successfully uninstalled future-0.16.0\n","  Attempting uninstall: dill\n","    Found existing installation: dill 0.3.4\n","    Uninstalling dill-0.3.4:\n","      Successfully uninstalled dill-0.3.4\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.6.0\n","    Uninstalling tensorflow-2.6.0:\n","      Successfully uninstalled tensorflow-2.6.0\n","  Attempting uninstall: google-cloud-language\n","    Found existing installation: google-cloud-language 1.2.0\n","    Uninstalling google-cloud-language-1.2.0:\n","      Successfully uninstalled google-cloud-language-1.2.0\n","  Attempting uninstall: google-cloud-bigquery\n","    Found existing installation: google-cloud-bigquery 1.21.0\n","    Uninstalling google-cloud-bigquery-1.21.0:\n","      Successfully uninstalled google-cloud-bigquery-1.21.0\n","  Attempting uninstall: joblib\n","    Found existing installation: joblib 1.0.1\n","    Uninstalling joblib-1.0.1:\n","      Successfully uninstalled joblib-1.0.1\n","  Attempting uninstall: attrs\n","    Found existing installation: attrs 21.2.0\n","    Uninstalling attrs-21.2.0:\n","      Successfully uninstalled attrs-21.2.0\n","  Attempting uninstall: google-cloud-storage\n","    Found existing installation: google-cloud-storage 1.18.1\n","    Uninstalling google-cloud-storage-1.18.1:\n","      Successfully uninstalled google-cloud-storage-1.18.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","pandas-gbq 0.13.3 requires google-cloud-bigquery[bqstorage,pandas]<2.0.0dev,>=1.11.1, but you have google-cloud-bigquery 2.18.0 which is incompatible.\n","multiprocess 0.70.12.2 requires dill>=0.3.4, but you have dill 0.3.1.1 which is incompatible.\n","jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.20 which is incompatible.\n","google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.26.0 which is incompatible.\n","google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed apache-beam-2.31.0 attrs-20.3.0 avro-python3-1.9.2.1 colorama-0.4.4 dill-0.3.1.1 docker-4.4.4 fastavro-1.4.4 fasteners-0.16.3 future-0.18.2 google-apitools-0.5.31 google-cloud-aiplatform-0.7.1 google-cloud-bigquery-2.18.0 google-cloud-bigtable-1.7.0 google-cloud-core-1.7.2 google-cloud-dlp-1.0.0 google-cloud-language-1.3.0 google-cloud-profiler-3.0.5 google-cloud-pubsub-1.7.0 google-cloud-spanner-1.19.1 google-cloud-storage-1.41.1 google-cloud-videointelligence-1.16.1 google-cloud-vision-1.0.0 google-crc32c-1.1.2 google-resumable-media-1.3.3 grpc-google-iam-v1-0.12.3 grpcio-1.34.1 grpcio-gcp-0.2.2 hdfs-2.6.0 ipython-7.26.0 joblib-0.14.1 keras-nightly-2.5.0.dev2021032900 keras-tuner-1.0.1 kubernetes-12.0.1 ml-metadata-1.2.0 ml-pipelines-sdk-1.2.0 packaging-20.9 prompt-toolkit-3.0.20 proto-plus-1.19.0 pyarrow-2.0.0 requests-2.26.0 tensorflow-2.5.1 tensorflow-data-validation-1.2.0 tensorflow-estimator-2.5.0 tensorflow-model-analysis-0.33.0 tensorflow-serving-api-2.5.2 tensorflow-transform-1.2.0 terminaltables-3.1.0 tfx-1.2.0 tfx-bsl-1.2.0 websocket-client-1.2.1\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["IPython","dill","google","prompt_toolkit","requests","tensorflow"]}}},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BA9KyD0446ka","executionInfo":{"status":"ok","timestamp":1629736927843,"user_tz":240,"elapsed":2797,"user":{"displayName":"Nik Bear Brown","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiM1Q26nVUtugBRkregYHIFfrTZNtlP8O-WMVvR=s64","userId":"17467854027018001755"}},"outputId":"a18921f8-01ec-48c2-8292-1b97056f9ae6"},"source":["!pip install tensorflow"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.5.1)\n","Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0)\n","Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.6.0)\n","Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0.dev2021032900)\n","Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n","Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n","Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.34.1)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.37.0)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n","Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n","Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n","Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n","Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.8.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (2.26.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (0.6.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.0.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (0.4.5)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.34.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (57.4.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (3.3.4)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.7.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.2.2)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (1.3.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow) (4.6.4)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.4.8)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.24.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.10)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2021.5.30)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (3.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow) (3.5.0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2GivNBNYjb3b"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"N-ePgV0Lj68Q"},"source":["### Import packages\n","\n","Let's begin by importing the required packages and modules. In case you want to replicate this in your local workstation, we used *Tensorflow v2.3.1* and *TFX v0.24.0*."]},{"cell_type":"code","metadata":{"id":"YIqpWK9efviJ","colab":{"base_uri":"https://localhost:8080/","height":522},"executionInfo":{"status":"error","timestamp":1629736932329,"user_tz":240,"elapsed":762,"user":{"displayName":"Nik Bear Brown","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiM1Q26nVUtugBRkregYHIFfrTZNtlP8O-WMVvR=s64","userId":"17467854027018001755"}},"outputId":"39a2f60a-71a1-4b85-ea1f-549f8f03f57c"},"source":["import tensorflow as tf\n","\n","from tfx.components import CsvExampleGen\n","from tfx.components import ExampleValidator\n","from tfx.components import SchemaGen\n","from tfx.components import StatisticsGen\n","from tfx.components import Transform\n","\n","from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n","from google.protobuf.json_format import MessageToDict\n","\n","import os\n","import pprint\n","pp = pprint.PrettyPrinter()"],"execution_count":4,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m_dep_map\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3015\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3016\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dep_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3017\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m   2812\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2813\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2814\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_provider\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: _DistInfoDistribution__dep_map","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m_parsed_pkg_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3006\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3007\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pkg_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3008\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m   2812\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2813\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2814\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_provider\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: _pkg_info","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-3cfd99e9039d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCsvExampleGen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExampleValidator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSchemaGen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tfx/components/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"Subpackage for TFX components.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# For component user to direct use tfx.components.[...] as an alias.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbulk_inferrer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBulkInferrer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimporter_node\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImporterNode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEvaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tfx/components/bulk_inferrer/component.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtfx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbulk_inferrer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdsl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase_beam_component\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdsl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexecutor_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tfx/components/bulk_inferrer/executor.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mabsl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mapache_beam\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtfx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/apache_beam/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mapache_beam\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcoders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mapache_beam\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mapache_beam\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypehints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mapache_beam\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/apache_beam/io/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mapache_beam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigquery\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mapache_beam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpubsub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mapache_beam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgcsio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/apache_beam/io/gcp/pubsub.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpubsub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0mpubsub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/cloud/pubsub.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpubsub_v1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPublisherClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpubsub_v1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSubscriberClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpubsub_v1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/cloud/pubsub_v1/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpubsub_v1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpubsub_v1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpublisher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpubsub_v1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msubscriber\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/cloud/pubsub_v1/publisher/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpubsub_v1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublisher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/cloud/pubsub_v1/publisher/client.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpubsub_v1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_gapic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpubsub_v1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpubsub_v1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgapic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpublisher_client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpubsub_v1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgapic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransports\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpublisher_grpc_transport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpubsub_v1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublisher\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/cloud/pubsub_v1/gapic/publisher_client.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0m_GAPIC_LIBRARY_VERSION\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"google-cloud-pubsub\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mget_distribution\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_provider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected string, Requirement, or Distribution\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mget_provider\u001b[0;34m(moduleOrReq)\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;34m\"\"\"Return an IResourceProvider for the named module or requirement\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mworking_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mrequire\u001b[0;34m(self, *requirements)\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0mincluded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meven\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mwere\u001b[0m \u001b[0malready\u001b[0m \u001b[0mactivated\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mworking\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m         \"\"\"\n\u001b[0;32m--> 886\u001b[0;31m         \u001b[0mneeded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_requirements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequirements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mneeded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(self, requirements, env, installer, replace_conflicting, extras)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# push the new requirements onto the stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m             \u001b[0mnew_requirements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m             \u001b[0mrequirements\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_requirements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mrequires\u001b[0;34m(self, extras)\u001b[0m\n\u001b[1;32m   2732\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrequires\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextras\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2733\u001b[0m         \u001b[0;34m\"\"\"List of Requirements needed for this distro if `extras` are used\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2734\u001b[0;31m         \u001b[0mdm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dep_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2735\u001b[0m         \u001b[0mdeps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2736\u001b[0m         \u001b[0mdeps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m_dep_map\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3016\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dep_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3017\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3018\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dep_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3019\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dep_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m_compute_dependencies\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3025\u001b[0m         \u001b[0mreqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m         \u001b[0;31m# Including any condition expressions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3027\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mreq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parsed_pkg_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Requires-Dist'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3028\u001b[0m             \u001b[0mreqs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_requirements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m_parsed_pkg_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3007\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pkg_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3008\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3009\u001b[0;31m             \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPKG_INFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3010\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pkg_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memail\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsestr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3011\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pkg_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mget_metadata\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1405\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_metadata_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m_get\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1611\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1612\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/usr/local/lib/python3.7/dist-packages/grpcio-1.39.0.dist-info/METADATA'"]}]},{"cell_type":"markdown","metadata":{"id":"37p5RT0V4A19"},"source":["### Define paths\n","\n","You will define a few global variables to indicate paths in the local workspace."]},{"cell_type":"code","metadata":{"id":"B9AQP90P4A19","executionInfo":{"status":"aborted","timestamp":1629736706899,"user_tz":240,"elapsed":214,"user":{"displayName":"Nik Bear Brown","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiM1Q26nVUtugBRkregYHIFfrTZNtlP8O-WMVvR=s64","userId":"17467854027018001755"}}},"source":["# location of the pipeline metadata store\n","_pipeline_root = './pipeline/'\n","\n","# directory of the raw data files\n","_data_root = './data/census_data'\n","\n","# path to the raw training data\n","_data_filepath = os.path.join(_data_root, 'adult.data')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n2cMMAbSkGfX"},"source":["### Preview the  dataset\n","\n","You will again be using the [Census Income dataset](https://archive.ics.uci.edu/ml/datasets/Adult) from the Week 1 ungraded lab so you can compare outputs when just using stand-alone TFDV and when using it under TFX. Just to remind, the data can be used to predict if an individual earns more than or less than 50k US Dollars annually. Here is the description of the features again: \n","\n","\n","* **age**: continuous.\n","* **workclass**: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n","* **fnlwgt**: continuous.\n","* **education**: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n","* **education-num**: continuous.\n","* **marital-status**: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n","* **occupation**: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n","* **relationship**: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n","* **race**: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n","* **sex**: Female, Male.\n","* **capital-gain**: continuous.\n","* **capital-loss**: continuous.\n","* **hours-per-week**: continuous.\n","* **native-country**: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands."]},{"cell_type":"code","metadata":{"id":"c5YPeLPFOXaD","executionInfo":{"status":"aborted","timestamp":1629736706901,"user_tz":240,"elapsed":215,"user":{"displayName":"Nik Bear Brown","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiM1Q26nVUtugBRkregYHIFfrTZNtlP8O-WMVvR=s64","userId":"17467854027018001755"}}},"source":["# preview the first few rows of the CSV file\n","!head {_data_filepath}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8ONIE_hdkPS4"},"source":["### Create the Interactive Context\n","\n","When pushing to production, you want to automate the pipeline execution using orchestrators such as [Apache Beam](https://beam.apache.org/) and [Kubeflow](https://www.kubeflow.org/). You will not be doing that just yet and will instead execute the pipeline from this notebook. When experimenting in a notebook environment, you will be *manually* executing the pipeline components (i.e. you are the orchestrator). For that, TFX provides the [Interactive Context](https://www.tensorflow.org/tfx/api_docs/python/tfx/orchestration/experimental/interactive/interactive_context/InteractiveContext) so you can step through each component and inspect its outputs."]},{"cell_type":"markdown","metadata":{"id":"tXx621iL4A2B"},"source":["You will initialize the `InteractiveContext` below. This will create a database in the `_pipeline_root` directory which the different components will use to save or get the state of the component executions. You will learn more about this in Week 3 when we discuss ML Metadata. For now, you can think of it as the data store that makes it possible for the different pipeline components to work together. \n","\n","*Note: You can configure the database to connect to but for this exercise, we will just use the default which is a newly created local sqlite file.* ***You will see the warning after running the cell below and you can safely ignore it.***"]},{"cell_type":"code","metadata":{"id":"0Rh6K5sUf9dd","executionInfo":{"status":"aborted","timestamp":1629736706901,"user_tz":240,"elapsed":215,"user":{"displayName":"Nik Bear Brown","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiM1Q26nVUtugBRkregYHIFfrTZNtlP8O-WMVvR=s64","userId":"17467854027018001755"}}},"source":["# Initialize the InteractiveContext with a local sqlite file.\n","# If you leave `_pipeline_root` blank, then the db will be created in a temporary directory.\n","# You can safely ignore the warning about the missing config file.\n","context = InteractiveContext(pipeline_root=_pipeline_root)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HdQWxfsVkzdJ"},"source":["## Run TFX components interactively\n","\n","With that, you can now run the pipeline interactively. You will see how to do that as you go through the different components below."]},{"cell_type":"markdown","metadata":{"id":"L9fwt9gQk3BR"},"source":["### ExampleGen\n","\n","You will start the pipeline with the [ExampleGen](https://www.tensorflow.org/tfx/guide/examplegen) component. This  will:\n","\n","*   split the data into training and evaluation sets (by default: 2/3 train, 1/3 eval).\n","*   convert each data row into `tf.train.Example` format. This [protocol buffer](https://developers.google.com/protocol-buffers) is designed for Tensorflow operations and is used by the TFX components.\n","*   compress and save the data collection under the `_pipeline_root` directory for other components to access. These examples are stored in `TFRecord` format. This optimizes read and write operations within Tensorflow especially if you have a large collection of data.\n","\n","Its constructor takes the path to your data source/directory. In our case, this is the `_data_root` path. The component supports several data sources such as CSV, tf.Record, and BigQuery. Since our data is a CSV file, we will use [CsvExampleGen](https://www.tensorflow.org/tfx/api_docs/python/tfx/components/CsvExampleGen) to ingest the data.\n","\n","Run the cell below to instantiate `CsvExampleGen`."]},{"cell_type":"code","metadata":{"id":"PyXjuMt8f-9u","scrolled":true,"executionInfo":{"status":"aborted","timestamp":1629736706902,"user_tz":240,"elapsed":216,"user":{"displayName":"Nik Bear Brown","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiM1Q26nVUtugBRkregYHIFfrTZNtlP8O-WMVvR=s64","userId":"17467854027018001755"}}},"source":["# Instantiate ExampleGen with the input CSV dataset\n","example_gen = CsvExampleGen(input_base=_data_root)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NZLr4nmI4A2D"},"source":["You can execute the component by calling the `run()` method of the `InteractiveContext`."]},{"cell_type":"code","metadata":{"id":"9Z3JSLpv4A2D","executionInfo":{"status":"aborted","timestamp":1629736706902,"user_tz":240,"elapsed":215,"user":{"displayName":"Nik Bear Brown","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiM1Q26nVUtugBRkregYHIFfrTZNtlP8O-WMVvR=s64","userId":"17467854027018001755"}}},"source":["# Execute the component\n","context.run(example_gen)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OqCoZh7KPUm9"},"source":["You will notice that an output cell showing the execution results is automatically shown. This metadata is recorded into the database created earlier. This allows you to keep track of your project runs. For example, if you run it again, you will notice the `.execution_id` incrementing.\n","\n","The output of the components are called *artifacts* and you can see an example by navigating through  `.component.outputs > ['examples'] > Channel > ._artifacts > [0]` above. It shows information such as where the converted data is stored (`.uri`) and the splits generated (`.split_names`).\n","\n","You can also examine the output artifacts programmatically with the code below."]},{"cell_type":"code","metadata":{"id":"880KkTAkPeUg","executionInfo":{"status":"aborted","timestamp":1629736706903,"user_tz":240,"elapsed":216,"user":{"displayName":"Nik Bear Brown","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiM1Q26nVUtugBRkregYHIFfrTZNtlP8O-WMVvR=s64","userId":"17467854027018001755"}}},"source":["# get the artifact object\n","artifact = example_gen.outputs['examples'].get()[0]\n","\n","# print split names and uri\n","print(f'split names: {artifact.split_names}')\n","print(f'artifact uri: {artifact.uri}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i5RViHIE4A2E"},"source":["If you're wondering , the `number` in `./pipeline/CsvExampleGen/examples/{number}` is the execution id associated with that dataset. If you restart the kernel of this workspace and re-run up to this cell, you will notice a new folder with a different id name created. This shows that TFX is keeping versions of your data so you can roll back if you want to investigate a particular execution."]},{"cell_type":"markdown","metadata":{"id":"J6vcbW_wPqvl"},"source":["As mentioned, the ingested data is stored in the directory shown in the `uri` field. It is also compressed using `gzip` and you can verify by running the cell below."]},{"cell_type":"code","metadata":{"id":"zMgsAfyx4A2F","executionInfo":{"status":"aborted","timestamp":1629736706903,"user_tz":240,"elapsed":216,"user":{"displayName":"Nik Bear Brown","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiM1Q26nVUtugBRkregYHIFfrTZNtlP8O-WMVvR=s64","userId":"17467854027018001755"}}},"source":["# Get the URI of the output artifact representing the training examples\n","train_uri = os.path.join(artifact.uri, 'train')\n","\n","# See the contents of the `train` folder\n","!ls {train_uri}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dQjRRW3F4A2F"},"source":["In a notebook environment, it may be useful to examine a few examples of the data especially if you're still experimenting. Since the data collection is saved in [TFRecord format](https://www.tensorflow.org/tutorials/load_data/tfrecord), you will need to use methods that work with that data type. You will need to unpack the individual examples from the `TFRecord` file and format it for printing. Let's do that in the following cells:"]},{"cell_type":"code","metadata":{"id":"H4XIXjiCPwzQ","executionInfo":{"status":"aborted","timestamp":1629736706903,"user_tz":240,"elapsed":216,"user":{"displayName":"Nik Bear Brown","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiM1Q26nVUtugBRkregYHIFfrTZNtlP8O-WMVvR=s64","userId":"17467854027018001755"}}},"source":["# Get the list of files in this directory (all compressed TFRecord files)\n","tfrecord_filenames = [os.path.join(train_uri, name)\n","                      for name in os.listdir(train_uri)]\n","\n","# Create a `TFRecordDataset` to read these files\n","dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LKLHUJgE4A2G","executionInfo":{"status":"aborted","timestamp":1629736706904,"user_tz":240,"elapsed":216,"user":{"displayName":"Nik Bear Brown","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiM1Q26nVUtugBRkregYHIFfrTZNtlP8O-WMVvR=s64","userId":"17467854027018001755"}}},"source":["# Define a helper function to get individual examples\n","def get_records(dataset, num_records):\n","    '''Extracts records from the given dataset.\n","    Args:\n","        dataset (TFRecordDataset): dataset saved by ExampleGen\n","        num_records (int): number of records to preview\n","    '''\n","    \n","    # initialize an empty list\n","    records = []\n","    \n","    # Use the `take()` method to specify how many records to get\n","    for tfrecord in dataset.take(num_records):\n","        \n","        # Get the numpy property of the tensor\n","        serialized_example = tfrecord.numpy()\n","        \n","        # Initialize a `tf.train.Example()` to read the serialized data\n","        example = tf.train.Example()\n","        \n","        # Read the example data (output is a protocol buffer message)\n","        example.ParseFromString(serialized_example)\n","        \n","        # convert the protocol bufffer message to a Python dictionary\n","        example_dict = (MessageToDict(example))\n","        \n","        # append to the records list\n","        records.append(example_dict)\n","        \n","    return records"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FmuMRt-y4A2J","executionInfo":{"status":"aborted","timestamp":1629736706905,"user_tz":240,"elapsed":217,"user":{"displayName":"Nik Bear Brown","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiM1Q26nVUtugBRkregYHIFfrTZNtlP8O-WMVvR=s64","userId":"17467854027018001755"}}},"source":["# Get 3 records from the dataset\n","sample_records = get_records(dataset, 3)\n","\n","# Print the output\n","pp.pprint(sample_records)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2gluYjccf-IP"},"source":["Now that `ExampleGen` has finished ingesting the data, the next step is data analysis."]},{"cell_type":"markdown","metadata":{"id":"csM6BFhtk5Aa"},"source":["### StatisticsGen\n","The [StatisticsGen](https://www.tensorflow.org/tfx/guide/statsgen) component computes statistics over your dataset for data analysis, as well as for use in downstream components (i.e. next steps in the pipeline). As mentioned earlier, this component uses TFDV under the hood so its output will be familiar to you.\n","\n","`StatisticsGen` takes as input the dataset we just ingested using `CsvExampleGen`."]},{"cell_type":"code","metadata":{"id":"MAscCCYWgA-9","executionInfo":{"status":"aborted","timestamp":1629736706907,"user_tz":240,"elapsed":219,"user":{"displayName":"Nik Bear Brown","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiM1Q26nVUtugBRkregYHIFfrTZNtlP8O-WMVvR=s64","userId":"17467854027018001755"}}},"source":["# Instantiate StatisticsGen with the ExampleGen ingested dataset\n","statistics_gen = StatisticsGen(\n","    examples=example_gen.outputs['examples'])\n","\n","# Execute the component\n","context.run(statistics_gen)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8m7BFHiE4A2K"},"source":["You can display the statistics with the `show()` method.\n","\n","*Note: You can safely ignore the warning shown when running the cell below.*"]},{"cell_type":"code","metadata":{"id":"Vywn16yw4A2K","executionInfo":{"status":"aborted","timestamp":1629736706907,"user_tz":240,"elapsed":218,"user":{"displayName":"Nik Bear Brown","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiM1Q26nVUtugBRkregYHIFfrTZNtlP8O-WMVvR=s64","userId":"17467854027018001755"}}},"source":["# Show the output statistics\n","context.show(statistics_gen.outputs['statistics'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HLKLTO9Nk60p"},"source":["### SchemaGen\n","\n","The [SchemaGen](https://www.tensorflow.org/tfx/guide/schemagen) component also uses TFDV to generate a schema based on your data statistics. As you've learned previously, a schema defines the expected bounds, types, and properties of the features in your dataset.\n","\n","`SchemaGen` will take as input the statistics that we generated with `StatisticsGen`, looking at the training split by default."]},{"cell_type":"code","metadata":{"id":"ygQvZ6hsiQ_J","executionInfo":{"status":"aborted","timestamp":1629736706908,"user_tz":240,"elapsed":219,"user":{"displayName":"Nik Bear Brown","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiM1Q26nVUtugBRkregYHIFfrTZNtlP8O-WMVvR=s64","userId":"17467854027018001755"}}},"source":["# Instantiate SchemaGen with the StatisticsGen ingested dataset\n","schema_gen = SchemaGen(\n","    statistics=statistics_gen.outputs['statistics'],\n","    )\n","\n","# Run the component\n","context.run(schema_gen)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zi6TxTUKXM6b"},"source":["You can then visualize the generated schema as a table."]},{"cell_type":"code","metadata":{"id":"Ec9vqDXpXeMb","executionInfo":{"status":"aborted","timestamp":1629736706908,"user_tz":240,"elapsed":219,"user":{"displayName":"Nik Bear Brown","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiM1Q26nVUtugBRkregYHIFfrTZNtlP8O-WMVvR=s64","userId":"17467854027018001755"}}},"source":["# Visualize the schema\n","context.show(schema_gen.outputs['schema'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jZkJ06vh4A2L"},"source":["Let's now move to the next step in the pipeline and see if there are any anomalies in the data."]},{"cell_type":"markdown","metadata":{"id":"V1qcUuO9k9f8"},"source":["### ExampleValidator\n","\n","The [ExampleValidator](https://www.tensorflow.org/tfx/guide/exampleval) component detects anomalies in your data based on the generated schema from the previous step. Like the previous two components, it also uses TFDV under the hood. \n","\n","`ExampleValidator` will take as input the statistics from `StatisticsGen` and the schema from `SchemaGen`. By default, it compares the statistics from the evaluation split to the schema from the training split."]},{"cell_type":"code","metadata":{"id":"XRlRUuGgiXks","executionInfo":{"status":"aborted","timestamp":1629736706908,"user_tz":240,"elapsed":219,"user":{"displayName":"Nik Bear Brown","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiM1Q26nVUtugBRkregYHIFfrTZNtlP8O-WMVvR=s64","userId":"17467854027018001755"}}},"source":["# Instantiate ExampleValidator with the StatisticsGen and SchemaGen ingested data\n","example_validator = ExampleValidator(\n","    statistics=statistics_gen.outputs['statistics'],\n","    schema=schema_gen.outputs['schema'])\n","\n","# Run the component.\n","context.run(example_validator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"855mrHgJcoer"},"source":["As with the previous component, you can also visualize the anomalies as a table."]},{"cell_type":"code","metadata":{"id":"TDyAAozQcrk3","executionInfo":{"status":"aborted","timestamp":1629736706909,"user_tz":240,"elapsed":220,"user":{"displayName":"Nik Bear Brown","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiM1Q26nVUtugBRkregYHIFfrTZNtlP8O-WMVvR=s64","userId":"17467854027018001755"}}},"source":["# Visualize the results\n","context.show(example_validator.outputs['anomalies'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jNP1-U1r4A2M"},"source":["With no anomalies detected, you can proceed to the next step in the pipeline."]},{"cell_type":"markdown","metadata":{"id":"JPViEz5RlA36"},"source":["### Transform\n","The [Transform](https://www.tensorflow.org/tfx/guide/transform) component performs feature engineering for both training and serving datasets. It uses the [TensorFlow Transform](https://www.tensorflow.org/tfx/transform/get_started) library introduced in the first ungraded lab of this week.\n","\n","`Transform` will take as input the data from `ExampleGen`, the schema from `SchemaGen`, as well as a module containing the preprocessing function.\n","\n","In this section, you will work on an example of a user-defined Transform code. The pipeline needs to load this as a module so you need to use the magic command `%% writefile` to save the file to disk. Let's first define a few constants that group the data's attributes according to the transforms we will perform later. This file will also be saved locally."]},{"cell_type":"code","metadata":{"id":"PuNSiUKb4YJf","executionInfo":{"status":"aborted","timestamp":1629736706909,"user_tz":240,"elapsed":220,"user":{"displayName":"Nik Bear Brown","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiM1Q26nVUtugBRkregYHIFfrTZNtlP8O-WMVvR=s64","userId":"17467854027018001755"}}},"source":["# Set the constants module filename\n","_census_constants_module_file = 'census_constants.py'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HPjhXuIF4YJh","executionInfo":{"status":"aborted","timestamp":1629736706910,"user_tz":240,"elapsed":220,"user":{"displayName":"Nik Bear Brown","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiM1Q26nVUtugBRkregYHIFfrTZNtlP8O-WMVvR=s64","userId":"17467854027018001755"}}},"source":["%%writefile {_census_constants_module_file}\n","\n","# Features with string data types that will be converted to indices\n","CATEGORICAL_FEATURE_KEYS = [\n","    'education', 'marital-status', 'occupation', 'race', 'relationship', 'workclass', 'sex', 'native-country'\n","]\n","\n","# Numerical features that are marked as continuous\n","NUMERIC_FEATURE_KEYS = ['fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n","\n","# Feature that can be grouped into buckets\n","BUCKET_FEATURE_KEYS = ['age']\n","\n","# Number of buckets used by tf.transform for encoding each bucket feature.\n","FEATURE_BUCKET_COUNT = {'age': 4}\n","\n","# Feature that the model will predict\n","LABEL_KEY = 'label'\n","\n","# Utility function for renaming the feature\n","def transformed_name(key):\n","    return key + '_xf'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Duj2Ax5z4YJl"},"source":["Next, you will work on the module that contains `preprocessing_fn()`. As you've seen in the previous lab, this function defines how you will transform the raw data into features that your model can train on (i.e. the next step in the pipeline). You will use the [tft module functions](https://www.tensorflow.org/tfx/transform/api_docs/python/tft) to make these transformations. \n","\n","*Note: After completing the entire notebook, we encourage you to go back to this section and try different tft functions aside from the ones already provided below. You can also modify the grouping of the feature keys in the constants file if you want. For example, you may want to scale some features to `[0, 1]` while others are scaled to the z-score. This will be good practice for this week's assignment.*"]},{"cell_type":"code","metadata":{"id":"4AJ9hBs94YJm","executionInfo":{"status":"aborted","timestamp":1629736706910,"user_tz":240,"elapsed":220,"user":{"displayName":"Nik Bear Brown","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiM1Q26nVUtugBRkregYHIFfrTZNtlP8O-WMVvR=s64","userId":"17467854027018001755"}}},"source":["# Set the transform module filename\n","_census_transform_module_file = 'census_transform.py'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MYmxxx9A4YJn","executionInfo":{"status":"aborted","timestamp":1629736706910,"user_tz":240,"elapsed":220,"user":{"displayName":"Nik Bear Brown","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiM1Q26nVUtugBRkregYHIFfrTZNtlP8O-WMVvR=s64","userId":"17467854027018001755"}}},"source":["%%writefile {_census_transform_module_file}\n","\n","import tensorflow as tf\n","import tensorflow_transform as tft\n","\n","import census_constants\n","\n","# Unpack the contents of the constants module\n","_NUMERIC_FEATURE_KEYS = census_constants.NUMERIC_FEATURE_KEYS\n","_CATEGORICAL_FEATURE_KEYS = census_constants.CATEGORICAL_FEATURE_KEYS\n","_BUCKET_FEATURE_KEYS = census_constants.BUCKET_FEATURE_KEYS\n","_FEATURE_BUCKET_COUNT = census_constants.FEATURE_BUCKET_COUNT\n","_LABEL_KEY = census_constants.LABEL_KEY\n","_transformed_name = census_constants.transformed_name\n","\n","\n","# Define the transformations\n","def preprocessing_fn(inputs):\n","    \"\"\"tf.transform's callback function for preprocessing inputs.\n","    Args:\n","        inputs: map from feature keys to raw not-yet-transformed features.\n","    Returns:\n","        Map from string feature key to transformed feature operations.\n","    \"\"\"\n","    outputs = {}\n","\n","    # Scale these features to the range [0,1]\n","    for key in _NUMERIC_FEATURE_KEYS:\n","        outputs[_transformed_name(key)] = tft.scale_to_0_1(\n","            inputs[key])\n","    \n","    # Bucketize these features\n","    for key in _BUCKET_FEATURE_KEYS:\n","        outputs[_transformed_name(key)] = tft.bucketize(\n","            inputs[key], _FEATURE_BUCKET_COUNT[key],\n","            always_return_num_quantiles=False)\n","\n","    # Convert strings to indices in a vocabulary\n","    for key in _CATEGORICAL_FEATURE_KEYS:\n","        outputs[_transformed_name(key)] = tft.compute_and_apply_vocabulary(inputs[key])\n","\n","    # Convert the label strings to an index\n","    outputs[_transformed_name(_LABEL_KEY)] = tft.compute_and_apply_vocabulary(inputs[_LABEL_KEY])\n","\n","    return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wgbmZr3sgbWW"},"source":["You can now pass the training data, schema, and transform module to the `Transform` component. You can ignore the warning messages generated by Apache Beam regarding type hints."]},{"cell_type":"code","metadata":{"id":"jHfhth_GiZI9","scrolled":false,"executionInfo":{"status":"aborted","timestamp":1629736706913,"user_tz":240,"elapsed":18,"user":{"displayName":"Nik Bear Brown","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiM1Q26nVUtugBRkregYHIFfrTZNtlP8O-WMVvR=s64","userId":"17467854027018001755"}}},"source":["# Ignore TF warning messages\n","tf.get_logger().setLevel('ERROR')\n","\n","# Instantiate the Transform component\n","transform = Transform(\n","    examples=example_gen.outputs['examples'],\n","    schema=schema_gen.outputs['schema'],\n","    module_file=os.path.abspath(_census_transform_module_file))\n","\n","# Run the component\n","context.run(transform)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fwAwb4rARRQ2"},"source":["Let's examine the output artifacts of `Transform` (i.e. `.component.outputs` from the output cell above). This component produces several outputs:\n","\n","* `transform_graph` is the graph that can perform the preprocessing operations. This graph will be included during training and serving to ensure consistent transformations of incoming data.\n","* `transformed_examples` points to the preprocessed training and evaluation data.\n","* `updated_analyzer_cache` are stored calculations from previous runs."]},{"cell_type":"markdown","metadata":{"id":"vyFkBd9AR1sy"},"source":["Take a peek at the `transform_graph` artifact.  It points to a directory containing three subdirectories."]},{"cell_type":"code","metadata":{"id":"5tRw4DneR3i7","executionInfo":{"status":"aborted","timestamp":1629736706913,"user_tz":240,"elapsed":18,"user":{"displayName":"Nik Bear Brown","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiM1Q26nVUtugBRkregYHIFfrTZNtlP8O-WMVvR=s64","userId":"17467854027018001755"}}},"source":["# Get the uri of the transform graph\n","transform_graph_uri = transform.outputs['transform_graph'].get()[0].uri\n","\n","# List the subdirectories under the uri\n","os.listdir(transform_graph_uri)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4fqV54CIR6Pu"},"source":["* The `metadata` subdirectory contains the schema of the original data.\n","* The `transformed_metadata` subdirectory contains the schema of the preprocessed data. \n","* The `transform_fn` subdirectory contains the actual preprocessing graph. \n","\n","You can also take a look at the first three transformed examples using the helper function defined earlier."]},{"cell_type":"code","metadata":{"id":"pwbW2zPKR_S4","executionInfo":{"status":"aborted","timestamp":1629736706914,"user_tz":240,"elapsed":18,"user":{"displayName":"Nik Bear Brown","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiM1Q26nVUtugBRkregYHIFfrTZNtlP8O-WMVvR=s64","userId":"17467854027018001755"}}},"source":["# Get the URI of the output artifact representing the transformed examples\n","train_uri = os.path.join(transform.outputs['transformed_examples'].get()[0].uri, 'train')\n","\n","# Get the list of files in this directory (all compressed TFRecord files)\n","tfrecord_filenames = [os.path.join(train_uri, name)\n","                      for name in os.listdir(train_uri)]\n","\n","# Create a `TFRecordDataset` to read these files\n","transformed_dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mSDZ2rJC7NQW","executionInfo":{"status":"aborted","timestamp":1629736706914,"user_tz":240,"elapsed":18,"user":{"displayName":"Nik Bear Brown","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiM1Q26nVUtugBRkregYHIFfrTZNtlP8O-WMVvR=s64","userId":"17467854027018001755"}}},"source":["# Get 3 records from the dataset\n","sample_records_xf = get_records(transformed_dataset, 3)\n","\n","# Print the output\n","pp.pprint(sample_records_xf)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7eOnUils4A2O"},"source":["**Congratulations!** You have now executed all the components in our pipeline. You will get hands-on practice as well with training and model evaluation in future courses but for now, we encourage you to try exploring the different components we just discussed. As mentioned earlier, a useful exercise for the upcoming assignment is to be familiar with using different `tft` functions in your transform module. You can also try loading a different dataset (such as [this](https://archive.ics.uci.edu/ml/datasets/Seoul+Bike+Sharing+Demand)) and see how you may want to transform those features."]}]}