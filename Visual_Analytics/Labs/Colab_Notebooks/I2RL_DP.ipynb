{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"I2RL_DP.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPLxAvpYiIvZUiKyDdshU2d"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"6gyQhTTu7Rrj","colab_type":"text"},"source":["# Dynamic Programming\n","\n","Dynamic programming. Break up a problem into a series of ordered subproblems; combine solutions to smaller subproblems to form solutions to a large subproblem.   \n","\n","## Fibonacci sequence \n","\n","Using dynamic programming in the calculation of the *n*th member of the\n","[Fibonacci sequence] improves its performance greatly. Here is a naïve\n","implementation, based directly on the mathematical definition:\n","\n","`   `**`function`**` fib(n)`\\\n","`       `**`if`**` n <= 1 `**`return`**` n`\\\n","`       `**`return`**` fib(n − 1) + fib(n − 2)`\n","\n","Notice that if we call, say, `fib(5)`, we produce a call tree that calls\n","the function on the same value many different times:\n","\n","1.  `fib(5)`\n","2.  `fib(4) + fib(3)`\n","3.  `(fib(3) + fib(2)) + (fib(2) + fib(1))`\n","4.  `((fib(2) + fib(1)) + (fib(1) + fib(0))) + ((fib(1) + fib(0)) + fib(1))`\n","5.  `(((fib(1) + fib(0)) + fib(1)) + (fib(1) + fib(0))) + ((fib(1) + fib(0)) + fib(1))`\n","\n","In particular, `fib(2)` was calculated three times from scratch. In\n","larger examples, many more values of `fib`, or *subproblems*, are\n","recalculated, leading to an exponential time algorithm.\n","\n","Now, suppose we have a simple [map] object, *m*, which maps each value\n","of `fib` that has already been calculated to its result, and we modify\n","our function to use it and update it. The resulting function requires\n","only [O][](*n*) time instead of exponential time (but requires\n","[O][](*n*) space):\n","\n","`   `**`var`**` m := `***`map`***`(0 → 0, 1 → 1)`\\\n","`   `**`function`**` fib(n)`\\\n","`       `**`if`` `*`key`***` n `**`is`` ``not`` ``in`` `*`map`***` m `\\\n","`           m[n] := fib(n − 1) + fib(n − 2)`\\\n","`       `**`return`**` m[n]`\n","\n","This technique of saving values that have already been calculated is\n","called *[memoization]*; this is the top-down approach, since we first\n","break the problem into subproblems and then calculate and store values.\n","\n","In the **bottom-up** approach, we calculate the smaller values of `fib`\n","first, then build larger values from them. This method also uses O(*n*)\n","time since it contains a loop that repeats n − 1 times, but it only\n","takes constant (O(1)) space, in contrast to the top-down approach which\n","requires O(*n*) space to store the map.\n","\n","`   `**`function`**` fib(n)`\\\n","`       `**`if`**` n = 0`\\\n","`           `**`return`**` 0`\\\n","`       `**`else`**\\\n","`           `**`var`**` previousFib := 0, currentFib := 1`\\\n","`           `**`repeat`**` n − 1 `**`times`**` `*`//`` ``loop`` ``is`` ``skipped`` ``if`` ``n`` ``=`` ``1`*\\\n","`               `**`var`**` newFib := previousFib + currentFib`\\\n","`               previousFib := currentFib`\\\n","`               currentFib  := newFib`\\\n","`       `**`return`**` currentFib`\n","\n","In both examples, we only calculate `fib(2)` one time, and then use it\n","to calculate both `fib(4)` and `fib(3)`, instead of computing it every\n","time either of them is evaluated.\n","\n","to \n","Markdown (pandoc)\n","\n","## A Dynamic Decision Problem \n","\n","Let the state at time $t$ be $x_t$. For a decision that begins at time\n","0, we take as given the initial state $x_0$. At any time, the set of\n","possible actions depends on the current state; we can write this as\n","$a_{t} \\in \\Gamma (x_t)$, where the action $a_t$ represents one or more\n","control variables. We also assume that the state changes from $x$ to a\n","new state $T(x,a)$ when action $a$ is taken, and that the current payoff\n","from taking action $a$ in state $x$ is $F(x,a)$. Finally, we assume\n","impatience, represented by a [discount factor] $0<\\beta<1$.\n","\n","Under these assumptions, an infinite-horizon decision problem takes the\n","following form:\n","\n","$$V(x_0) \\; = \\; \\max_{ \\left \\{ a_{t} \\right \\}_{t=0}^{\\infty} }  \\sum_{t=0}^{\\infty} \\beta^t F(x_t,a_{t}),$$\n","\n","subject to the constraints\n","\n","$$a_{t} \\in \\Gamma (x_t), \\; x_{t+1}=T(x_t,a_t), \\; \\forall t = 0, 1, 2, \\dots$$\n","\n","Notice that we have defined notation $V(x_0)$ to denote the optimal\n","value that can be obtained by maximizing this objective function subject\n","to the assumed constraints. This function is the *value function*. It is\n","a function of the initial state variable $x_0$, since the best value\n","obtainable depends on the initial situation.\n","\n","### Bellman\\'s principle of optimality  \n","\n","The dynamic programming method breaks this decision problem into smaller\n","subproblems. Bellman\\'s *principle of optimality* describes how to do\n","this:\n","\n","> Principle of Optimality: An optimal policy has the property that\n","> whatever the initial state and initial decision are, the remaining\n","> decisions must constitute an optimal policy with regard to the state\n","> resulting from the first decision.  \n","\n","In computer science, a problem that can be broken apart like this is\n","said to have optimal substructure. In the context of dynamic game\n","theory, this principle is analogous to the concept of subgame perfect\n","equilibrium, although what constitutes an optimal policy in this case\n","is conditioned on the decision-maker\\'s opponents choosing similarly\n","optimal policies from their points of view.\n","\n","As suggested by the *principle of optimality*, we will consider the\n","first decision separately, setting aside all future decisions (we will\n","start afresh from time 1 with the new state $x_1$). Collecting the\n","future decisions in brackets on the right, the above infinite-horizon\n","decision problem is equivalent\n","to:  \n","\n","$$\\max_{ a_0 } \\left \\{ F(x_0,a_0)\n","+ \\beta  \\left[ \\max_{ \\left \\{ a_{t} \\right \\}_{t=1}^{\\infty} }\n","\\sum_{t=1}^{\\infty} \\beta^{t-1} F(x_t,a_{t}):\n","a_{t} \\in \\Gamma (x_t), \\; x_{t+1}=T(x_t,a_t), \\; \\forall t \\geq 1 \\right] \\right \\}$$\n","\n","subject to the constraints\n","\n","$$a_0 \\in \\Gamma (x_0), \\; x_1=T(x_0,a_0).$$\n","\n","Here we are choosing $a_0$, knowing that our choice will cause the time\n","1 state to be $x_1=T(x_0,a_0)$. That new state will then affect the\n","decision problem from time 1 on. The whole future decision problem\n","appears inside the square brackets on the right.\n","\n","## The Bellman Equation (Recursive Definition)\n","\n","\n","The following is a recursive definition of Bellman equation. It can be simplified even further if we\n","drop time subscripts and plug in the value of the next state:\n","\n","$$V(x) = \\max_{a \\in \\Gamma (x) } \\{ F(x,a) + \\beta V(T(x,a)) \\}.$$\n","\n","The Bellman equation is classified as a [functional equation], because\n","solving it means finding the unknown function *V*, which is the *value\n","function*. Recall that the value function describes the best possible\n","value of the objective, as a function of the state *x*. By calculating\n","the value function, we will also find the function *a*(*x*) that\n","describes the optimal action as a function of the state; this is called\n","the *policy function*."]},{"cell_type":"code","metadata":{"id":"XZYLZHhr-TiG","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}