{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Thompson_Sampling_&_The_Multi_Armed_Bandit_Problem.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"KLHLVLOYi-l7"},"source":["# Reinforcement Learning - Thompson Sampling & the Multi-Armed Bandit Problem\n","This notebook, implements **Thompson Sampling** in order to solve an instance of the famous **Multi-Armed Bandit Problem**.\n","\n","This notebook is adapted for the notebook in Dr. Daniel Soper's YouTube channel https://www.youtube.com/user/DanSoper33 and \n","Nando de Freitas YouTube channel https://www.youtube.com/channel/UC0z_jCi0XWqI8awUuQRFnyw and Wikepedia (Thompson sampling https://en.wikipedia.org/wiki/Thompson_sampling and Multi-Armed Bandit Problem https://en.wikipedia.org/wiki/Multi-armed_bandit). In particular, the videos and Machine learning - Bayesian optimization and multi-armed bandits https://youtu.be/vz3D36VXefI.  The norebook also borrows from Thompson Sampling by Andre Cianflone  https://github.com/andrecianflone/thompson\n","\n","\n","### Thompson sampling  \n","\n","\n","**Thompson sampling**, named after William R. Thompson, is a\n","heuristic for choosing actions that addresses the\n","exploration-exploitation dilemma in the multi-armed bandit problem. It\n","consists of choosing the action that maximizes the expected reward with\n","respect to a randomly drawn belief.  The algorithm addresses a broad range of problems in a computationally efficient manner and is therefore enjoying wide use.  For a more extensive review of the theory, checkout A Tutorial on Thompson Sampling by Russo et al., 2017.  https://arxiv.org/abs/1707.02038\n","\n","\n","\n","Thompson sampling is powerful but simple algorithm that implicitly balances exploration and exploitation based on quality and uncertainty. \n","If we have a k-armed bandit and model the probability that each arm gives us a positive reward. The goal is of course to maximize our rewards by pulling the most promising arm. \n","\n","Thompson sampling, on the other hand, incorporates uncertainty by modelling the bandit's Bernouilli parameter with a prior beta distribution. Thompson sampling can be generalized to sample from any arbitrary distributions over its parameters.\n","\n","The beauty of the algorithm is that it always chooses the action with the highest expected reward, with the twist that this reward is weighted by *uncertainty* through the posterior distribution.\n","\n","This is a Bayesian approach to the bandit problem. In our Bernouilli bandit setup, each action $k$ returns reward of 1 with probability $\\theta_k$, and 0 with probability $1-\\theta_k$. At the beginning of a simulation, each $\\theta_k$ is sampled from a uniform distribution $\\theta_k \\sim Uniform(0,1)$ with $\\theta_k$ held constant for the rest of that simulation (in the stationary case). The agent begins with a prior belief of the reward of each arm $k$ with a beta distribution, where $\\alpha = \\beta = 1$. The prior probability density of each $\\theta_k$ is:\n","\n","$$\n","p(\\theta_k) = \\frac{\\Gamma(\\alpha_k + \\beta_k)}{\\Gamma(\\alpha_k)\\Gamma(\\beta_k)} \\theta_k^{\\alpha_k -1} (1-\\theta_k)^{\\beta_k-1}\n","$$\n","\n","An action is chosen by first sampling from the beta distribution, followed by choosing the action with highest mean reward:\n","$$\n","x_t = \\text{argmax}_k (\\hat{\\theta}_k), \\quad \\hat{\\theta}_k \\sim \\text{beta}(\\alpha_k, \\beta_k)\n","$$\n","\n","According to Bayes' rule, an action's posterior distribution is updated depending on the reward $r_t$ received:\n","$$\n","(\\alpha_k, \\beta_k) = (\\alpha_k, \\beta_k) + (r_t, 1-r_t)\n","$$\n","\n","Thus the actions' posterior distribution and uncertainty are constantly updated throughout the simulation.\n","The idea\n","-----------\n","\n","The top graph below shows a posterior probability of a reward $P(x)$ (e.g. getting a winning pull on a slot machine or getting a click in an ad campaign), given some action space $x$ (e.g. choosing a slot machine or ad channel). The bands represent the uncertainty or variance around the estimates of $P(x)$. \n","\n","![Thompson sampling](https://raw.githubusercontent.com/nikbearbrown/Google_Colab/master/img/Thompson_Sampling.png)\n","\n","\n","_Thompson sampling from Nando de Freitas YouTube_\n","\n","Note that the two points on the right dominate those on the left so we never will try those.  However we may try sampling at 0.8 (e.g. slot machine 8) as there is the possibility that when the uncertainty is reduced it is better than 0.9 (e.g. slot machine 8).  We choose points where $\\mu({x})$ is high for exploitation. We choose points where $\\mu({x})$ is high and $\\sigma({x})$ is high for exploration. Note that in exploration there is no need to choose points where $\\sigma({x})$ is zero during exploration as well are confident about those values.\n","\n","An $\\epsilon$-greedy algorithm would, with probability $\\epsilon$, just as likely choose any action. The highly certain poor actions to the right of the graph are wasteful exploratory actiona.\n","\n","In practice we want to balance this tradeoff of exploitation versus exploration with an acquistion function that takes the form:\n","\n","$\\mu({x}) + \\kappa\\sigma({x})$ \n","\n","\n"," where $\\kappa$ is a exporation parameter, a constant times the variance. If you think exporation is important then you would choose a large $\\kappa$, a small $\\kappa$, say $0.1$ would favor exploitation and a $\\kappa$ of $0$ is pure exploitation.\n","\n","In Thompson sampling one doesn't have an explcit $\\kappa$ parameter, rather the uncertainty information explicitly captured via a posterior distribution. \n","\n","### Real-world example\n","\n","Imagine you have \\\\$1,000 to play the slot machines (bandits) and it costs a dollar to play a bandit. There are six slot machines available, and each turn playing a machine costs. Or imagine you have \\\\$100,000 and are placing \\\\$100 bids in various social media advertising chanels. The probability of winning on any given turn (which is called the ***conversion rate***) is unknown. Note that in this simplified examples the slots always pay out the same amount, or in the advertising chanel a win is getting a click. How would you adjust the problem if the slots could pay out different amounts? \n","\n","The probility of winning varies from bandit to bandit, but is constant for that bandit. How would you adjust the problem if the bandit could change the payout in response the the environment? For example ad channels payout probility was dependent on what was being advertised. Some channels were better for some things.\n","\n","The idea to to identify the best bandit or ad channel as quickly as possible.\n","\n","\n","### A simulation with slots\n","\n","Let's look at some code. The notebook code is mostly from  Dr. Daniel Soper's YouTube channel https://www.youtube.com/user/DanSoper33 "]},{"cell_type":"code","metadata":{"id":"ceOG7gbaAHVM"},"source":["#import libraries\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fw63WRQwghRp"},"source":["### Define Environment"]},{"cell_type":"code","metadata":{"id":"UtWPQQ5BASwf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607730791342,"user_tz":300,"elapsed":360,"user":{"displayName":"Nik Bear Brown","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiM1Q26nVUtugBRkregYHIFfrTZNtlP8O-WMVvR=s64","userId":"17467854027018001755"}},"outputId":"39144439-bc3e-450d-f072-cecade0ca0f1"},"source":["#Define the total number of turns (i.e., the number of times we will play a slot machine).\n","#Remember, we have $1,000 available, and each turn costs $1. We thus have 1,000 turns.\n","number_of_turns = 1000\n","\n","#define the total number of slot machines\n","number_of_bandits = 11\n","\n","#Define arrays where we can keep track of our wins (positive rewards) \n","#and losses (negative rewards) for each slot machine.\n","number_of_positive_rewards = np.zeros(number_of_bandits)\n","number_of_negative_rewards = np.zeros(number_of_bandits)\n","\n","#define a seed for the random number generator (to ensure that results are reproducible)\n","np.random.seed(5)\n","\n","#create a random conversion rate between 1% and 15% for each slot machine\n","# conversion_rates = np.random.uniform(0.001, 0.33, number_of_bandits)\n","conversion_rates = np.random.uniform(0.001, 0.33, number_of_bandits)\n","\n","#Show conversion rates for each slot machine. Remember that in a real-world scenario\n","#the decision-maker would not know this information!\n","for i in range(number_of_bandits):\n","  print('Conversion rate for slot machine {0}: {1:.2%}'.format(i, conversion_rates[i]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Conversion rate for slot machine 0: 7.40%\n","Conversion rate for slot machine 1: 28.75%\n","Conversion rate for slot machine 2: 6.90%\n","Conversion rate for slot machine 3: 30.32%\n","Conversion rate for slot machine 4: 16.17%\n","Conversion rate for slot machine 5: 20.23%\n","Conversion rate for slot machine 6: 25.30%\n","Conversion rate for slot machine 7: 17.16%\n","Conversion rate for slot machine 8: 9.86%\n","Conversion rate for slot machine 9: 6.28%\n","Conversion rate for slot machine 10: 2.76%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"52ZrENla-5EJ"},"source":["Note that if we get bandits/ad channels with very close high payout, say 13.19% and 13.86%. It may take a large number of tries to determine which is truely best. As the aquisition function simultainiously balances exploration and explotation then these two options should quickly $\\mu({x}) + \\kappa\\sigma({x})$ dominate the rest so the learner will quickly make good choices.\n","\n","Conversion rate for slot machine 0: 4.11%  \n","Conversion rate for slot machine 1: 13.19%  \n","Conversion rate for slot machine 2: 3.89%  \n","Conversion rate for slot machine 3: 13.86%  \n","Conversion rate for slot machine 4: 7.84%  \n","Conversion rate for slot machine 5: 9.56%  "]},{"cell_type":"markdown","metadata":{"id":"UAnwdAhRg19w"},"source":["### Create the Data Set"]},{"cell_type":"code","metadata":{"id":"1Gmwcfl2Ekx4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607730791343,"user_tz":300,"elapsed":356,"user":{"displayName":"Nik Bear Brown","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiM1Q26nVUtugBRkregYHIFfrTZNtlP8O-WMVvR=s64","userId":"17467854027018001755"}},"outputId":"b8eb1171-a280-411c-d1e8-87606b223bdd"},"source":["#The data set is a matrix with one row for each turn, and one column for each slot machine.\n","#Each item in the matrix represents the outcome of what would happen if we were to play a  \n","#particular slot machine on that particular turn. A value of \"1\" indicates that we would win, \n","#while a value of \"0\" indicates that we would lose. The number of \"wins\" for each slot machine\n","#is determined by its conversion rate.\n","outcomes = np.zeros((number_of_turns, number_of_bandits)) #create a two-dimensional numpy array, and fill it with zeros\n","for turn_index in range(number_of_turns): #for each turn\n","    for slot_machine_index in range(number_of_bandits): #for each slot machine\n","        #Get a random number between 0.0 and 1.0.\n","        #If the random number is less than or equal to this slot machine's conversion rate, then set the outcome to \"1\".\n","        #Otherwise, the outcome will be \"0\" because the entire matrix was initially filled with zeros.\n","        if np.random.rand() <= conversion_rates[slot_machine_index]:\n","            outcomes[turn_index][slot_machine_index] = 1\n","\n","#display the first 15 rows of data\n","print(outcomes[0:15, 0:6]) #this sort of indexing means \"rows 0 to 14\" (i.e., the first 15 rows) and \"columns 0 through 5\" (i.e., the first six columns)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 1. 0. 0.]\n"," [0. 1. 0. 1. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 1. 1.]\n"," [0. 0. 0. 0. 1. 0.]\n"," [0. 1. 0. 1. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 0. 1. 0.]\n"," [0. 1. 1. 1. 1. 0.]\n"," [0. 1. 0. 0. 0. 0.]\n"," [0. 1. 1. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xizw3zulA23b"},"source":["One reads the row below as slot/ad channel 0 and 4 won and the rest lost.  \n","\n","[1. 0. 0. 0. 1. 0.]"]},{"cell_type":"code","metadata":{"id":"l5RFn_AJhmLA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607730791461,"user_tz":300,"elapsed":470,"user":{"displayName":"Nik Bear Brown","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiM1Q26nVUtugBRkregYHIFfrTZNtlP8O-WMVvR=s64","userId":"17467854027018001755"}},"outputId":"fdc52ad7-cc1a-4947-aefa-99028a79fb6b"},"source":["#show means (i.e., conversion rates) for each column (i.e., for each slot machine)\n","for i in range(number_of_bandits):\n","  print('Mean for column {0}: {1:.2%}'.format(i, np.mean(outcomes[:, i])))\n","\n","#show true conversion rate\n","for i in range(number_of_bandits):\n","  print('True conversion rate for column {0}: {1:.2%}'.format(i, conversion_rates[i]))  "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mean for column 0: 7.30%\n","Mean for column 1: 28.70%\n","Mean for column 2: 6.50%\n","Mean for column 3: 31.50%\n","Mean for column 4: 14.60%\n","Mean for column 5: 20.90%\n","Mean for column 6: 24.30%\n","Mean for column 7: 17.70%\n","Mean for column 8: 8.80%\n","Mean for column 9: 7.30%\n","Mean for column 10: 3.00%\n","True conversion rate for column 0: 7.40%\n","True conversion rate for column 1: 28.75%\n","True conversion rate for column 2: 6.90%\n","True conversion rate for column 3: 30.32%\n","True conversion rate for column 4: 16.17%\n","True conversion rate for column 5: 20.23%\n","True conversion rate for column 6: 25.30%\n","True conversion rate for column 7: 17.16%\n","True conversion rate for column 8: 9.86%\n","True conversion rate for column 9: 6.28%\n","True conversion rate for column 10: 2.76%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iINsn-zDrHII"},"source":["# Daniel Soper's Approach for K bandits with Thompson Sampling\n","\n","Let's simulate using Thompson Sampling to determine which slot machine to play for each turn...\n","\n","Note this simulation is using a beta distribution rather than a Guassianm becuase it is being used to model trails in the form of wins and losses.\n","\n","In probability theory, the **beta distribution** is a\n","family of continuous probability distributions defined on the interval\n","\\[0, 1\\] parameterized by two positive shape parameters, denoted by\n","*α* and *β*, that appear as exponents of the random variable and control\n","the shape of the distribution.\n","\n","The beta distribution has been applied to model the behavior of random\n","variables limited to intervals of finite length in a wide variety of\n","disciplines.\n","\n","In Bayesian inference, the beta distribution is the [conjugate prior\n","probability distribution] for the Bernoulli, binomial, negative\n","binomial and geometric distributions. The beta distribution is a\n","suitable model for the random behavior of percentages and proportions.\n","\n","![Beta distribution](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Beta_distribution_pdf.svg/1062px-Beta_distribution_pdf.svg.png)\n","\n","_From https://en.wikipedia.org/wiki/Beta_distribution\n","\n","In the simulation below note that the max_beta is the highest beta generated for that round.  So we don't have an explicit hyperparameter   $\\kappa$ rather we are relying on taking a random sample. So rather than an acquistion function that takes the form:\n","\n","$\\mu({x}) + \\kappa\\sigma({x})$ \n","\n","it has this form\n","\n","$\\mathcal{X}$\n","\n","where $\\mathcal{X}$ is a random variable.\n","\n","**Algorithm: Thompson($K$,$\\alpha$, $\\beta$)**  \n","for $t$ = 1,2, ..., do  \n","  &nbsp;&nbsp; // sample action parameter from beta distribution  \n","  &nbsp;&nbsp; for $k = 1, \\dots, K$ do  \n","  &nbsp;&nbsp; &nbsp;&nbsp; Sample $\\hat{\\theta}_k \\sim \\text{beta}(\\alpha_k, \\beta_k)$  \n","  &nbsp;&nbsp; end for\n","  \n","  &nbsp;&nbsp; // select action, get reward  \n","  &nbsp;&nbsp; $x_t \\leftarrow \\text{argmax}_k \\hat{\\theta}_k$  \n","  &nbsp;&nbsp; $r_t \\leftarrow \\text{observe}(x_t)$  \n","  \n","  &nbsp;&nbsp; // update beta parameters  \n","  &nbsp;&nbsp; $(\\alpha_{x_t}, \\beta_{x_t}) \\leftarrow (\\alpha_{x_t}, \\beta_{x_t})+(r_t, 1-r_t)$  \n","end for\n","\n","The is known as beta-greedy "]},{"cell_type":"code","metadata":{"id":"8HDhqWdzi2oD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607730791462,"user_tz":300,"elapsed":467,"user":{"displayName":"Nik Bear Brown","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiM1Q26nVUtugBRkregYHIFfrTZNtlP8O-WMVvR=s64","userId":"17467854027018001755"}},"outputId":"990e8b09-568a-43e0-8d4c-1056be708123"},"source":["#for each turn\n","\n","for turn_index in range(number_of_turns):\n","    index_of_machine_to_play = -1\n","    max_beta = -1 # note that max beta\n","\n","    #determine which slot machine to play for this turn\n","    for slot_machine_index in range(number_of_bandits): #for each slot machine\n","        #Define the shape parameters for the beta distribution. The shape will depend on the number\n","        #of wins and losses that have thus far been observed for this particular slot machine.\n","        a = number_of_positive_rewards[slot_machine_index] + 1\n","        b = number_of_negative_rewards[slot_machine_index] + 1\n","\n","        #Get a random value from the beta distribution whose shape is defined by the number of\n","        #wins and losses that have thus far been observed for this slot machine\n","        random_beta = np.random.beta(a, b)\n","\n","        #if this is the largest beta value thus far observed for this iteration\n","        if random_beta > max_beta:\n","            max_beta = random_beta #update the maximum beta value thus far observed\n","            index_of_machine_to_play = slot_machine_index #set the machine to play to the current machine\n","    \n","    #play the selected slot machine, and record whether we win or lose\n","    if outcomes[turn_index][index_of_machine_to_play] == 1:\n","        number_of_positive_rewards[index_of_machine_to_play] += 1\n","    else:\n","        number_of_negative_rewards[index_of_machine_to_play] += 1\n","\n","print('Number of turns {0}:'.format(number_of_turns))  \n","\n","#compute and display the total number of times each slot machine was played\n","number_of_times_played = number_of_positive_rewards + number_of_negative_rewards \n","for slot_machine_index in range(number_of_bandits): #for each slot machine\n","    print('Slot machine {0} was played {1} times that is, {2:.2%}'.format(slot_machine_index, number_of_times_played[slot_machine_index], (number_of_times_played[slot_machine_index]/number_of_turns)))\n","\n","#identify and display the best slot machine to play\n","print('\\nOverall Conclusion: The best slot machine to play is machine {}!'.format(np.argmax(number_of_times_played)))\n","\n","#show true conversion rate\n","for i in range(6):\n","  print('True conversion rate for column {0}: {1:.2%}'.format(i, conversion_rates[i]))  "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of turns 1000:\n","Slot machine 0 was played 13.0 times that is, 1.30%\n","Slot machine 1 was played 478.0 times that is, 47.80%\n","Slot machine 2 was played 39.0 times that is, 3.90%\n","Slot machine 3 was played 72.0 times that is, 7.20%\n","Slot machine 4 was played 44.0 times that is, 4.40%\n","Slot machine 5 was played 68.0 times that is, 6.80%\n","Slot machine 6 was played 154.0 times that is, 15.40%\n","Slot machine 7 was played 67.0 times that is, 6.70%\n","Slot machine 8 was played 35.0 times that is, 3.50%\n","Slot machine 9 was played 16.0 times that is, 1.60%\n","Slot machine 10 was played 14.0 times that is, 1.40%\n","\n","Overall Conclusion: The best slot machine to play is machine 1!\n","True conversion rate for column 0: 7.40%\n","True conversion rate for column 1: 28.75%\n","True conversion rate for column 2: 6.90%\n","True conversion rate for column 3: 30.32%\n","True conversion rate for column 4: 16.17%\n","True conversion rate for column 5: 20.23%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"H5rlPKqy1fAf"},"source":["### Compare the Performance of Thompson Sampling vs. a Random Sampling Strategy\n","\n","Note that a random sampling can be a reasonable baseline to compare many RL algorithms."]},{"cell_type":"code","metadata":{"id":"FFaeHlVE15fe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607730791462,"user_tz":300,"elapsed":462,"user":{"displayName":"Nik Bear Brown","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiM1Q26nVUtugBRkregYHIFfrTZNtlP8O-WMVvR=s64","userId":"17467854027018001755"}},"outputId":"9b5d6e69-2bda-4ba4-acc9-39967d7c8a94"},"source":["#compute total number of wins using Thompson Sampling strategy\n","total_wins_thompson_sampling = np.sum(number_of_positive_rewards)\n","\n","#determine how many times we would win if we randomly choose a slot machine to play for each turn\n","total_wins_random_sampling = 0\n","for turn_index in range(number_of_turns):\n","  index_of_machine_to_play = np.random.randint(0, number_of_bandits) #randomly choose a machine to play\n","  if outcomes[turn_index][index_of_machine_to_play] == 1:\n","    total_wins_random_sampling += 1\n","\n","#display results\n","print('Total wins with Thompson Sampling: {0:.0f}'.format(total_wins_thompson_sampling))\n","print('Total wins with Random Sampling: {0:.0f}'.format(total_wins_random_sampling))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total wins with Thompson Sampling: 237\n","Total wins with Random Sampling: 163\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3tlM4eA5b85A"},"source":["# Andre Cianflone's Approach for K bandits with Thompson Sampling\n","\n","Andre Cianflone also implements the two beta algorithm form and compares it to t the $\\epsilon$-greedy algorithm and Upper Confidence Bound (UBC) algorithm. See https://github.com/andrecianflone/thompson\n","and https://colab.research.google.com/drive/1BHVH712x2Q2As9E5nN5Y8UR74T8w6AMO#scrollTo=lnzzQ4WZFLQ1\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"OJHIZ3Ru9be7"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"R4L_Vjr5hpCZ"},"source":["## References\n","[1]: Russo, Daniel, Benjamin Van Roy, Abbas Kazerouni, and Ian Osband. \"A Tutorial on Thompson Sampling.\" arXiv preprint arXiv:1707.02038 (2017). [link](https://arxiv.org/abs/1707.02038)\n","\n","[2]: Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. Vol. 1, no. 1. Cambridge: MIT press, 1998."]},{"cell_type":"markdown","metadata":{"id":"crQqSsV2b7yI"},"source":[""]}]}